{"id": "2505.09316", "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging", "authors": ["Hongjin Qian", "Zheng Liu"], "abstract": "Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents.", "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": "16 pages", "pdf_url": "https://arxiv.org/pdf/2505.09316.pdf", "abstract_url": "https://arxiv.org/abs/2505.09316", "categories": ["Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为InForage的强化学习框架，通过信息觅食理论（IFT）优化大型语言模型（LLMs）的外部检索增强推理过程，使其能够动态适应复杂任务中的信息需求变化。", "motivation": "解决传统检索增强生成方法在处理模糊、多步骤或动态变化信息需求时的不足，通过动态推理时检索提升LLMs的性能。", "method": "采用强化学习框架InForage，模拟动态信息寻求过程，通过奖励中间检索质量鼓励LLMs迭代收集和整合信息。", "result": "在通用问答、多跳推理任务及新开发的实时网络问答数据集上的广泛评估显示，InForage优于基线方法。", "conclusion": "InForage有效构建了强大、自适应且高效的推理代理，为复杂信息需求下的检索增强推理提供了新方向。"}}
{"id": "2505.09286", "title": "A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data", "authors": ["Jiin Park", "Misuk Kim"], "abstract": "Effectively analyzing online review data is essential across industries. However, many existing studies are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets. To address these limitations, we propose a multilingual, scalable, and unsupervised framework for cross-domain aspect detection. This framework is designed for multi-aspect labeling of multilingual and multi-domain review data. In this study, we apply automatic labeling to Korean and English review datasets spanning various domains and assess the quality of the generated labels through extensive experiments. Aspect category candidates are first extracted through clustering, and each review is then represented as an aspect-aware embedding vector using negative sampling. To evaluate the framework, we conduct multi-aspect labeling and fine-tune several pretrained language models to measure the effectiveness of the automatically generated labels. Results show that these models achieve high performance, demonstrating that the labels are suitable for training. Furthermore, comparisons with publicly available large language models highlight the framework's superior consistency and scalability when processing large-scale data. A human evaluation also confirms that the quality of the automatic labels is comparable to those created manually. This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments. Future research will explore automatic review summarization and the integration of artificial intelligence agents to further improve the efficiency and depth of review analysis.", "subjects": "Computation and Language (cs.CL)", "comments": "36 pages, 3 figures", "pdf_url": "https://arxiv.org/pdf/2505.09286.pdf", "abstract_url": "https://arxiv.org/abs/2505.09286", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种多语言、可扩展且无监督的框架，用于跨领域方面检测，适用于多语言和多领域评论数据的多方面标注。通过实验评估，证明了该框架生成的标签适合训练，并在处理大规模数据时表现出优异的连贯性和可扩展性。", "motivation": "解决现有研究局限于特定领域和语言，或依赖需要大规模标记数据集的监督学习方法的限制。", "method": "通过聚类提取方面类别候选，然后使用负采样将每条评论表示为方面感知的嵌入向量。", "result": "实验结果显示，使用自动生成标签的预训练语言模型表现优异，且自动标签的质量与人工创建的标签相当。", "conclusion": "本研究展示了一种克服监督方法限制、适应多语言和多环境的多方面标注方法的潜力，未来研究将探索自动评论总结和人工智能代理的集成以进一步提高评论分析的效率和深度。"}}
{"id": "2505.09388", "title": "Qwen3 Technical Report", "authors": ["An Yang", "Anfeng Li", "Baosong Yang", "Beichen Zhang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Gao", "Chengen Huang", "Chenxu Lv", "Chujie Zheng", "Dayiheng Liu", "Fan Zhou", "Fei Huang", "Feng Hu", "Hao Ge", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Yang", "Jiaxi Yang", "Jing Zhou", "Jingren Zhou", "Junyang Lin", "Kai Dang", "Keqin Bao", "Kexin Yang", "Le Yu", "Lianghao Deng", "Mei Li", "Mingfeng Xue", "Mingze Li", "Pei Zhang", "Peng Wang", "Qin Zhu", "Rui Men", "Ruize Gao", "Shixuan Liu", "Shuang Luo", "Tianhao Li", "Tianyi Tang", "Wenbiao Yin", "Xingzhang Ren", "Xinyu Wang", "Xinyu Zhang", "Xuancheng Ren", "Yang Fan", "Yang Su", "Yichang Zhang", "Yinger Zhang", "Yu Wan", "Yuqiong Liu", "Zekun Wang", "Zeyu Cui", "Zhenru Zhang", "Zhipeng Zhou", "Zihan Qiu"], "abstract": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09388.pdf", "abstract_url": "https://arxiv.org/abs/2505.09388", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "Qwen3是Qwen模型家族的最新版本，包括密集和混合专家（MoE）架构的大型语言模型（LLMs），参数规模从0.6到2350亿。关键创新包括集成思考模式和非思考模式到统一框架中，引入思考预算机制，以及通过旗舰模型知识减少构建小规模模型所需的计算资源。Qwen3在多语言支持、代码生成、数学推理等任务上达到最先进水平，并将多语言支持从29种扩展到119种语言和方言。", "motivation": "解决需要在不同模型之间切换以应对复杂推理和快速响应的问题，以及减少构建高性能小规模模型所需的计算资源。", "method": "集成思考模式和非思考模式到统一框架中，引入思考预算机制，利用旗舰模型知识优化小规模模型的构建。", "result": "Qwen3在多样化的基准测试中实现了最先进的结果，包括代码生成、数学推理等任务，并将多语言支持扩展到119种语言和方言。", "conclusion": "Qwen3通过其创新的统一框架和思考预算机制，不仅提高了性能和效率，还增强了全球可访问性，同时促进了社区驱动的研究和开发。"}}
{"id": "2505.09595", "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "abstract": "Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Multiagent Systems (cs.MA)", "comments": "Preprint. Submitted to the Journal of Artificial Intelligence Research (JAIR) on April 29, 2025", "pdf_url": "https://arxiv.org/pdf/2505.09595.pdf", "abstract_url": "https://arxiv.org/abs/2505.09595", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "WorldView-Bench是一个评估大型语言模型（LLMs）全球文化包容性（GCI）的基准，旨在解决LLMs中西方中心主义和文化同质化的问题。", "motivation": "解决大型语言模型（LLMs）在训练和对齐过程中强化西方中心主义认识论和社会文化规范，导致文化同质化并限制其反映全球文明多样性的能力。", "method": "采用Multiplex Worldview理论，通过自由形式生成评估测量文化极化，实施两种干预策略：上下文实现的多路复用LLMs和多代理系统（MAS）实现的多路复用LLMs。", "result": "MAS实现的多路复用LLMs将视角分布得分（PDS）熵从基线的13%显著提高到94%，同时情感倾向转为积极（67.7%），文化平衡性增强。", "conclusion": "多路复用感知的AI评估在减轻LLMs中的文化偏见方面具有潜力，为更包容和伦理对齐的AI系统铺平了道路。"}}
{"id": "2505.08988", "title": "Generalization in Monitored Markov Decision Processes (Mon-MDPs)", "authors": ["Montaser Mohammedalamen", "Michael Bowling"], "abstract": "Reinforcement learning (RL) typically models the interaction between the agent and environment as a Markov decision process (MDP), where the rewards that guide the agent's behavior are always observable. However, in many real-world scenarios, rewards are not always observable, which can be modeled as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have been limited to simple, tabular cases, restricting their applicability to real-world problems. This work explores Mon-MDPs using function approximation (FA) and investigates the challenges involved. We show that combining function approximation with a learned reward model enables agents to generalize from monitored states with observable rewards, to unmonitored environment states with unobservable rewards. Therefore, we demonstrate that such generalization with a reward model achieves near-optimal policies in environments formally defined as unsolvable. However, we identify a critical limitation of such function approximation, where agents incorrectly extrapolate rewards due to overgeneralization, resulting in undesirable behaviors. To mitigate overgeneralization, we propose a cautious police optimization method leveraging reward uncertainty. This work serves as a step towards bridging this gap between Mon-MDP theory and real-world applications.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "Under Review", "pdf_url": "https://arxiv.org/pdf/2505.08988.pdf", "abstract_url": "https://arxiv.org/abs/2505.08988", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了在监控马尔可夫决策过程（Mon-MDPs）中使用函数逼近（FA）的挑战，展示了结合函数逼近和学习奖励模型能使代理从有可观察奖励的监控状态泛化到无观察奖励的非监控环境状态，并提出了缓解过度泛化的谨慎策略优化方法。", "motivation": "解决在现实世界场景中奖励不总是可观察的问题，扩展Mon-MDPs在复杂问题中的应用。", "method": "使用函数逼近（FA）结合学习奖励模型，并提出谨慎策略优化方法以缓解过度泛化。", "result": "展示了通过奖励模型实现泛化能在形式上定义为不可解的环境中实现接近最优的策略，但也识别了函数逼近的局限性，即代理因过度泛化而错误推断奖励。", "conclusion": "本研究为缩小Mon-MDP理论与实际应用之间的差距迈出了一步，提出了解决过度泛化问题的方法，促进了Mon-MDPs在更广泛领域的应用。"}}
{"id": "2505.08995", "title": "Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning", "authors": ["Ardian Selmonaj", "Oleg Szehr", "Giacomo Del Rio", "Alessandro Antonucci", "Adrian Schneider", "Michael Rüegsegger"], "abstract": "This work presents a Hierarchical Multi-Agent Reinforcement Learning framework for analyzing simulated air combat scenarios involving heterogeneous agents. The objective is to identify effective Courses of Action that lead to mission success within preset simulations, thereby enabling the exploration of real-world defense scenarios at low cost and in a safe-to-fail setting. Applying deep Reinforcement Learning in this context poses specific challenges, such as complex flight dynamics, the exponential size of the state and action spaces in multi-agent systems, and the capability to integrate real-time control of individual units with look-ahead planning. To address these challenges, the decision-making process is split into two levels of abstraction: low-level policies control individual units, while a high-level commander policy issues macro commands aligned with the overall mission targets. This hierarchical structure facilitates the training process by exploiting policy symmetries of individual agents and by separating control from command tasks. The low-level policies are trained for individual combat control in a curriculum of increasing complexity. The high-level commander is then trained on mission targets given pre-trained control policies. The empirical validation confirms the advantages of the proposed framework.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Robotics (cs.RO)", "comments": "Published as journal chapter in Deep Learning Applications, Vol. 1, by Taylor & Francis", "pdf_url": "https://arxiv.org/pdf/2505.08995.pdf", "abstract_url": "https://arxiv.org/abs/2505.08995", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Multiagent Systems (cs.MA)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种分层多智能体强化学习框架，用于分析涉及异构智能体的模拟空战场景，旨在识别导致任务成功的有效行动方案，从而以低成本和安全的失败设置探索现实世界的防御场景。", "motivation": "解决在复杂飞行动力学、多智能体系统中状态和动作空间的指数大小以及实时控制与前瞻规划结合的挑战下，应用深度强化学习于空战战术分析的问题。", "method": "将决策过程分为两个抽象层次：低层策略控制单个单位，高层指挥官策略发布与整体任务目标一致的宏观命令。这种分层结构通过利用个体智能体的策略对称性和分离控制与命令任务来促进训练过程。", "result": "实证验证证实了所提出框架的优势。", "conclusion": "分层多智能体强化学习框架有效地解决了空战战术分析中的挑战，为现实世界防御场景的探索提供了一种低成本和安全的方法。"}}
{"id": "2505.09012", "title": "Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation", "authors": ["Bo Meng", "Chenghao Xu", "Yongli Zhu"], "abstract": "Cascading failures in power grids can lead to grid collapse, causing severe disruptions to social operations and economic activities. In certain cases, multi-stage cascading failures can occur. However, existing cascading-failure-mitigation strategies are usually single-stage-based, overlooking the complexity of the multi-stage scenario. This paper treats the multi-stage cascading failure problem as a reinforcement learning task and develops a simulation environment. The reinforcement learning agent is then trained via the deterministic policy gradient algorithm to achieve continuous actions. Finally, the effectiveness of the proposed approach is validated on the IEEE 14-bus and IEEE 118-bus systems.", "subjects": "Artificial Intelligence (cs.AI); Systems and Control (eess.SY)", "comments": "This paper has been accepted and presented at ICLR 2025 in Singapore, Apr. 28, 2025", "pdf_url": "https://arxiv.org/pdf/2505.09012.pdf", "abstract_url": "https://arxiv.org/abs/2505.09012", "categories": ["Artificial Intelligence (cs.AI)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于深度强化学习的方法，用于电力电网多级级联故障的缓解。通过将多级级联故障问题视为强化学习任务，并开发模拟环境，使用确定性策略梯度算法训练强化学习代理以实现连续动作。在IEEE 14-bus和IEEE 118-bus系统上验证了所提方法的有效性。", "motivation": "电力电网中的级联故障可能导致电网崩溃，严重影响社会运行和经济活动。现有的级联故障缓解策略通常是基于单阶段的，忽视了多级场景的复杂性。", "method": "将多级级联故障问题视为强化学习任务，开发模拟环境，并使用确定性策略梯度算法训练强化学习代理以实现连续动作。", "result": "在IEEE 14-bus和IEEE 118-bus系统上验证了所提方法的有效性。", "conclusion": "本文提出的深度强化学习方法有效地缓解了电力电网中的多级级联故障，为解决复杂电网故障问题提供了新的思路。"}}
{"id": "2505.09024", "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind", "authors": ["Aaron Baughman", "Rahul Agarwal", "Eduardo Morales", "Gozde Akay"], "abstract": "We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": "9 pages, 6 figures, 3 tables", "pdf_url": "https://arxiv.org/pdf/2505.09024.pdf", "abstract_url": "https://arxiv.org/abs/2505.09024", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了一种元提示方法，旨在通过代理强化学习技术，使大型语言模型（LLM）能够生成与人类心理预期相似的文本，解决理论与心智（ToM）对齐问题。", "motivation": "解决大型语言模型生成文本与人类心理预期之间的对齐问题，提高生成内容的质量和相关性。", "method": "应用代理强化学习技术，其中一个LLM作为法官（LLMaaJ）教导另一个LLM通过上下文学习生成内容，同时考虑人类对生成文本特质的预期和非预期。", "result": "实验结果显示，人类内容审阅者的期望与AI生成内容的对齐率达到53.8%，平均迭代次数为4.38。通过优化，内容质量得到提升，特别是在网球动作报道的覆盖面上。", "conclusion": "该方法不仅在2024年美国网球公开赛中得到应用，还被推广到其他体育和娱乐现场活动中，展示了其在提高内容生成质量和相关性方面的潜力。"}}
{"id": "2505.09029", "title": "Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control", "authors": ["Hazim Alzorgan", "Abolfazl Razi"], "abstract": "Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient (TD3), depend on basic noise-based exploration, which can result in less than optimal policy convergence. In this study, we introduce Monte Carlo Beam Search (MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts with TD3 to improve exploration and action selection. MCBS produces several candidate actions around the policy's output and assesses them through short-horizon rollouts, enabling the agent to make better-informed choices. We test MCBS across various continuous-control benchmarks, including HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency and performance compared to standard TD3 and other baseline methods like SAC, PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy learning through structured look-ahead search while ensuring computational efficiency. Additionally, we offer a detailed analysis of crucial hyperparameters, such as beam width and rollout depth, and explore adaptive strategies to optimize MCBS for complex control tasks. Our method shows a higher convergence rate across different environments compared to TD3, SAC, PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward within around 200 thousand timesteps compared to 400 thousand timesteps for the second-best method.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09029.pdf", "abstract_url": "https://arxiv.org/abs/2505.09029", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新的混合方法——蒙特卡洛束搜索（MCBS），结合束搜索和蒙特卡洛滚动与TD3，以改进探索和动作选择。MCBS在多个连续控制基准测试中表现出优于标准TD3和其他基线方法的样本效率和性能。", "motivation": "解决基于噪声的探索在行动者-评论家方法（如TD3）中可能导致政策收敛不理想的问题。", "method": "引入蒙特卡洛束搜索（MCBS），通过在政策输出周围生成多个候选动作并通过短视界滚动评估它们，使代理能够做出更明智的选择。", "result": "MCBS在HalfCheetah-v4、Walker2d-v5和Swimmer-v5等连续控制基准测试中显示出更高的样本效率和性能，比TD3、SAC、PPO和A2C等方法更快达到90%的最大可达到奖励。", "conclusion": "MCBS通过结构化前瞻搜索增强了政策学习，同时保证了计算效率，适用于复杂的控制任务。"}}
{"id": "2505.09031", "title": "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification", "authors": ["Adarsh Kumar", "Hwiyoon Kim", "Jawahar Sai Nathani", "Neil Roy"], "abstract": "Hallucination, where large language models (LLMs) generate confident but incorrect or irrelevant information, remains a key limitation in their application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has emerged as a promising method for improving multistep reasoning by guiding models through intermediate steps. However, CoT alone does not fully address the hallucination problem. In this work, we investigate how combining CoT with retrieval-augmented generation (RAG), as well as applying self-consistency and self-verification strategies, can reduce hallucinations and improve factual accuracy. By incorporating external knowledge sources during reasoning and enabling models to verify or revise their own outputs, we aim to generate more accurate and coherent responses. We present a comparative evaluation of baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification techniques. Our results highlight the effectiveness of each method and identify the most robust approach for minimizing hallucinations while preserving fluency and reasoning depth.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09031.pdf", "abstract_url": "https://arxiv.org/abs/2505.09031", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文探讨了如何通过结合思维链（CoT）、检索增强生成（RAG）、自我一致性和自我验证策略来减少大型语言模型（LLMs）的幻觉问题，提高事实准确性。", "motivation": "解决大型语言模型在复杂、开放式任务中产生自信但不正确或无关信息（即幻觉）的关键限制。", "method": "结合思维链（CoT）提示、检索增强生成（RAG）、自我一致性和自我验证策略。", "result": "比较评估显示，这些方法在减少幻觉同时保持流畅性和推理深度方面各有成效，确定了最鲁棒的方法。", "conclusion": "结合CoT、RAG、自我一致性和自我验证的策略能有效减少LLMs的幻觉，提高生成回答的准确性和连贯性。"}}
{"id": "2505.09114", "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer", "authors": ["Minh Hoang Nguyen", "Linh Le Pham Van", "Thommen George Karimpanal", "Sunil Gupta", "Hung Le"], "abstract": "Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains. However, DT requires high-quality, comprehensive data to perform optimally. In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance. To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning. CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios. Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches. Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications. These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agents' performance and generalization capabilities.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09114.pdf", "abstract_url": "https://arxiv.org/abs/2505.09114", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了Counterfactual Reasoning Decision Transformer (CRDT)，一个受反事实推理启发的新框架，旨在解决决策变换器(DT)在现实应用中因数据不足和次优行为稀缺而面临的挑战。CRDT通过生成和利用反事实经验，增强了DT在未知场景中的决策能力。", "motivation": "决策变换器(DT)在现代强化学习中扮演着重要角色，但其性能依赖于高质量、全面的数据。在现实应用中，训练数据的缺乏和最优行为的稀缺使得在离线数据集上训练变得困难，次优数据会阻碍性能。", "method": "提出了Counterfactual Reasoning Decision Transformer (CRDT)，这是一个受反事实推理启发的新框架，通过生成和利用反事实经验来增强DT的能力。", "result": "在Atari和D4RL基准测试中，包括数据有限和动态改变的场景，CRDT的表现优于传统的DT方法。此外，反事实推理使DT代理能够获得缝合能力，无需架构修改即可结合次优轨迹。", "conclusion": "这些结果突出了反事实推理在增强强化学习代理性能和泛化能力方面的潜力。"}}
{"id": "2505.09289", "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"", "authors": ["Pedro M. P. Curvo", "Mara Dragomir", "Salvador Torpes", "Mohammadmahdi Rahimi"], "abstract": "This study evaluates and extends the findings made by Piatti et al., who introduced GovSim, a simulation framework designed to assess the cooperative decision-making capabilities of large language models (LLMs) in resource-sharing scenarios. By replicating key experiments, we validate claims regarding the performance of large models, such as GPT-4-turbo, compared to smaller models. The impact of the universalization principle is also examined, with results showing that large models can achieve sustainable cooperation, with or without the principle, while smaller models fail without it. In addition, we provide multiple extensions to explore the applicability of the framework to new settings. We evaluate additional models, such as DeepSeek-V3 and GPT-4o-mini, to test whether cooperative behavior generalizes across different architectures and model sizes. Furthermore, we introduce new settings: we create a heterogeneous multi-agent environment, study a scenario using Japanese instructions, and explore an \"inverse environment\" where agents must cooperate to mitigate harmful resource distributions. Our results confirm that the benchmark can be applied to new models, scenarios, and languages, offering valuable insights into the adaptability of LLMs in complex cooperative tasks. Moreover, the experiment involving heterogeneous multi-agent systems demonstrates that high-performing models can influence lower-performing ones to adopt similar behaviors. This finding has significant implications for other agent-based applications, potentially enabling more efficient use of computational resources and contributing to the development of more effective cooperative AI systems.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "11 Tables, 9 Figures", "pdf_url": "https://arxiv.org/pdf/2505.09289.pdf", "abstract_url": "https://arxiv.org/abs/2505.09289", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本研究评估并扩展了Piatti等人关于大型语言模型(LLMs)在资源共享场景中合作决策能力的研究，通过复制关键实验验证了大型模型如GPT-4-turbo的性能优于小型模型，并探讨了普遍化原则的影响。研究还通过引入新模型、场景和语言，证明了该框架的广泛适用性，特别是在异构多智能体环境中，高性能模型能引导低性能模型采取相似行为，这对开发更有效的合作AI系统具有重要意义。", "motivation": "评估和扩展Piatti等人关于LLMs在资源共享中合作决策能力的研究，验证大型模型相对于小型模型的性能优势，并探索普遍化原则的影响及其在新场景下的适用性。", "method": "通过复制关键实验、引入新模型（如DeepSeek-V3和GPT-4o-mini）、创建异构多智能体环境、使用日语指令研究新场景，以及探索“逆向环境”来评估LLMs的合作行为。", "result": "大型模型能在有无普遍化原则的情况下实现可持续合作，而小型模型无此原则则失败；异构多智能体环境中高性能模型能影响低性能模型的行为；框架适用于新模型、场景和语言。", "conclusion": "研究证实了LLMs在复杂合作任务中的适应性和框架的广泛适用性，特别是在异构多智能体系统中高性能模型对低性能模型的积极影响，为开发更有效的合作AI系统提供了重要见解。"}}
{"id": "2505.09396", "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "abstract": "The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09396.pdf", "abstract_url": "https://arxiv.org/abs/2505.09396", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文探讨了大型语言模型（LLMs）在代理系统中的角色，特别是其在游戏理论设置中复制人类战略推理的能力。通过比较三种代理设计，研究发现人类启发的认知结构可以增强LLM代理与人类战略行为的一致性，但代理设计复杂性与人类相似性之间的关系是非线性的。", "motivation": "随着大型语言模型（LLMs）的迅速崛起，人工智能（AI）研究转向了代理系统，这引发了对LLM基础代理在多大程度上能复制人类战略推理的关键问题。", "method": "研究通过评估三种代理设计：简单的游戏理论模型、非结构化的LLM作为代理模型以及集成到传统代理框架中的LLM，使用猜测游戏作为测试平台，将这些代理与人类参与者在一般推理模式和基于角色的个体目标上进行对比。", "result": "分析覆盖了25种代理配置的2000多个推理样本，显示人类启发的认知结构可以增强LLM代理与人类战略行为的一致性，但代理设计复杂性与人类相似性之间的关系是非线性的。", "conclusion": "研究强调了底层LLM能力的关键依赖性，并指出了简单架构增强的局限性，为未来在代理系统中整合人类战略推理提供了方向。"}}
{"id": "2505.09614", "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?", "authors": ["Anthony GX-Chen", "Dongyan Lin", "Mandana Samiei", "Doina Precup", "Blake A. Richards", "Rob Fergus", "Kenneth Marino"], "abstract": "Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established \"Blicket Test\" paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This \"disjunctive bias\" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09614.pdf", "abstract_url": "https://arxiv.org/abs/2505.09614", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了语言模型（LM）代理在探索和理解世界因果结构方面的能力，发现它们存在与人类相似的‘析取偏见’，并提出了一种减少这种偏见的测试时采样方法。", "motivation": "研究语言模型代理是否具备有效探索和理解世界因果结构的能力，以及它们是否存在系统性偏见导致错误结论。", "method": "使用发展心理学中建立的‘Blicket Test’范式来检验LMs探索和推断因果关系的能力，并提出了一种测试时采样方法来减少偏见。", "result": "发现LMs在推断常见的、直观的析取因果关系时表现可靠，但在处理不常见但证据充分的合取关系时系统性地困难，这种‘析取偏见’在不同模型家族、大小和提示策略中持续存在。", "conclusion": "LMs可能从训练数据中继承了深层次的推理启发式方法，表现出与成人相似的推理模式。提出的测试时采样方法显著减少了析取偏见，使LMs更接近科学、因果严谨的推理目标。"}}
{"id": "2505.08842", "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries", "authors": ["Zekun Wu", "Seonglae Cho", "Umar Mohammed", "Cristian Munoz", "Kleyton Costa", "Xin Guan", "Theo King", "Ze Wang", "Emre Kazim", "Adriano Koshiyama"], "abstract": "Open-source AI libraries are foundational to modern AI systems but pose significant, underexamined risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance. We present LibVulnWatch, a graph-based agentic assessment framework that performs deep, source-grounded evaluations of these libraries. Built on LangGraph, the system coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases. LibVulnWatch generates reproducible, governance-aligned scores across five critical domains, publishing them to a public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely used libraries, including ML frameworks, LLM inference engines, and agent orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library. These include critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability. By translating high-level governance principles into practical, verifiable metrics, LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.", "subjects": "Cryptography and Security (cs.CR); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.08842.pdf", "abstract_url": "https://arxiv.org/abs/2505.08842", "categories": ["Cryptography and Security (cs.CR)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "LibVulnWatch是一个基于图的代理评估框架，用于深入评估开源AI库的风险，包括安全、许可、维护等方面，通过协调专门代理的网络来提取、验证和量化风险，并生成可重复的治理对齐分数。", "motivation": "开源AI库在现代AI系统中扮演着基础性角色，但在安全、许可、维护、供应链完整性和法规遵从性等方面存在未被充分审视的风险。", "method": "基于LangGraph构建的图基代理评估框架，协调一系列专门代理的有向无环图，从可信源提取、验证和量化风险。", "result": "应用于20个广泛使用的库，系统覆盖了高达88%的OpenSSF Scorecard检查，同时发现每个库多达19个额外风险，包括关键的远程代码执行漏洞等。", "conclusion": "LibVulnWatch通过将高级治理原则转化为实际可验证的指标，为技术AI治理提供了一个可扩展、透明的机制，用于持续的供应链风险评估和明智的库选择。"}}
{"id": "2412.15404", "title": "A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science", "authors": ["Ahmet Yasin Aytar", "Kemal Kilic", "Kamer Kaya"], "abstract": "In the rapidly evolving field of data science, efficiently navigating the expansive body of academic literature is crucial for informed decision-making and innovation. This paper presents an enhanced Retrieval-Augmented Generation (RAG) application, an artificial intelligence (AI)-based system designed to assist data scientists in accessing precise and contextually relevant academic resources. The AI-powered application integrates advanced techniques, including the GeneRation Of BIbliographic Data (GROBID) technique for extracting bibliographic information, fine-tuned embedding models, semantic chunking, and an abstract-first retrieval method, to significantly improve the relevance and accuracy of the retrieved information. This implementation of AI specifically addresses the challenge of academic literature navigation. A comprehensive evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS) framework demonstrates substantial improvements in key metrics, particularly Context Relevance, underscoring the system's effectiveness in reducing information overload and enhancing decision-making processes. Our findings highlight the potential of this enhanced Retrieval-Augmented Generation system to transform academic exploration within data science, ultimately advancing the workflow of research and innovation in the field.", "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2412.15404.pdf", "abstract_url": "https://arxiv.org/abs/2412.15404", "categories": ["Information Retrieval (cs.IR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了一种增强的检索增强生成（RAG）应用，旨在帮助数据科学家更高效地访问精确且上下文相关的学术资源。", "motivation": "解决数据科学领域快速发展的学术文献导航问题，以减少信息过载并提升决策过程。", "method": "整合了GROBID技术、微调嵌入模型、语义分块和摘要优先检索方法，以提高检索信息的相关性和准确性。", "result": "使用RAGAS框架进行的全面评估显示，在关键指标上，特别是上下文相关性方面，有显著改进。", "conclusion": "增强的RAG系统有潜力改变数据科学领域的学术探索，进而推动研究和创新的工作流程。"}}
{"id": "2505.09436", "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios", "authors": ["Raghav Garg", "Kapil Sharma", "Karan Gupta"], "abstract": "Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09436.pdf", "abstract_url": "https://arxiv.org/abs/2505.09436", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本文介绍了CXMArena，一个专为评估AI在客户体验管理（CXM）操作环境中性能而设计的大规模合成基准数据集。该数据集通过模拟品牌CXM实体，包括知识文章、问题分类和联系中心对话，以及注入受控噪声和严格的自动验证，来紧密代表现实世界的分布。CXMArena提供了针对五个重要操作任务的专用基准，基线实验显示了当前模型在这些任务上的挑战。", "motivation": "大型语言模型（LLMs）在客户体验管理（CXM）方面具有巨大潜力，尤其是在联系中心操作中。然而，由于数据稀缺（由于隐私问题）和当前基准的限制，评估它们在复杂操作环境中的实际效用受到阻碍。现有的基准往往缺乏现实主义，未能融入深度的知识库（KB）整合、现实世界的噪音或超越对话流畅性的关键操作任务。", "method": "为了解决这一问题，我们引入了CXMArena，这是一个新颖的、大规模合成的基准数据集，专门设计用于评估AI在操作CXM上下文中的性能。我们开发了一个可扩展的LLM驱动的管道，模拟品牌的CXM实体，这些实体构成了我们数据集的基础，如包括产品规格、问题分类和联系中心对话的知识文章。通过受控噪声注入（由领域专家指导）和严格的自动验证，这些实体紧密代表现实世界的分布。", "result": "我们的基线实验强调了基准的难度：即使是最先进的嵌入和生成模型在文章搜索上的准确率也只有68%，而标准嵌入方法在知识库细化上的F1得分低至0.3，这突出了当前模型在需要复杂管道和解决方案而非传统技术方面的重大挑战。", "conclusion": "CXMArena的发布为评估AI在客户体验管理中的性能提供了一个现实且具有挑战性的基准，揭示了当前模型在处理复杂CXM任务时的局限性，并指出了未来研究的方向。"}}
{"id": "2505.08803", "title": "Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models", "authors": ["Zizhao Hu", "Mohammad Rostami", "Jesse Thomason"], "abstract": "Recent research has highlighted the risk of generative model collapse, where performance progressively degrades when continually trained on self-generated data. However, existing exploration on model collapse is limited to single, unimodal models, limiting our understanding in more realistic scenarios, such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving. We expand the synthetic data training and model collapse study to multi-modal vision-language generative systems, such as vision-language models (VLMs) and text-to-image diffusion models, as well as recursive generate-train loops with multiple models. We find that model collapse, previously observed in single-modality generative models, exhibits distinct characteristics in the multi-modal context, such as improved vision-language alignment and increased variance in VLM image-captioning task. Additionally, we find that general approaches such as increased decoding budgets, greater model diversity, and relabeling with frozen models can effectively mitigate model collapse. Our findings provide initial insights and practical guidelines for reducing the risk of model collapse in self-improving multi-agent AI systems and curating robust multi-modal synthetic datasets.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.08803.pdf", "abstract_url": "https://arxiv.org/abs/2505.08803", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了在多模态视觉语言生成系统（如视觉语言模型和文本到图像扩散模型）中，模型崩溃的风险及其独特特征，并提出了有效的缓解策略。", "motivation": "解决在多模态AI代理通过合成数据自主交互和持续进化的现实场景中，模型崩溃问题的理解和缓解。", "method": "将合成数据训练和模型崩溃研究扩展到多模态视觉语言生成系统，包括递归生成-训练循环与多模型交互。", "result": "发现多模态上下文中的模型崩溃表现出独特特征，如视觉语言对齐的改善和VLM图像字幕任务中方差的增加，并提出增加解码预算、模型多样性和使用冻结模型重新标记等策略有效缓解模型崩溃。", "conclusion": "研究结果为减少自我改进的多代理AI系统中模型崩溃的风险和构建稳健的多模态合成数据集提供了初步见解和实用指南。"}}
{"id": "2505.08807", "title": "Security of Internet of Agents: Attacks and Countermeasures", "authors": ["Yuntao Wang", "Yanghe Pan", "Shaolong Guo", "Zhou Su"], "abstract": "With the rise of large language and vision-language models, AI agents have evolved into autonomous, interactive systems capable of perception, reasoning, and decision-making. As they proliferate across virtual and physical domains, the Internet of Agents (IoA) has emerged as a key infrastructure for enabling scalable and secure coordination among heterogeneous agents. This survey offers a comprehensive examination of the security and privacy landscape in IoA systems. We begin by outlining the IoA architecture and its distinct vulnerabilities compared to traditional networks, focusing on four critical aspects: identity authentication threats, cross-agent trust issues, embodied security, and privacy risks. We then review existing and emerging defense mechanisms and highlight persistent challenges. Finally, we identify open research directions to advance the development of resilient and privacy-preserving IoA ecosystems.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": "11 pages, 5 figures, 3 tables, submitted to IEEE OJCS", "pdf_url": "https://arxiv.org/pdf/2505.08807.pdf", "abstract_url": "https://arxiv.org/abs/2505.08807", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文全面调查了智能代理互联网（IoA）系统的安全与隐私问题，探讨了其架构、独特漏洞、现有防御机制及未来研究方向。", "motivation": "随着大型语言和视觉语言模型的兴起，AI代理已发展成为能够感知、推理和决策的自主交互系统。它们在虚拟和物理领域的普及使得IoA成为实现异构代理之间可扩展和安全协调的关键基础设施。本文旨在解决IoA系统中的安全与隐私挑战。", "method": "本文首先概述了IoA架构及其与传统网络相比的独特漏洞，重点关注四个关键方面：身份认证威胁、跨代理信任问题、体现安全和隐私风险。然后，回顾了现有和新兴的防御机制，并突出了持续的挑战。", "result": "通过分析，本文指出了IoA系统中的主要安全与隐私问题，并总结了现有的防御措施及其局限性。", "conclusion": "最后，本文提出了开放的研究方向，以推动开发具有弹性和隐私保护的IoA生态系统。"}}
{"id": "2505.08825", "title": "Multi-source Plume Tracing via Multi-Agent Reinforcement Learning", "authors": ["Pedro Antonio Alarcon Granadeno", "Theodore Chambers", "Jane Cleland-Huang"], "abstract": "Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon gas leak (2015) demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment. Traditional methods, such as gradient-based or biologically inspired approaches, often fail in realistic, turbulent conditions. To address these challenges, we present a Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing multiple airborne pollution sources using a swarm of small uncrewed aerial systems (sUAS). Our method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical action-observation pairs, effectively approximating latent states. Unlike prior work, we use a general-purpose simulation environment based on the Gaussian Plume Model (GPM), incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources. The incorporation of action histories as part of the inputs further enhances the adaptability of our model in complex, partially observable environments. Extensive simulations show that our algorithm significantly outperforms conventional approaches. Specifically, our model allows agents to explore only 1.29\\% of the environment to successfully locate pollution sources.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)", "comments": "13 pages, 7 figures", "pdf_url": "https://arxiv.org/pdf/2505.08825.pdf", "abstract_url": "https://arxiv.org/abs/2505.08825", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于多智能体强化学习（MARL）的算法，用于通过小型无人飞行器系统（sUAS）群定位多个空气污染源，以解决传统方法在现实湍流条件下失效的问题。", "motivation": "工业灾难如博帕尔灾难（1984年）和阿利索峡谷气体泄漏（2015年）表明，迫切需要快速可靠的烟羽追踪算法以保护公共健康和环境。", "method": "该方法将问题建模为部分可观察马尔可夫游戏（POMG），采用基于长短期记忆（LSTM）的动作特定双深度循环Q网络（ADDRQN），利用历史动作-观察对的完整序列有效近似潜在状态。", "result": "大量模拟显示，该算法显著优于传统方法，使智能体仅需探索环境的1.29%即可成功定位污染源。", "conclusion": "该研究提出了一种在多源烟羽追踪中表现出色的MARL算法，为复杂、部分可观察环境中的污染源定位提供了有效解决方案。"}}
{"id": "2505.08829", "title": "Aggregating Concepts of Fairness and Accuracy in Predictive Systems", "authors": ["David Kinney"], "abstract": "An algorithm that outputs predictions about the state of the world will almost always be designed with the implicit or explicit goal of outputting accurate predictions (i.e., predictions that are likely to be true). In addition, the rise of increasingly powerful predictive algorithms brought about by the recent revolution in artificial intelligence has led to an emphasis on building predictive algorithms that are fair, in the sense that their predictions do not systematically evince bias or bring about harm to certain individuals or groups. This state of affairs presents two conceptual challenges. First, the goals of accuracy and fairness can sometimes be in tension, and there are no obvious normative guidelines for managing the trade-offs between these two desiderata when they arise. Second, there are many distinct ways of measuring both the accuracy and fairness of a predictive algorithm; here too, there are no obvious guidelines on how to aggregate our preferences for predictive algorithms that satisfy disparate measures of fairness and accuracy to various extents. The goal of this paper is to address these challenges by arguing that there are good reasons for using a linear combination of accuracy and fairness metrics to measure the all-things-considered value of a predictive algorithm for agents who care about both accuracy and fairness. My argument depends crucially on a classic result in the preference aggregation literature due to Harsanyi. After making this formal argument, I apply my result to an analysis of accuracy-fairness trade-offs using the COMPAS dataset compiled by Angwin et al.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.08829.pdf", "abstract_url": "https://arxiv.org/abs/2505.08829", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了在预测系统中如何平衡公平性和准确性的问题，提出使用线性组合的方法来综合评估预测算法的价值。", "motivation": "随着人工智能技术的发展，预测算法在追求准确性的同时，也需要考虑公平性，避免对特定个体或群体造成偏见或伤害。然而，准确性和公平性之间可能存在冲突，且缺乏明确的规范来指导如何在这两者之间进行权衡。", "method": "作者提出使用线性组合的方法来综合评估预测算法的价值，这一方法基于Harsanyi在偏好聚合文献中的经典结果。", "result": "通过应用这一方法对COMPAS数据集进行分析，作者展示了如何在准确性和公平性之间进行权衡。", "conclusion": "本文为那些同时关心准确性和公平性的决策者提供了一种评估预测算法价值的有效方法，为解决准确性和公平性之间的冲突提供了理论支持。"}}
{"id": "2505.08844", "title": "CellTypeAgent: Trustworthy cell type annotation with Large Language Models", "authors": ["Jiawen Chen", "Jianghao Zhang", "Huaxiu Yao", "Yun Li"], "abstract": "Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis. We present a trustworthy large language model (LLM)-agent, CellTypeAgent, which integrates LLMs with verification from relevant databases. CellTypeAgent achieves higher accuracy than existing methods while mitigating hallucinations. We evaluated CellTypeAgent across nine real datasets involving 303 cell types from 36 tissues. This combined approach holds promise for more efficient and reliable cell type annotation.", "subjects": "Genomics (q-bio.GN); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.08844.pdf", "abstract_url": "https://arxiv.org/abs/2505.08844", "categories": ["Genomics (q-bio.GN)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "CellTypeAgent是一种可信赖的大型语言模型（LLM）代理，用于单细胞RNA测序分析中的细胞类型注释，通过整合LLMs和相关数据库的验证，实现了比现有方法更高的准确性，并减少了幻觉现象。", "motivation": "解决单细胞RNA测序分析中细胞类型注释这一关键但繁琐的步骤，提高注释的准确性和可靠性。", "method": "结合大型语言模型（LLMs）和相关数据库的验证，开发了CellTypeAgent。", "result": "在涉及36个组织的303种细胞类型的九个真实数据集上评估，CellTypeAgent显示出更高的准确性。", "conclusion": "CellTypeAgent的结合方法为更高效和可靠的细胞类型注释提供了希望。"}}
{"id": "2505.09003", "title": "Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition", "authors": ["Zeki Doruk Erden", "Donia Gasmi", "Boi Faltings"], "abstract": "Continual learning for reinforcement learning agents remains a significant challenge, particularly in preserving and leveraging existing information without an external signal to indicate changes in tasks or environments. In this study, we explore the effectiveness of autoencoders in detecting new tasks and matching observed environments to previously encountered ones. Our approach integrates policy optimization with familiarity autoencoders within an end-to-end continual learning system. This system can recognize and learn new tasks or environments while preserving knowledge from earlier experiences and can selectively retrieve relevant knowledge when re-encountering a known environment. Initial results demonstrate successful continual learning without external signals to indicate task changes or reencounters, showing promise for this methodology.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "Published in the Autonomous Robots and Multirobot Systems (ARMS) workshop at AAMAS 2025", "pdf_url": "https://arxiv.org/pdf/2505.09003.pdf", "abstract_url": "https://arxiv.org/abs/2505.09003", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了自动编码器在检测新任务和匹配观察到的环境到先前遇到的环境中的有效性，提出了一种集成策略优化和熟悉度自动编码器的端到端持续学习系统。", "motivation": "强化学习代理在持续学习中面临重大挑战，尤其是在没有外部信号指示任务或环境变化的情况下保留和利用现有信息。", "method": "我们的方法将策略优化与熟悉度自动编码器集成在一个端到端的持续学习系统中。", "result": "初步结果表明，无需外部信号指示任务变化或重新遇到，即可成功实现持续学习。", "conclusion": "这种方法在识别和学习新任务或环境的同时，能够保留早期经验的知识，并在重新遇到已知环境时选择性地检索相关知识，显示了这一方法的前景。"}}
{"id": "2505.09081", "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation", "authors": ["Gaurav Koley"], "abstract": "Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.", "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.09081.pdf", "abstract_url": "https://arxiv.org/abs/2505.09081", "categories": ["Social and Information Networks (cs.SI)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了SALM（社交代理语言模型框架），一种将语言模型集成到社交网络模拟中的新方法，旨在解决传统基于规则的代理建模在捕捉人类社交互动细微动态方面的局限性。", "motivation": "传统的基于代理的社交系统建模方法依赖于基于规则的行为，这限制了其捕捉超越预定义规则和利用语言模型对人类社交互动的上下文理解的能力。", "method": "SALM框架采用了一种分层提示架构，实现了在超过4,000个时间步长的模拟中的时间稳定性，同时减少了73%的令牌使用；一个基于注意力的记忆系统，实现了80%的缓存命中率；以及关于人格稳定性的正式界限。", "result": "通过针对SNAP自我网络的广泛验证，SALM展示了第一个能够模拟长期社交现象同时保持经验验证的行为保真度的基于LLM的框架。", "conclusion": "SALM框架为社交网络模拟提供了一种新的、更高效和稳定的方法，能够更好地理解和模拟人类社交互动的复杂动态。"}}
{"id": "2505.09486", "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity Injection", "authors": ["Seyed Roozbeh Razavi Rohani", "Khashayar Khajavi", "Wesley Chung", "Mo Chen", "Sharan Vaswani"], "abstract": "Loss of plasticity in deep neural networks is the gradual reduction in a model's capacity to incrementally learn and has been identified as a key obstacle to learning in non-stationary problem settings. Recent work has shown that deep linear networks tend to be resilient towards loss of plasticity. Motivated by this observation, we propose Adaptive Linearization (AdaLin), a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss. Unlike prior methods that rely on regularization or periodic resets, AdaLin equips every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow. This adaptive modulation ensures sufficient gradient signal and sustains continual learning without introducing additional hyperparameters or requiring explicit task boundaries. When used with conventional activation functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in mitigating plasticity loss in off-policy reinforcement learning agents. We perform a systematic set of ablations that show that neuron-level adaptation is crucial for good performance and analyze a number of metrics in the network that might be correlated to loss of plasticity.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "Accepted in 4th Conference on Lifelong Learning Agents (CoLLAs), 2025", "pdf_url": "https://arxiv.org/pdf/2505.09486.pdf", "abstract_url": "https://arxiv.org/abs/2505.09486", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为AdaLin的方法，通过动态调整每个神经元的激活函数来缓解深度神经网络在持续学习中的塑性丧失问题。AdaLin通过引入可学习参数和门控机制，在不增加额外超参数或需要明确任务边界的情况下，有效维持了持续学习的能力。", "motivation": "深度神经网络在非平稳问题设置中逐渐丧失增量学习能力（即塑性丧失）是持续学习的主要障碍。本文旨在解决这一问题。", "method": "提出Adaptive Linearization (AdaLin)方法，通过为每个神经元配备可学习参数和门控机制，动态调整激活函数，注入线性以维持梯度流。", "result": "在多个标准基准测试（如Random Label和Permuted MNIST等）及更复杂场景（如类增量学习）中，AdaLin显著提升了性能，并有效缓解了塑性丧失。", "conclusion": "AdaLin通过神经元级别的自适应调整，有效缓解了塑性丧失问题，为持续学习提供了一种无需额外超参数或明确任务边界的解决方案。"}}
