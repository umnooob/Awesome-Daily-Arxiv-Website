{"id": "2504.15304", "title": "Can Machine Learning Agents Deal with Hard Choices?", "authors": ["Kangyu Wang"], "abstract": "Machine Learning ML agents have been increasingly used in decision-making across a wide range of tasks and environments. These ML agents are typically designed to balance multiple objectives when making choices. Understanding how their decision-making processes align with or diverge from human reasoning is essential. Human agents often encounter hard choices, that is, situations where options are incommensurable; neither option is preferred, yet the agent is not indifferent between them. In such cases, human agents can identify hard choices and resolve them through deliberation. In contrast, current ML agents, due to fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot identify hard choices, let alone resolve them. Neither Scalarised Optimisation nor Pareto Optimisation, the two principal MOO approaches, can capture incommensurability. This limitation generates three distinct alignment problems: the alienness of ML decision-making behaviour from a human perspective; the unreliability of preference-based alignment strategies for hard choices; and the blockage of alignment strategies pursuing multiple objectives. Evaluating two potential technical solutions, I recommend an ensemble solution that appears most promising for enabling ML agents to identify hard choices and mitigate alignment problems. However, no known technique allows ML agents to resolve hard choices through deliberation, as they cannot autonomously change their goals. This underscores the distinctiveness of human agency and urges ML researchers to reconceptualise machine autonomy and develop frameworks and methods that can better address this fundamental gap.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "22 pages excluding bibliography, 27 pagas including bibliography, 3 figures", "pdf_url": "https://arxiv.org/pdf/2504.15304.pdf", "abstract_url": "https://arxiv.org/abs/2504.15304", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了机器学习代理在处理难以抉择的情况时的局限性，与人类决策过程的对比，以及可能的解决方案。", "motivation": "解决机器学习代理在面临难以抉择（即选项不可比较）时的决策问题，以及这些决策过程与人类推理的差异。", "method": "分析了多目标优化（MOO）方法的局限性，评估了两种潜在的技术解决方案，并推荐了一种集成解决方案。", "result": "发现当前机器学习代理无法识别或解决难以抉择的情况，这导致了三种不同的对齐问题。推荐的集成解决方案在识别难以抉择的情况和缓解对齐问题方面显示出潜力。", "conclusion": "强调了人类代理的独特性，呼吁机器学习研究者重新构想机器自主性，并开发能够更好地解决这一基本差距的框架和方法。"}}
{"id": "2504.16047", "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "abstract": "Foundation models, trained on vast amounts of data using self-supervised techniques, have emerged as a promising frontier for advancing artificial intelligence (AI) applications in medicine. This study evaluates three different vision-language foundation models (RAD-DINO, CheXagent, and BiomedCLIP) on their ability to capture fine-grained imaging features for radiology tasks. The models were assessed across classification, segmentation, and regression tasks for pneumothorax and cardiomegaly on chest radiographs. Self-supervised RAD-DINO consistently excelled in segmentation tasks, while text-supervised CheXagent demonstrated superior classification performance. BiomedCLIP showed inconsistent performance across tasks. A custom segmentation model that integrates global and local features substantially improved performance for all foundation models, particularly for challenging pneumothorax segmentation. The findings highlight that pre-training methodology significantly influences model performance on specific downstream tasks. For fine-grained segmentation tasks, models trained without text supervision performed better, while text-supervised models offered advantages in classification and interpretability. These insights provide guidance for selecting foundation models based on specific clinical applications in radiology.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.16047.pdf", "abstract_url": "https://arxiv.org/abs/2504.16047", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本研究评估了三种视觉语言基础模型（RAD-DINO、CheXagent和BiomedCLIP）在放射学任务中捕捉细粒度成像特征的能力。研究发现，预训练方法显著影响模型在特定下游任务中的表现，为放射学中的临床应用选择基础模型提供了指导。", "motivation": "评估视觉语言基础模型在放射学任务中的表现，以指导临床应用中模型的选择。", "method": "评估了三种不同的视觉语言基础模型在胸部X光片的分类、分割和回归任务上的表现，并开发了一个整合全局和局部特征的自定义分割模型。", "result": "RAD-DINO在分割任务中表现最佳，CheXagent在分类任务中表现优异，而BiomedCLIP的表现则不一致。自定义分割模型显著提高了所有基础模型的性能。", "conclusion": "预训练方法对模型在特定下游任务中的表现有显著影响。无文本监督训练的模型在细粒度分割任务中表现更好，而文本监督模型在分类和可解释性方面具有优势。"}}
{"id": "2504.16082", "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "authors": ["Ziqi Pang", "Yu-Xiong Wang"], "abstract": "We propose MR. Video, an agentic long video understanding framework that demonstrates the simple yet effective MapReduce principle for processing long videos: (1) Map: independently and densely perceiving short video clips, and (2) Reduce: jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed short video perception without being limited by context length. Compared with existing video agents that typically rely on sequential key segment selection, the Map operation enables simpler and more scalable sequence parallel perception of short video segments. Its Reduce step allows for more comprehensive context aggregation and reasoning, surpassing explicit key segment retrieval. This MapReduce principle is applicable to both VLMs and video agents, and we use LLM agents to validate its effectiveness.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "Preprint", "pdf_url": "https://arxiv.org/pdf/2504.16082.pdf", "abstract_url": "https://arxiv.org/abs/2504.16082", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "MR. Video是一个基于MapReduce原则的长视频理解框架，通过独立密集感知短视频片段（Map）和联合聚合所有片段信息（Reduce），有效处理长视频。", "motivation": "解决现有序列到序列视觉语言模型（VLMs）和视频代理在处理长视频时受限于上下文长度和依赖顺序关键段选择的问题。", "method": "采用MapReduce原则：Map阶段独立密集感知短视频片段，Reduce阶段联合聚合信息。", "result": "MR. Video在短视频感知和上下文聚合方面表现优于现有方法，验证了MapReduce原则的有效性。", "conclusion": "MapReduce原则适用于VLMs和视频代理，为长视频理解提供了更简单、可扩展且全面的解决方案。"}}
{"id": "2504.15434", "title": "AGI Is Coming... Right After AI Learns to Play Wordle", "authors": ["Sarath Shekkizhar", "Romain Cosentino"], "abstract": "This paper investigates multimodal agents, in particular, OpenAI's Computer-User Agent (CUA), trained to control and complete tasks through a standard computer interface, similar to humans. We evaluated the agent's performance on the New York Times Wordle game to elicit model behaviors and identify shortcomings. Our findings revealed a significant discrepancy in the model's ability to recognize colors correctly depending on the context. The model had a $5.36\\%$ success rate over several hundred runs across a week of Wordle. Despite the immense enthusiasm surrounding AI agents and their potential to usher in Artificial General Intelligence (AGI), our findings reinforce the fact that even simple tasks present substantial challenges for today's frontier AI models. We conclude with a discussion of the potential underlying causes, implications for future development, and research directions to improve these AI systems.", "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15434.pdf", "abstract_url": "https://arxiv.org/abs/2504.15434", "categories": ["Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文研究了多模态代理，特别是OpenAI的计算机用户代理（CUA），在类似人类的标准化计算机界面上完成任务的能力。通过评估代理在《纽约时报》Wordle游戏中的表现，揭示了模型在识别颜色方面的显著差异，成功率仅为5.36%。研究强调了即使是简单任务对当前前沿AI模型也构成重大挑战，并对潜在原因、未来发展的影响及改进方向进行了讨论。", "motivation": "探索多模态代理（如OpenAI的CUA）在标准化计算机界面上完成任务的能力，特别是在识别颜色等简单任务中的表现，以评估当前AI模型在实现人工通用智能（AGI）方面的进展和挑战。", "method": "通过在《纽约时报》Wordle游戏中评估OpenAI的CUA代理的表现，分析模型行为和识别其不足。", "result": "模型在识别颜色方面存在显著差异，成功率仅为5.36%，表明即使是简单任务对当前AI模型也构成重大挑战。", "conclusion": "研究强调了当前AI模型在实现AGI方面的局限性，提出了改进方向和未来研究的潜在路径，以克服这些挑战。"}}
{"id": "2504.15313", "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind", "authors": ["Yajie Yu", "Yue Feng"], "abstract": "Multi-agents has exhibited significant intelligence in real-word simulations with Large language models (LLMs) due to the capabilities of social cognition and knowledge retrieval. However, existing research on agents equipped with effective cognition chains including reasoning, planning, decision-making and reflecting remains limited, especially in the dynamically interactive scenarios. In addition, unlike human, prompt-based responses face challenges in psychological state perception and empirical calibration during uncertain gaming process, which can inevitably lead to cognition bias. In light of above, we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework characterized by systematically acquiring intentions of others and adaptively optimizing irrational strategies for continual enhancement. Specifically, PolicyEvol-Agent first obtains reflective expertise patterns and then integrates a range of cognitive operations with Theory of Mind alongside internal and external perspectives. Simulation results, outperforming RL-based models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent for final gaming victory. Moreover, the policy evolution mechanism reveals the effectiveness of dynamic guideline adjustments in both automatic and human evaluation.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15313.pdf", "abstract_url": "https://arxiv.org/abs/2504.15313", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "PolicyEvol-Agent是一个基于大型语言模型（LLMs）的多智能体框架，旨在通过环境感知和自我意识进化策略，特别是在动态交互场景中。", "motivation": "解决现有智能体在动态交互场景中认知链（包括推理、规划、决策和反思）的不足，以及在不确定游戏过程中心理状态感知和经验校准的挑战。", "method": "引入PolicyEvol-Agent框架，通过系统获取他人意图和自适应优化非理性策略，结合心智理论（Theory of Mind）和内外部视角进行认知操作。", "result": "模拟结果显示，PolicyEvol-Agent在最终游戏胜利上优于基于强化学习（RL）的模型和基于智能体的方法，政策进化机制在自动和人类评估中均显示出动态指南调整的有效性。", "conclusion": "PolicyEvol-Agent通过动态指南调整和心智理论的结合，显著提升了智能体在复杂交互环境中的表现，为多智能体系统的进一步发展提供了新的方向。"}}
{"id": "2504.15801", "title": "A closer look at how large language models trust humans: patterns and biases", "authors": ["Valeria Lerman", "Yaniv Dover"], "abstract": "As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15801.pdf", "abstract_url": "https://arxiv.org/abs/2504.15801", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文研究了大型语言模型（LLMs）与人类在决策情境中的信任动态，特别是LLMs如何基于人类的能力、善意和诚信这三个主要信任维度来发展有效信任。通过43,200次模拟实验，发现LLMs的信任发展与人类相似，但在某些情况下会受到年龄、宗教和性别等人口统计变量的偏见影响，尤其是在金融场景中。", "motivation": "随着LLMs和基于LLM的代理在决策情境中与人类的互动日益增多，理解人类与AI代理之间的信任动态成为一个核心问题。现有文献主要研究人类如何信任AI代理，而对LLM-based代理如何发展对人类的有效信任知之甚少。", "method": "利用已建立的行为理论，开发了一种方法，研究LLMs的信任是否依赖于人类的三个主要信任维度：能力、善意和诚信，并研究人口统计变量如何影响有效信任。在43,200次模拟实验中，对五种流行的语言模型在五种不同情境下进行了测试。", "result": "发现LLM的信任发展与人类的信任发展总体相似。在大多数情况下，LLM的信任强烈依赖于信任worthiness，但在某些情况下，也会受到年龄、宗教和性别等人口统计变量的偏见影响，尤其是在金融场景中。不同模型在估计信任时表现出差异，有时信任worthiness和人口统计因素对有效信任的预测力较弱。", "conclusion": "这些发现呼吁更好地理解AI对人类信任的动态，并监控偏见和信任发展模式，以防止在信任敏感的AI应用中产生 unintended 和潜在有害的结果。"}}
{"id": "2504.15457", "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models", "authors": ["Paresh Chaudhary", "Yancheng Liang", "Daphne Chen", "Simon S. Du", "Natasha Jaques"], "abstract": "Being able to cooperate with new people is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is one avenue for searching for such data and ensuring that agents are robust. However, it is difficult to apply in the cooperative setting because adversarial policies intentionally learn to sabotage the task instead of simulating valid cooperation partners. To address this challenge, we propose a novel strategy for overcoming self-sabotage that combines a pre-trained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method GOAT: Generative Online Adversarial Training. In this framework, the GOAT dynamically searches for and generates coordination strategies where the learning policy -- the Cooperator agent -- underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by updating only the generative model's embedding while keeping its parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state-of-the-art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15457.pdf", "abstract_url": "https://arxiv.org/abs/2504.15457", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为GOAT的新策略，通过结合预训练的生成模型和对抗训练来改进人类与AI的协作能力，旨在解决在协作环境中对抗训练难以应用的问题。", "motivation": "解决在协作环境中对抗训练难以应用的问题，因为对抗策略会故意破坏任务而不是模拟有效的协作伙伴。", "method": "提出GOAT（Generative Online Adversarial Training）策略，结合预训练的生成模型模拟有效的协作代理策略，并通过对抗训练最大化遗憾。", "result": "在Overcooked基准测试中，GOAT与真实人类伙伴的协作表现达到了最先进的水平，证明了其在泛化到多样化人类行为方面的有效性。", "conclusion": "GOAT策略通过动态搜索和生成协作策略，使合作者代理暴露于各种具有挑战性的交互场景中，从而实现了更好的泛化能力。"}}
{"id": "2504.15552", "title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models", "authors": ["Gengxian Cao", "Fengyuan Li", "Hong Duan", "Ye Yang", "Bofeng Wang", "Donghe Li"], "abstract": "This paper introduces a novel multi-Agent framework that automates the end to end production of Qinqiang opera by integrating Large Language Models , visual generation, and Text to Speech synthesis. Three specialized agents collaborate in sequence: Agent1 uses an LLM to craft coherent, culturally grounded scripts;Agent2 employs visual generation models to render contextually accurate stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally expressive vocal performances. In a case study on Dou E Yuan, the system achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence, and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point improvement over a Single Agent baseline. Ablation experiments demonstrate that removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively, underscoring the value of modular collaboration. This work showcases how AI driven pipelines can streamline and scale the preservation of traditional performing arts, and points toward future enhancements in cross modal alignment, richer emotional nuance, and support for additional opera genres.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "17 pages,7 figures,1 tables", "pdf_url": "https://arxiv.org/pdf/2504.15552.pdf", "abstract_url": "https://arxiv.org/abs/2504.15552", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种新颖的多代理框架，通过整合大型语言模型、视觉生成和文本到语音合成技术，自动化生产秦腔歌剧。三个专业代理依次协作：代理1使用LLM制作连贯、文化根基深厚的剧本；代理2利用视觉生成模型渲染上下文准确的舞台场景；代理3通过TTS生成同步、情感丰富的语音表演。在《窦娥冤》的案例研究中，该系统在剧本忠实度、视觉连贯性和语音准确性方面分别获得专家评分3.8、3.5和3.8，总分3.6，比单代理基线提高了0.3分。消融实验表明，移除代理2或代理3分别导致分数下降0.4和0.5分，强调了模块化协作的价值。", "motivation": "解决传统表演艺术保护和规模化生产中的自动化问题", "method": "采用多代理框架，整合大型语言模型、视觉生成和文本到语音合成技术", "result": "在《窦娥冤》案例中，系统在剧本忠实度、视觉连贯性和语音准确性方面获得较高评分，总分比单代理基线提高0.3分", "conclusion": "AI驱动的流水线可以简化和规模化传统表演艺术的保护，未来可在跨模态对齐、情感丰富度和支持更多歌剧类型方面进行增强"}}
{"id": "2504.15699", "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation", "authors": ["Ning Wang", "Zihan Yan", "Weiyang Li", "Chuan Ma", "He Chen", "Tao Xiang"], "abstract": "Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "9 pages", "pdf_url": "https://arxiv.org/pdf/2504.15699.pdf", "abstract_url": "https://arxiv.org/abs/2504.15699", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新颖的输入调节框架和EAsafetyBench安全基准，专门为保障嵌入式代理的行为安全而设计，包括分类法定义、数据集整理、调节器架构等，并通过Pinpoint方案提高了调节效率。", "motivation": "解决嵌入式代理在广泛部署前必须确保其行为安全的问题，现有研究缺乏专门针对嵌入式代理的安全基准和输入调节方法。", "method": "提出了一个全面的输入调节框架，包括创建EAsafetyBench安全基准和Pinpoint提示解耦输入调节方案，利用掩蔽注意力机制隔离功能提示的影响。", "result": "在多样化的基准数据集和模型上进行的大量实验验证了所提方法的可行性和有效性，平均检测准确率达到94.58%，调节处理时间仅为每实例0.002秒。", "conclusion": "研究提出的方法在保障嵌入式代理安全方面表现出色，不仅提高了检测准确率，还显著减少了处理时间，为嵌入式代理的安全部署提供了有效解决方案。"}}
{"id": "2504.15716", "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models", "authors": ["Jie Zhu", "Qian Chen", "Huaixia Dou", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "abstract": "Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to address these challenges through reasoning-augmented supervision and reinforcement learning. Central to our approach is DianJin-R1-Data, a high-quality dataset constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), combining diverse financial reasoning scenarios with verified annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that generates both reasoning steps and final answers. To further refine reasoning quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning method that incorporates dual reward signals: one encouraging structured outputs and another rewarding answer correctness. We evaluate our models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental results show that DianJin-R1 models consistently outperform their non-reasoning counterparts, especially on complex financial tasks. Moreover, on the real-world CCC dataset, our single-call reasoning models match or even surpass the performance of multi-agent systems that require significantly more computational cost. These findings demonstrate the effectiveness of DianJin-R1 in enhancing financial reasoning through structured supervision and reward-aligned learning, offering a scalable and practical solution for real-world applications.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15716.pdf", "abstract_url": "https://arxiv.org/abs/2504.15716", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "DianJin-R1是一个增强大型语言模型在金融领域推理能力的框架，通过推理增强监督和强化学习解决金融任务中的领域知识、精确计算和合规性问题。", "motivation": "解决大型语言模型在金融领域推理中的核心挑战，包括领域特定知识、精确数值计算和严格遵守合规规则。", "method": "使用DianJin-R1-Data高质量数据集，结合推理增强监督和强化学习（特别是Group Relative Policy Optimization方法），对Qwen2.5模型进行微调，生成推理步骤和最终答案。", "result": "DianJin-R1模型在五个基准测试中（三个金融数据集和两个通用推理基准）表现优于非推理对应模型，尤其在复杂金融任务上，且在真实世界CCC数据集上，单次调用推理模型的性能匹配或超过计算成本更高的多代理系统。", "conclusion": "DianJin-R1通过结构化监督和奖励对齐学习有效增强了金融推理能力，为实际应用提供了可扩展和实用的解决方案。"}}
{"id": "2504.15719", "title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences", "authors": ["Anna Karnysheva", "Christian Drescher", "Dietrich Klakow"], "abstract": "As large language models (LLMs) become integral to intelligent user interfaces (IUIs), their role as decision-making agents raises critical concerns about alignment. Although extensive research has addressed issues such as factuality, bias, and toxicity, comparatively little attention has been paid to measuring alignment to preferences, i.e., the relative desirability of different alternatives, a concept used in decision making, economics, and social choice theory. However, a reliable decision-making agent makes choices that align well with user preferences.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15719.pdf", "abstract_url": "https://arxiv.org/abs/2504.15719", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了大型语言模型（LLMs）在智能用户界面（IUIs）中作为决策代理时与用户偏好对齐的问题，强调了在决策制定、经济学和社会选择理论中衡量偏好对齐的重要性。", "motivation": "随着大型语言模型（LLMs）在智能用户界面（IUIs）中的应用日益广泛，其作为决策代理的角色引发了关于对齐问题的关键关注。尽管已有大量研究解决了事实性、偏见和毒性等问题，但对偏好对齐（即不同选择的相对可取性）的测量关注较少。", "method": "本文提出了一种实现理性选择功能的方法，并测量其与用户偏好的对齐程度。", "result": "关键发现包括大型语言模型在决策制定中能够较好地与用户偏好对齐，这对于开发可靠的决策代理至关重要。", "conclusion": "本文的主要结论是，为了确保大型语言模型作为决策代理的可靠性，必须重视并测量其与用户偏好的对齐程度。这为未来的研究和技术开发提供了重要方向。"}}
{"id": "2504.15785", "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "abstract": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2504.15785.pdf", "abstract_url": "https://arxiv.org/abs/2504.15785", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为WALL-E 2.0的无训练“世界对齐”方法，通过从探索轨迹中提取符号知识并编码成可执行代码，以弥补大型语言模型（LLMs）与特定环境动态之间的知识差距。进一步提出了一个基于模型的代理，利用模型预测控制（MPC）框架，通过LLM代理作为前瞻优化器，与神经符号世界模型互动，显著提高了在新环境中的学习效率。在火星（类似Minecraft）和ALFWorld（实体化室内环境）的开放世界挑战中，WALL-E 2.0的表现显著优于现有方法。", "motivation": "解决大型语言模型（LLMs）作为世界模型时，其先验知识与特定环境动态之间的差距问题，以提高LLM代理的性能。", "method": "提出无训练的“世界对齐”方法，从探索轨迹中提取符号知识（如动作规则、知识图和场景图）并编码成可执行代码；进一步提出基于模型的代理WALL-E 2.0，利用模型预测控制（MPC）框架，通过LLM代理作为前瞻优化器与神经符号世界模型互动。", "result": "在火星和ALFWorld的开放世界挑战中，WALL-E 2.0显著优于现有方法，如在火星上成功率超过基线16.1%-51.6%，分数至少提高61.7%；在ALFWorld中，仅4次迭代后即达到98%的成功率。", "conclusion": "WALL-E 2.0通过世界对齐和神经符号学习，显著提高了LLM代理作为世界模型的准确性和在新环境中的学习效率，为构建高效、准确的LLM代理提供了新方向。"}}
{"id": "2504.15545", "title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining", "authors": ["Zizhi Chen", "Xinyu Zhang", "Minghao Han", "Yizhou Liu", "Ziyun Qian", "Weifeng Zhang", "Xukun Zhang", "Jingwei Wei", "Lihua Zhang"], "abstract": "In histopathology, tissue sections are typically stained using common H&E staining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific tissue structures. The rapid advancement of deep learning offers an effective solution for generating virtually stained images, significantly reducing the time and labor costs associated with traditional histochemical staining. However, a new challenge arises in separating the fundamental visual characteristics of tissue sections from the visual differences induced by staining agents. Additionally, virtual staining often overlooks essential pathological knowledge and the physical properties of staining, resulting in only style-level transfer. To address these issues, we introduce, for the first time in virtual staining tasks, a pathological vision-language large model (VLM) as an auxiliary tool. We integrate contrastive learnable prompts, foundational concept anchors for tissue sections, and staining-specific concept anchors to leverage the extensive knowledge of the pathological VLM. This approach is designed to describe, frame, and enhance the direction of virtual staining. Furthermore, we have developed a data augmentation method based on the constraints of the VLM. This method utilizes the VLM's powerful image interpretation capabilities to further integrate image style and structural information, proving beneficial in high-precision pathological diagnostics. Extensive evaluations on publicly available multi-domain unpaired staining datasets demonstrate that our method can generate highly realistic images and enhance the accuracy of downstream tasks, such as glomerular detection and segmentation. Our code is available at:", "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15545.pdf", "abstract_url": "https://arxiv.org/abs/2504.15545", "categories": ["Image and Video Processing (eess.IV)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于病理视觉语言大模型（VLM）的辅助工具，用于解决组织切片虚拟染色中的挑战，通过整合对比可学习提示、组织切片的基础概念锚点和染色特定概念锚点，以及开发基于VLM约束的数据增强方法，显著提高了虚拟染色的真实性和下游任务的准确性。", "motivation": "解决组织切片虚拟染色中难以区分组织基本视觉特征与染色剂引起的视觉差异的问题，以及虚拟染色忽视病理知识和染色物理特性，仅实现风格层面转换的局限性。", "method": "引入病理视觉语言大模型（VLM）作为辅助工具，整合对比可学习提示、组织切片的基础概念锚点和染色特定概念锚点，开发基于VLM约束的数据增强方法。", "result": "在公开的多领域未配对染色数据集上的广泛评估表明，该方法能生成高度真实的图像，并提高下游任务（如肾小球检测和分割）的准确性。", "conclusion": "该方法不仅提高了虚拟染色的真实性和准确性，还为高精度病理诊断提供了有益的工具，代码已公开。"}}
{"id": "2504.15616", "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction", "authors": ["Kai Chen", "Xiaodong Zhao", "Yujie Huang", "Guoyu Fang", "Xiao Song", "Ruiping Wang", "Ziyuan Wang"], "abstract": "The analysis and prediction of agent trajectories are crucial for decision-making processes in intelligent systems, with precise short-term trajectory forecasting being highly significant across a range of applications. Agents and their social interactions have been quantified and modeled by researchers from various perspectives; however, substantial limitations exist in the current work due to the inherent high uncertainty of agent intentions and the complex higher-order influences among neighboring groups. SocialMOIF is proposed to tackle these challenges, concentrating on the higher-order intention interactions among neighboring groups while reinforcing the primary role of first-order intention interactions between neighbors and the target agent. This method develops a multi-order intention fusion model to achieve a more comprehensive understanding of both direct and indirect intention information. Within SocialMOIF, a trajectory distribution approximator is designed to guide the trajectories toward values that align more closely with the actual data, thereby enhancing model interpretability. Furthermore, a global trajectory optimizer is introduced to enable more accurate and efficient parallel predictions. By incorporating a novel loss function that accounts for distance and direction during training, experimental results demonstrate that the model outperforms previous state-of-the-art baselines across multiple metrics in both dynamic and static datasets.", "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)", "comments": "11 pages,6 figures", "pdf_url": "https://arxiv.org/pdf/2504.15616.pdf", "abstract_url": "https://arxiv.org/abs/2504.15616", "categories": ["Machine Learning (cs.LG)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "SocialMOIF是一种用于行人轨迹预测的多阶意图融合模型，专注于邻居群体间的高阶意图交互，并强化邻居与目标代理之间的一阶意图交互的主要作用。通过设计轨迹分布近似器和全局轨迹优化器，以及引入考虑距离和方向的新损失函数，该模型在动态和静态数据集中均优于先前的最先进基线。", "motivation": "解决智能系统中代理轨迹预测的高不确定性和复杂高阶影响问题。", "method": "开发多阶意图融合模型，包括轨迹分布近似器和全局轨迹优化器，并使用新颖的损失函数。", "result": "模型在多个指标上优于先前的最先进基线。", "conclusion": "SocialMOIF通过更全面地理解直接和间接意图信息，提高了轨迹预测的准确性和效率。"}}
{"id": "2504.16062", "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "authors": ["Hardik Shah", "Jiaxu Xing", "Nico Messikommer", "Boyang Sun", "Marc Pollefeys", "Davide Scaramuzza"], "abstract": "Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration.", "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.16062.pdf", "abstract_url": "https://arxiv.org/abs/2504.16062", "categories": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "ForesightNav是一种新颖的探索策略，受人类想象和推理启发，赋予机器人预测未探索区域的上下文信息（如占用和语义细节）的能力，从而高效选择长期导航目标，显著提升在未知环境中的探索效率。", "motivation": "解决自主机器人在未知环境中如何像人类一样利用先验知识进行高效探索的问题。", "method": "采用基于想象的探索策略，预测未探索区域的占用和语义信息，以指导长期导航目标的选取。", "result": "在Structured3D数据集上验证了方法的有效性，展示了准确的占用预测和对未见过场景几何的优异预测能力，探索效率显著提升，PointNav完成率达到100%，ObjectNav的SPL达到67%。", "conclusion": "想象驱动的推理能力可以显著增强自主系统的泛化能力和探索效率，为未来的自主机器人探索提供了新的思路。"}}
{"id": "2504.16073", "title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation", "authors": ["Zhiyuan Hu", "Shiyun Xiong", "Yifan Zhang", "See-Kiong Ng", "Anh Tuan Luu", "Bo An", "Shuicheng Yan", "Bryan Hooi"], "abstract": "Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in three GUI navigation tasks, achieving a 3.4% improvement in single step action accuracy for static environments, along with a around 33% increase in task success rate in one dynamic environment. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.16073.pdf", "abstract_url": "https://arxiv.org/abs/2504.16073", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种在推理时通过过程监督奖励模型引导视觉语言模型（VLM）代理进行GUI导航的方法，以解决现有框架在复杂GUI环境中生成正确动作的困难。", "motivation": "当前的视觉语言模型（VLM）在处理复杂的图形用户界面（GUI）交互任务时，尽管能力有所提升，但仍难以在具有挑战性的GUI环境中生成正确的动作。此外，现有的轨迹级评估和优化技术由于反馈延迟和局部优化问题而效果不佳。", "method": "提出了一种方法，通过在推理时使用奖励模型对VLM代理进行过程监督，以优化每一步的推理动作，从而提高在静态和动态环境中的性能。", "result": "在三个GUI导航任务中，该方法在静态环境中的单步动作准确率提高了3.4%，在一个动态环境中的任务成功率提高了约33%。结合轨迹反射和重试机制后，任务成功率进一步提升。", "conclusion": "通过过程监督奖励模型在推理时引导VLM代理，可以显著提高GUI导航任务的性能，尤其是在动态环境中。这一方法为改进VLM在复杂GUI交互任务中的表现提供了新的方向。"}}
{"id": "2504.15301", "title": "A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations", "authors": ["Zoi Lygizou", "Dimitris Kalles"], "abstract": "Trust management provides an alternative solution for securing open, dynamic, and distributed multi-agent systems, where conventional cryptographic methods prove to be impractical. However, existing trust models face challenges related to agent mobility, changing behaviors, and the cold start problem. To address these issues we introduced a biologically inspired trust model in which trustees assess their own capabilities and store trust data locally. This design improves mobility support, reduces communication overhead, resists disinformation, and preserves privacy. Despite these advantages, prior evaluations revealed limitations of our model in adapting to provider population changes and continuous performance fluctuations. This study proposes a novel algorithm, incorporating a self-classification mechanism for providers to detect performance drops potentially harmful for the service consumers. Simulation results demonstrate that the new algorithm outperforms its original version and FIRE, a well-known trust and reputation model, particularly in handling dynamic trustee behavior. While FIRE remains competitive under extreme environmental changes, the proposed algorithm demonstrates greater adaptability across various conditions. In contrast to existing trust modeling research, this study conducts a comprehensive evaluation of our model using widely recognized trust model criteria, assessing its resilience against common trust-related attacks while identifying strengths, weaknesses, and potential countermeasures. Finally, several key directions for future research are proposed.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15301.pdf", "abstract_url": "https://arxiv.org/abs/2504.15301", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Distributed, Parallel, and Cluster Computing (cs.DC)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种受生物启发的信任模型，用于开放多代理系统，该模型能够有效应对代理性能的快速波动。通过引入自我分类机制，新算法在处理动态受托行为方面优于原有版本及知名信任模型FIRE。", "motivation": "解决开放、动态、分布式多代理系统中信任管理面临的挑战，如代理移动性、行为变化及冷启动问题。", "method": "采用生物启发的信任模型，受托方评估自身能力并本地存储信任数据，引入自我分类机制检测对服务消费者可能有害的性能下降。", "result": "新算法在动态受托行为处理上优于原版本和FIRE模型，展现出更强的适应能力，尤其在应对极端环境变化时。", "conclusion": "提出的信任模型在多种条件下表现出优越的适应性和抗攻击能力，为未来信任模型研究提供了方向。"}}
{"id": "2504.15585", "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Xinfeng Li", "Yifan Jiang", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Tianlin Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Tianwei Zhang", "Xingjun Ma", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Yuval Elovici", "Bhavya Kailkhura", "Bo Li", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Shuicheng Yan", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15585.pdf", "abstract_url": "https://arxiv.org/abs/2504.15585", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文首次提出了'全栈'安全的概念，系统地考虑了大型语言模型（LLMs）从训练、部署到最终商业化的整个过程中的安全问题。与现有的LLM安全调查相比，我们的工作展示了几个独特的优势：全面的视角、广泛的文献支持和独特的见解。", "motivation": "随着LLMs在研究和商业领域的日益突出，其安全性和安全性影响已成为研究人员、公司乃至每个国家日益关注的问题。现有的LLM安全调查主要集中在LLM生命周期的特定阶段，缺乏对整个'生命链'的全面理解。", "method": "本文通过定义LLM的完整生命周期，包括数据准备、预训练、后训练、部署和最终商业化，并基于对800多篇论文的详尽回顾，系统地组织和覆盖了安全问题。", "result": "通过系统的文献分析，我们为每个章节开发了可靠的路线图和视角，并确定了有前途的研究方向，包括数据生成的安全性、对齐技术、模型编辑和基于LLM的代理系统。", "conclusion": "我们的工作为追求未来在这一领域工作的研究人员提供了宝贵的指导，并首次提出了'全栈'安全的概念，以更全面地理解LLM的安全问题。"}}
{"id": "2504.15629", "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "authors": ["Harsh Maheshwari", "Srikanth Tenneti", "Alwarappan Nakkiran"], "abstract": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of Large Language Models (LLMs), revolutionizing information search and consumption. RAG systems combine traditional search capabilities with LLMs to generate comprehensive answers to user queries, ideally with accurate citations. However, in our experience of developing a RAG product, LLMs often struggle with source attribution, aligning with other industry studies reporting citation accuracy rates of only about 74% for popular generative search engines. To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches cross-check generated citations against retrieved articles using methods including keyword + semantic matching, fine tuned model with BERTScore, and a lightweight LLM-based technique. Our experimental results demonstrate a relative improvement of 15.46% in the overall accuracy metrics of our RAG system. This significant enhancement potentially enables a shift from our current larger language model to a relatively smaller model that is approximately 12x more cost-effective and 3x faster in inference time, while maintaining comparable performance. This research contributes to enhancing the reliability and trustworthiness of AI-generated content in information retrieval and summarization tasks which is critical to gain customer trust especially in commercial products.", "subjects": "Information Retrieval (cs.IR); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15629.pdf", "abstract_url": "https://arxiv.org/abs/2504.15629", "categories": ["Information Retrieval (cs.IR)", "Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了CiteFix，一种通过后处理引用校正来提高RAG准确性的方法。", "motivation": "解决大型语言模型在RAG系统中引用准确性不高的问题，以提高生成内容的可靠性和可信度。", "method": "采用关键词+语义匹配、基于BERTScore的微调模型和轻量级LLM技术后处理算法来校正引用。", "result": "实验结果显示，RAG系统的整体准确率指标相对提高了15.46%，同时可能使用更小、更快、成本更低的模型。", "conclusion": "这项研究有助于提高AI生成内容在信息检索和摘要任务中的可靠性和可信度，对商业产品赢得客户信任至关重要。"}}
{"id": "2504.15369", "title": "Solving New Tasks by Adapting Internet Video Knowledge", "authors": ["Calvin Luo", "Zilai Zeng", "Yilun Du", "Chen Sun"], "abstract": "Video generative models demonstrate great promise in robotics by serving as visual planners or as policy supervisors. When pretrained on internet-scale data, such video models intimately understand alignment with natural language, and can thus facilitate generalization to novel downstream behavior through text-conditioning. However, they may not be sensitive to the specificities of the particular environment the agent inhabits. On the other hand, training video models on in-domain examples of robotic behavior naturally encodes environment-specific intricacies, but the scale of available demonstrations may not be sufficient to support generalization to unseen tasks via natural language specification. In this work, we investigate different adaptation techniques that integrate in-domain information with large-scale pretrained video models, and explore the extent to which they enable novel text-conditioned generalization for robotic tasks, while also considering their independent data and resource considerations. We successfully demonstrate across robotic environments that adapting powerful video models with small scales of example data can successfully facilitate generalization to novel behaviors. In particular, we present a novel adaptation strategy, termed Inverse Probabilistic Adaptation, that not only consistently achieves strong generalization performance across robotic tasks and settings, but also exhibits robustness to the quality of adaptation data, successfully solving novel tasks even when only suboptimal in-domain demonstrations are available.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2504.15369.pdf", "abstract_url": "https://arxiv.org/abs/2504.15369", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了通过适应互联网视频知识来解决新任务的方法，特别是在机器人技术中，通过结合大规模预训练视频模型和领域内示例数据，实现了对新颖文本条件行为的泛化。", "motivation": "解决视频生成模型在机器人技术中如何结合互联网规模数据和特定环境信息，以支持通过文本条件泛化到新任务的问题。", "method": "研究了不同的适应技术，特别是提出了一种名为逆概率适应的新策略，以整合领域内信息和大规模预训练视频模型。", "result": "在机器人环境中，通过小规模示例数据适应强大的视频模型，成功实现了对新行为的泛化，逆概率适应策略在不同任务和设置中表现优异，且对适应数据的质量具有鲁棒性。", "conclusion": "适应强大的视频模型与领域内数据结合，可以有效促进机器人任务中的新颖文本条件泛化，逆概率适应策略提供了一种高效且鲁棒的解决方案。"}}
{"id": "2504.15424", "title": "LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study", "authors": ["Nishath Rajiv Ranasinghe", "Shawn M. Jones", "Michal Kucer", "Ayan Biswas", "Daniel O'Malley", "Alexander Buschmann Most", "Selma Liliane Wanna", "Ajay Sreekumar"], "abstract": "Large Language Models (LLMs) are increasingly being leveraged for generating and translating scientific computer codes by both domain-experts and non-domain experts. Fortran has served as one of the go to programming languages in legacy high-performance computing (HPC) for scientific discoveries. Despite growing adoption, LLM-based code translation of legacy code-bases has not been thoroughly assessed or quantified for its usability. Here, we studied the applicability of LLM-based translation of Fortran to C++ as a step towards building an agentic-workflow using open-weight LLMs on two different computational platforms. We statistically quantified the compilation accuracy of the translated C++ codes, measured the similarity of the LLM translated code to the human translated C++ code, and statistically quantified the output similarity of the Fortran to C++ translation.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": "12 pages, 7 figures, 2 tables", "pdf_url": "https://arxiv.org/pdf/2504.15424.pdf", "abstract_url": "https://arxiv.org/abs/2504.15424", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文研究了利用大型语言模型（LLMs）将遗留的FORTRAN代码翻译为C++代码的适用性，作为构建基于开放权重LLMs的代理工作流程的一步。研究在两个不同的计算平台上进行，统计量化了翻译后C++代码的编译准确性、与人工翻译C++代码的相似性，以及FORTRAN到C++翻译的输出相似性。", "motivation": "解决遗留高性能计算（HPC）中科学发现的FORTRAN代码向现代编程语言C++的翻译问题，评估和量化LLM基于代码翻译的可用性。", "method": "使用开放权重的大型语言模型（LLMs）在两个不同的计算平台上进行FORTRAN到C++的代码翻译，统计量化编译准确性、代码相似性和输出相似性。", "result": "研究发现，LLM辅助的FORTRAN到C++代码翻译在编译准确性、与人工翻译代码的相似性以及输出相似性方面表现出一定的效果。", "conclusion": "LLM辅助的代码翻译可以作为将遗留FORTRAN代码迁移到现代编程语言的有效工具，但需要进一步的研究来优化翻译质量和效率。"}}
{"id": "2504.15425", "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL", "authors": ["Songyuan Zhang", "Oswin So", "Mitchell Black", "Zachary Serlin", "Chuchu Fan"], "abstract": "Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Optimization and Control (math.OC)", "comments": "28 pages, 16 figures; Accepted by Robotics: Science and Systems 2025", "pdf_url": "https://arxiv.org/pdf/2504.15425.pdf", "abstract_url": "https://arxiv.org/abs/2504.15425", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Multiagent Systems (cs.MA)", "Optimization and Control (math.OC)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为Def-MARL的新型多智能体强化学习算法，旨在解决多机器人系统中的安全最优控制问题。通过使用分布式epigraph形式，该算法在保持训练稳定性的同时，实现了零约束违反的安全要求。", "motivation": "多机器人系统在协作完成任务时需要保持安全，现有的安全多智能体强化学习算法在训练过程中存在不稳定的问题。", "method": "采用epigraph形式的约束优化方法，证明了集中式epigraph形式问题可以通过每个智能体分布式解决，从而提出了Def-MARL算法。", "result": "在8个不同任务和2个不同模拟器上的实验表明，Def-MARL在整体性能、满足安全约束和训练稳定性方面表现最佳。", "conclusion": "Def-MARL算法不仅在模拟实验中表现出色，还能在真实世界的Crazyflie四旋翼飞行器上安全协调智能体完成复杂协作任务，优于其他方法。"}}
{"id": "2504.15546", "title": "A Framework for Testing and Adapting REST APIs as LLM Tools", "authors": ["Jayachandu Bandlamudi", "Ritwik Chaudhuri", "Neelamadhav Gantayat", "Kushal Mukherjee", "Prerna Agarwal", "Renuka Sindhgatta", "Sameep Mehta"], "abstract": "Large Language Models (LLMs) are enabling autonomous agents to perform complex workflows using external tools or functions, often provided via REST APIs in enterprise systems. However, directly utilizing these APIs as tools poses challenges due to their complex input schemas, elaborate responses, and often ambiguous documentation. Current benchmarks for tool testing do not adequately address these complexities, leading to a critical gap in evaluating API readiness for agent-driven automation. In this work, we present a novel testing framework aimed at evaluating and enhancing the readiness of REST APIs to function as tools for LLM-based agents. Our framework transforms apis as tools, generates comprehensive test cases for the APIs, translates tests cases into natural language instructions suitable for agents, enriches tool definitions and evaluates the agent's ability t correctly invoke the API and process its inputs and responses. To provide actionable insights, we analyze the outcomes of 750 test cases, presenting a detailed taxonomy of errors, including input misinterpretation, output handling inconsistencies, and schema mismatches. Additionally, we classify these test cases to streamline debugging and refinement of tool integrations. This work offers a foundational step toward enabling enterprise APIs as tools, improving their usability in agent-based applications.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15546.pdf", "abstract_url": "https://arxiv.org/abs/2504.15546", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新颖的测试框架，旨在评估和增强REST API作为基于LLM的代理工具的准备工作。该框架通过将API转化为工具、生成全面的测试用例、将测试用例转化为适合代理的自然语言指令、丰富工具定义以及评估代理正确调用API和处理其输入和响应的能力，来解决当前工具测试基准未能充分解决的复杂性问题。通过对750个测试案例的结果分析，本文提供了一种详细的错误分类法，包括输入误解、输出处理不一致和模式不匹配等，并为工具集成的调试和优化提供了分类。", "motivation": "大型语言模型（LLMs）使自主代理能够使用外部工具或功能执行复杂的工作流程，这些工具或功能通常通过企业系统中的REST API提供。然而，直接将这些API作为工具使用会因其复杂的输入模式、精细的响应和往往模糊的文档而面临挑战。当前的工具测试基准未能充分解决这些复杂性，导致在评估API为代理驱动自动化准备就绪方面存在关键差距。", "method": "本文提出的框架通过将API转化为工具、生成全面的测试用例、将测试用例转化为适合代理的自然语言指令、丰富工具定义以及评估代理正确调用API和处理其输入和响应的能力，来评估和增强REST API作为基于LLM的代理工具的准备工作。", "result": "通过对750个测试案例的结果分析，本文提供了一种详细的错误分类法，包括输入误解、输出处理不一致和模式不匹配等，并为工具集成的调试和优化提供了分类。", "conclusion": "这项工作为将企业API作为工具奠定了基础，提高了它们在基于代理的应用中的可用性。"}}
{"id": "2504.15766", "title": "Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction", "authors": ["Tobias Demmler", "Lennart Hartung", "Andreas Tamke", "Thao Dang", "Alexander Hegai", "Karsten Haug", "Lars Mikelsons"], "abstract": "In autonomous driving, accurately predicting the movements of other traffic participants is crucial, as it significantly influences a vehicle's planning processes. Modern trajectory prediction models strive to interpret complex patterns and dependencies from agent and map data. The Motion Transformer (MTR) architecture and subsequent work define the most accurate methods in common benchmarks such as the Waymo Open Motion Benchmark. The MTR model employs pre-generated static intention points as initial goal points for trajectory prediction. However, the static nature of these points frequently leads to misalignment with map data in specific traffic scenarios, resulting in unfeasible or unrealistic goal points. Our research addresses this limitation by integrating scene-specific dynamic intention points into the MTR model. This adaptation of the MTR model was trained and evaluated on the Waymo Open Motion Dataset. Our findings demonstrate that incorporating dynamic intention points has a significant positive impact on trajectory prediction accuracy, especially for predictions over long time horizons. Furthermore, we analyze the impact on ground truth trajectories which are not compliant with the map data or are illegal maneuvers.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.15766.pdf", "abstract_url": "https://arxiv.org/abs/2504.15766", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "在自动驾驶领域，准确预测其他交通参与者的运动对车辆规划至关重要。本研究通过将场景特定的动态意图点集成到运动变换器（MTR）模型中，解决了静态意图点与地图数据不匹配的问题，显著提高了轨迹预测的准确性。", "motivation": "解决静态意图点在特定交通场景中与地图数据不匹配，导致预测目标点不可行或不现实的问题。", "method": "将场景特定的动态意图点集成到MTR模型中，并在Waymo Open Motion Dataset上进行训练和评估。", "result": "动态意图点的引入显著提高了轨迹预测的准确性，尤其是在长时间跨度的预测上。", "conclusion": "动态意图点的集成有效解决了静态意图点的局限性，提高了轨迹预测的准确性和现实性。"}}
{"id": "2504.16078", "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities", "authors": ["Thomas Schmied", "Jörg Bornschein", "Jordi Grau-Moya", "Markus Wulfmeier", "Razvan Pascanu"], "abstract": "The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as $\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2504.16078.pdf", "abstract_url": "https://arxiv.org/abs/2504.16078", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文研究了大型语言模型（LLMs）在决策场景中的表现不佳问题，特别是贪婪性、频率偏差和知行差距三种常见失败模式，并提出通过强化学习（RL）对自我生成的思维链（CoT）理由进行微调来缓解这些缺点。实验表明，RL微调通过增加探索和缩小知行差距来增强LLMs的决策能力。", "motivation": "大型语言模型（LLMs）在各种代理应用中显示出潜力，但在决策场景中表现出次优探索和知行差距，即无法有效利用模型中的知识进行行动。本研究旨在系统地探讨LLMs在决策中表现不佳的原因，并提出改进方法。", "method": "通过强化学习（RL）对自我生成的思维链（CoT）理由进行微调，以缓解贪婪性、频率偏差和知行差距等问题。在多臂老虎机、上下文老虎机和井字棋等实验中验证了方法的有效性。", "result": "实验结果表明，RL微调显著提高了LLMs的决策能力，通过增加探索行为和缩小知行差距。同时，研究还探讨了经典探索机制（如ε-贪婪）和LLM特定方法（如自我纠正和自我一致性）在微调LLMs以进行更有效决策中的应用。", "conclusion": "通过RL微调可以有效地改善LLMs在决策场景中的表现，特别是在增加探索和缩小知行差距方面。这为未来LLMs在更复杂决策任务中的应用提供了新的可能性。"}}
