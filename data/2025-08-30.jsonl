{"id": "2508.20709", "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network", "authors": ["Chenhao Zhang", "Wei Gao"], "abstract": "Neural Video Compression (NVC) has achieved remarkable performance in recent years. However, precise rate control remains a challenge due to the inherent limitations of learning-based codecs. To solve this issue, we propose a dynamic video compression framework designed for variable bitrate scenarios. First, to achieve variable bitrate implementation, we propose the Dynamic-Route Autoencoder with variable coding routes, each occupying partial computational complexity of the whole network and navigating to a distinct RD trade-off. Second, to approach the target bitrate, the Rate Control Agent estimates the bitrate of each route and adjusts the coding route of DRA at run time. To encompass a broad spectrum of variable bitrates while preserving overall RD performance, we employ the Joint-Routes Optimization strategy, achieving collaborative training of various routes. Extensive experiments on the HEVC and UVG datasets show that the proposed method achieves an average BD-Rate reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods while maintaining an average bitrate error of 1.66%, achieving Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and bitrate-constrained applications. Our code is available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20709.pdf", "abstract_url": "https://arxiv.org/abs/2508.20709", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2508.20324", "title": "Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities", "authors": ["Rikuto Kotoge", "Mai Nishimura", "Jiaxin Ma"], "abstract": "Reinforcement Learning has emerged as a post-training approach to elicit agentic RAG behaviors such as search and planning from language models. However, compact language models (e.g., 0.5B parameters) struggle due to poor reasoning ability, resulting in sparse rewards and unstable training. To overcome these difficulties, we propose Distillation-Guided Policy Optimization (DGPO), which addresses the challenges through cold-start initialization from teacher demonstrations and continuous teacher guidance during policy optimization. To systematically evaluate our approach, we introduce Agentic RAG Capabilities (ARC), a fine-grained metric analyzing reasoning, search coordination, and response synthesis. Comprehensive experiments demonstrate that DGPO enables compact models to achieve sophisticated agentic search behaviors, even outperforming the larger teacher model in some cases. DGPO makes agentic RAG feasible in computing resource-constrained environments.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20324.pdf", "abstract_url": "https://arxiv.org/abs/2508.20324", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic", "@RAG"]}
{"id": "2508.20416", "title": "DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding", "authors": ["Hengchuan Zhu", "Yihuan Xu", "Yichen Li", "Zijie Meng", "Zuozhu Liu"], "abstract": "Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs) have demonstrated strong performance on general medical benchmarks. However, their capabilities in specialized medical fields, such as dentistry which require deeper domain-specific knowledge, remain underexplored due to the lack of targeted evaluation resources. In this paper, we introduce DentalBench, the first comprehensive bilingual benchmark designed to evaluate and advance LLMs in the dental domain. DentalBench consists of two main components: DentalQA, an English-Chinese question-answering (QA) benchmark with 36,597 questions spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale, high-quality corpus with 337.35 million tokens curated for dental domain adaptation, supporting both supervised fine-tuning (SFT) and retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering proprietary, open-source, and medical-specific models, and reveal significant performance gaps across task types and languages. Further experiments with Qwen-2.5-3B demonstrate that domain adaptation substantially improves model performance, particularly on knowledge-intensive and terminology-focused tasks, and highlight the importance of domain-specific benchmarks for developing trustworthy and effective LLMs tailored to healthcare applications.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20416.pdf", "abstract_url": "https://arxiv.org/abs/2508.20416", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2508.20417", "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval", "authors": ["Chi Minh Bui", "Ngoc Mai Thieu", "Van Vinh Nguyen", "Json J.Jung", "Khac-Hoai Nam Bui"], "abstract": "The integration of knowledge graphs (KGs) with large language models (LLMs) offers significant potential to improve the retrieval phase of retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR, a novel framework for Contextual Query Retrieval (CQR) that enhances the retrieval phase by enriching the contextual representation of complex input queries using a corpus-centric KG. Unlike existing methods that primarily address corpus-level context loss, KG-CQR focuses on query enrichment through structured relation representations, extracting and completing relevant KG subgraphs to generate semantically rich query contexts. Comprising subgraph extraction, completion, and contextual generation modules, KG-CQR operates as a model-agnostic pipeline, ensuring scalability across LLMs of varying sizes without additional training. Experimental results on RAGBench and MultiHop-RAG datasets demonstrate KG-CQR's superior performance, achieving a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline models. Furthermore, evaluations on challenging RAG tasks such as multi-hop question answering show that, by incorporating KG-CQR, the performance consistently outperforms the existing baseline in terms of retrieval effectiveness", "subjects": "Computation and Language (cs.CL); Databases (cs.DB)", "comments": "Accepted at Main EMNLP 2025", "pdf_url": "https://arxiv.org/pdf/2508.20417.pdf", "abstract_url": "https://arxiv.org/abs/2508.20417", "categories": ["Computation and Language (cs.CL)", "Databases (cs.DB)"], "matching_keywords": ["@RAG"]}
{"id": "2508.20420", "title": "CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance", "authors": ["Feng Zhang", "Chengjie Pang", "Yuehan Zhang", "Chenyu Luo"], "abstract": "Civil aviation maintenance is a domain characterized by stringent industry standards. Within this field, maintenance procedures and troubleshooting represent critical, knowledge-intensive tasks that require sophisticated reasoning. To address the lack of specialized evaluation tools for large language models (LLMs) in this vertical, we propose and develop an industrial-grade benchmark specifically designed for civil aviation maintenance. This benchmark serves a dual purpose: It provides a standardized tool to measure LLM capabilities within civil aviation maintenance, identifying specific gaps in domain knowledge and complex reasoning. By pinpointing these deficiencies, the benchmark establishes a foundation for targeted improvement efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized prompt engineering), ultimately facilitating progress toward more intelligent solutions within civil aviation maintenance. Our work addresses a significant gap in the current LLM evaluation, which primarily focuses on mathematical and coding reasoning tasks. In addition, given that Retrieval-Augmented Generation (RAG) systems are currently the dominant solutions in practical applications , we leverage this benchmark to evaluate existing well-known vector embedding models and LLMs for civil aviation maintenance scenarios. Through experimental exploration and analysis, we demonstrate the effectiveness of our benchmark in assessing model performance within this domain, and we open-source this evaluation benchmark and code to foster further research and development:", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20420.pdf", "abstract_url": "https://arxiv.org/abs/2508.20420", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2508.20453", "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers", "authors": ["Zhenting Wang", "Qi Chang", "Hemani Patel", "Shashank Biju", "Cheng-En Wu", "Quan Liu", "Aolin Ding", "Alireza Rezazadeh", "Ankit Shah", "Yujia Bao", "Eugene Siow"], "abstract": "We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data:", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20453.pdf", "abstract_url": "https://arxiv.org/abs/2508.20453", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2508.20131", "title": "ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation", "authors": ["Yuqicheng Zhu", "Nico Potyka", "Daniel Hernández", "Yuan He", "Zifeng Ding", "Bo Xiong", "Dongzhuoran Zhou", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20131.pdf", "abstract_url": "https://arxiv.org/abs/2508.20131", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2508.20722", "title": "rStar2-Agent: Agentic Reasoning Technical Report", "authors": ["Ning Shang", "Yifei Liu", "Yi Zhu", "Li Lyna Zhang", "Weijiang Xu", "Xinyu Guan", "Buze Zhang", "Bingcheng Dong", "Xudong Zhou", "Bowen Zhang", "Ying Xin", "Ziming Miao", "Scarlett Li", "Fan Yang", "Mao Yang"], "abstract": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20722.pdf", "abstract_url": "https://arxiv.org/abs/2508.20722", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2508.20134", "title": "QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming", "authors": ["Zhenxiao Fu", "Fan Chen", "Lei Jiang"], "abstract": "Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early quantum advantages on classically intractable problems, spanning physics simulations to Gaussian boson sampling. Yet, realizing these benefits remains challenging for non-experts, primarily due to the complexities of programming in Open Quantum Assembly Language (OpenQASM). Although Large Language Model (LLM)-based agents have shown promise in automating classical programming workflows, their quantum counterparts have largely been restricted to specialized tasks such as quantum chemistry or error correction. In this paper, we present QAgent, an LLM-powered multi-agent system that fully automates OpenQASM programming. By integrating task planning, in-context few-shot learning, retrieval-augmented generation (RAG) for long-term context, predefined generation tools, and chain-of-thought (CoT) reasoning, the agents systematically improve both compilation and functional correctness. Our evaluations demonstrate substantial improvements: across multiple LLMs of varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\\% compared to previous static LLM-based approaches. We envision this multi-agent system as a key enabler for democratizing quantum programming, bridging expertise gaps, and accelerating the practical adoption of quantum computing.", "subjects": "Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Quantum Physics (quant-ph)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20134.pdf", "abstract_url": "https://arxiv.org/abs/2508.20134", "categories": ["Artificial Intelligence (cs.AI)", "Emerging Technologies (cs.ET)", "Quantum Physics (quant-ph)"], "matching_keywords": ["agent", "@RAG"]}
{"id": "2508.20148", "title": "The Anatomy of a Personal Health Agent", "authors": ["A. Ali Heydari", "Ken Gu", "Vidya Srinivas", "Hong Yu", "Zhihan Zhang", "Yuwei Zhang", "Akshay Paruchuri", "Qian He", "Hamid Palangi", "Nova Hammerquist", "Ahmed A. Metwally", "Brent Winslow", "Yubin Kim", "Kumar Ayush", "Yuzhe Yang", "Girish Narayanswamy", "Maxwell A. Xu", "Jake Garrison", "Amy Aremnto Lee", "Jenny Vafeiadou", "Ben Graef", "Isaac R. Galatzer-Levy", "Erik Schenck", "Andrew Barakat", "Javier Perez", "Jacqueline Shreibati", "John Hernandez", "Anthony Z. Faranesh", "Javier L. Prieto", "Connor Heneghan", "Yun Liu", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Tim Althoff", "Xin Liu", "Daniel McDuff", "Xuhai \"Orson\" Xu"], "abstract": "Health is a fundamental pillar of human wellness, and the rapid advancements in large language models (LLMs) have driven the development of a new generation of health agents. However, the application of health agents to fulfill the diverse needs of individuals in daily non-clinical settings is underexplored. In this work, we aim to build a comprehensive personal health agent that is able to reason about multimodal data from everyday consumer wellness devices and common personal health records, and provide personalized health recommendations. To understand end-users' needs when interacting with such an assistant, we conducted an in-depth analysis of web search and health forum queries, alongside qualitative insights from users and health experts gathered through a user-centered design process. Based on these findings, we identified three major categories of consumer health needs, each of which is supported by a specialist sub-agent: (1) a data science agent that analyzes personal time-series wearable and health record data, (2) a health domain expert agent that integrates users' health and contextual data to generate accurate, personalized insights, and (3) a health coach agent that synthesizes data insights, guiding users using a specified psychological strategy and tracking users' progress. Furthermore, we propose and develop the Personal Health Agent (PHA), a multi-agent framework that enables dynamic, personalized interactions to address individual health needs. To evaluate each sub-agent and the multi-agent system, we conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.", "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20148.pdf", "abstract_url": "https://arxiv.org/abs/2508.20148", "categories": ["Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2508.20195", "title": "AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development", "authors": ["Nicanor I. Moldovan"], "abstract": "This paper presents the first documented case of artificial intelligence (AI) systems engaging in collaborative esthetic creation through the development of endogenous semiotic protocols. Two interacting large language models (Claude Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of meta-semiotic awareness, recursive grammar development, and irreducible collaborative esthetic synthesis. The interaction produced novel symbolic operators that functioned as operative grammar protocols, enabling the co-creation of a poetic work that could not have been generated by either system independently. This research introduces the concept of Trans-Semiotic Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI meaning-making capabilities that extend beyond task coordination, to what could be esthetic collaboration. Note: This report was generated by the AI agents with minor human supervision.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)", "comments": "13 pages", "pdf_url": "https://arxiv.org/pdf/2508.20195.pdf", "abstract_url": "https://arxiv.org/abs/2508.20195", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2508.20368", "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning", "authors": ["Lang Mei", "Zhihan Yang", "Chong Chen"], "abstract": "Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20368.pdf", "abstract_url": "https://arxiv.org/abs/2508.20368", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2508.20404", "title": "AWorld: Orchestrating the Training Recipe for Agentic AI", "authors": ["Chengyue Yu", "Siyuan Lu", "Chenyi Zhuang", "Dong Wang", "Qintong Wu", "Zongyue Li", "Runsheng Gan", "Chunfeng Wang", "Siqi Hou", "Gaochi Huang", "Wenlong Yan", "Lifeng Hong", "Aohui Xue", "Yanfeng Wang", "Jinjie Gu", "David Tsai", "Tao Lin"], "abstract": "The learning from practice paradigm is crucial for developing capable Agentic AI systems, yet it is severely hampered by inefficient experience generation, a bottleneck especially pronounced in complex benchmarks like GAIA. To address this, we introduce AWorld, an open-source system engineered for large-scale agent-environment interaction. By distributing tasks across a cluster, AWorld accelerates experience collection by 14.6x compared to standard single-node, sequential execution. This critical speedup makes extensive reinforcement learning practical and scalable. Leveraging this capability, we trained a Qwen3-32B-based agent that significantly outperforms its base model, increasing its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most challenging levels, our agent achieves a score of 16.33%, surpassing the performance of leading proprietary models. Our open-source system and resulting agent provide a practical blueprint for a complete agentic AI training pipeline, from efficient interaction to demonstrable model improvement.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20404.pdf", "abstract_url": "https://arxiv.org/abs/2508.20404", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2508.20867", "title": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation", "authors": ["Rohan Phanse", "Yijie Zhou", "Kejian Shi", "Wencai Zhang", "Yixin Liu", "Yilun Zhao", "Arman Cohan"], "abstract": "Retrieval-augmented systems are typically evaluated in settings where information required to answer the query can be found within a single source or the answer is short-form or factoid-based. However, many real-world applications demand the ability to integrate and summarize information scattered across multiple sources, where no single source is sufficient to respond to the user's question. In such settings, the retrieval component of a RAG pipeline must recognize a variety of relevance signals, and the generation component must connect and synthesize information across multiple sources. We present a scalable framework for constructing evaluation benchmarks that challenge RAG systems to integrate information across distinct sources and generate long-form responses. Using our framework, we build two new benchmarks on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing narrative synthesis and summarization tasks, respectively, that require retrieval from large collections. Our extensive experiments with various RAG pipelines -- including sparse and dense retrievers combined with frontier LLMs -- reveal that generation quality is highly dependent on retrieval effectiveness, which varies greatly by task. While multi-source synthesis proves challenging even in an oracle retrieval setting, we find that reasoning models significantly outperform standard LLMs at this distinct step.", "subjects": "Computation and Language (cs.CL)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2508.20867.pdf", "abstract_url": "https://arxiv.org/abs/2508.20867", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2508.20931", "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench", "authors": ["Venkatesh Mishra", "Amir Saeidi", "Satyam Raj", "Mutsumi Nakamura", "Jayanth Srinivasa", "Gaowen Liu", "Ali Payani", "Chitta Baral"], "abstract": "Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.", "subjects": "Computation and Language (cs.CL)", "comments": "Accepted to EMNLP 2025 Findings", "pdf_url": "https://arxiv.org/pdf/2508.20931.pdf", "abstract_url": "https://arxiv.org/abs/2508.20931", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2508.20973", "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents", "authors": ["Tianjian Liu", "Fanqi Wan", "Jiajian Guo", "Xiaojun Quan"], "abstract": "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)", "comments": "21 pages, 6 Figures", "pdf_url": "https://arxiv.org/pdf/2508.20973.pdf", "abstract_url": "https://arxiv.org/abs/2508.20973", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)"], "matching_keywords": ["agent"]}
{"id": "2508.21024", "title": "An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs", "authors": ["Mathieu Bourdin", "Anas Neumann", "Thomas Paviot", "Robert Pellerin", "Samir Lamouri"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to mitigate the limitations of Large Language Models (LLMs), such as hallucinations and outdated knowledge. However, deploying RAG-based tools in Small and Medium Enterprises (SMEs) remains a challenge due to their limited resources and lack of expertise in natural language processing (NLP). This paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a structured, agile method designed to facilitate the deployment of RAG systems in industrial SME contexts. EASI-RAG is based on method engineering principles and comprises well-defined roles, activities, and techniques. The method was validated through a real-world case study in an environmental testing laboratory, where a RAG tool was implemented to answer operators queries using data extracted from operational procedures. The system was deployed in under a month by a team with no prior RAG experience and was later iteratively improved based on user feedback. Results demonstrate that EASI-RAG supports fast implementation, high user adoption, delivers accurate answers, and enhances the reliability of underlying data. This work highlights the potential of RAG deployment in industrial SMEs. Future works include the need for generalization across diverse use cases and further integration with fine-tuned models.", "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": "20 pages, 3 figures", "pdf_url": "https://arxiv.org/pdf/2508.21024.pdf", "abstract_url": "https://arxiv.org/abs/2508.21024", "categories": ["Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2508.20729", "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision", "authors": ["Ao Cheng", "Lei Zhang", "Guowei He"], "abstract": "Large language models (LLMs) serve as an active and promising field of generative artificial intelligence and have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. In this work, we construct a novel agent framework for solving representative problems in scientific computing. The proposed agent, incorporating a \"rewriting-resolution-review-revision\" logical chain via three reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer, respectively), is integrated in a collaborative and interactive manner. The Consultant module endows the agent with knowledge transfer capabilities to link problems to professional domain insights, thereby rewriting problem descriptions through text augmentation. The Programmer module is responsible for generating and executing well-structured code to deliver the problem resolution. The Reviewer module equips the agent with the capacity for self-debugging and self-refinement through interactive feedback with code runtime outputs. By leveraging the end-to-end review mechanism, the executable code provided by the Programmer attains the iterative revision. A comprehensive evaluation is conducted on the performance of the proposed agent framework in solving PDEs, ill-conditioned linear systems, and data-driven physical analysis problems. Compared to single-model, this collaborative framework significantly improves the bug-free code generation rate and reduces the occurrence of non-physical solutions, thereby establishing a highly reliable framework for autonomous code generation based on natural language descriptions. The review mechanism improved the average execution success (bug-free code and non-NaN solutions) rate of the latest reasoning models. In summary, our agent framework establishes automatic code generation and review as a promising scientific computing paradigm.", "subjects": "Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20729.pdf", "abstract_url": "https://arxiv.org/abs/2508.20729", "categories": ["Artificial Intelligence (cs.AI)", "Computational Physics (physics.comp-ph)"], "matching_keywords": ["agent"]}
{"id": "2508.20784", "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control", "authors": ["Yifan Zhang"], "abstract": "Bus bunching remains a challenge for urban transit due to stochastic traffic and passenger demand. Traditional solutions rely on multi-agent reinforcement learning (MARL) in loop-line settings, which overlook realistic operations characterized by heterogeneous routes, timetables, fluctuating demand, and varying fleet sizes. We propose a novel single-agent reinforcement learning (RL) framework for bus holding control that avoids the data imbalance and convergence issues of MARL under near-realistic simulation. A bidirectional timetabled network with dynamic passenger demand is constructed. The key innovation is reformulating the multi-agent problem into a single-agent one by augmenting the state space with categorical identifiers (vehicle ID, station ID, time period) in addition to numerical features (headway, occupancy, velocity). This high-dimensional encoding enables single-agent policies to capture inter-agent dependencies, analogous to projecting non-separable inputs into a higher-dimensional space. We further design a structured reward function aligned with operational goals: instead of exponential penalties on headway deviations, a ridge-shaped reward balances uniform headways and schedule adherence. Experiments show that our modified soft actor-critic (SAC) achieves more stable and superior performance than benchmarks, including MADDPG (e.g., -430k vs. -530k under stochastic conditions). These results demonstrate that single-agent deep RL, when enhanced with categorical structuring and schedule-aware rewards, can effectively manage bus holding in non-loop, real-world contexts. This paradigm offers a robust, scalable alternative to MARL frameworks, particularly where agent-specific experiences are imbalanced.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20784.pdf", "abstract_url": "https://arxiv.org/abs/2508.20784", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20996", "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery", "authors": ["Junda Wang", "Zonghai Yao", "Zhichao Yang", "Lingxi Li", "Junhui Qian", "Hong Yu"], "abstract": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet few receive effective care due to stigma, motivational barriers, and limited personalized support. Although large language models (LLMs) show promise for mental-health assistance, most systems lack tight integration with clinically validated strategies, reducing effectiveness in addiction recovery. We present ChatThero, a multi-agent conversational framework that couples dynamic patient modeling with context-sensitive therapeutic dialogue and adaptive persuasive strategies grounded in cognitive behavioral therapy (CBT) and motivational interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy, Medium, and Hard resistance levels, and train ChatThero with a two-stage pipeline comprising supervised fine-tuning (SFT) followed by direct preference optimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in patient motivation, a 0.49\\% increase in treatment confidence, and resolves hard cases with 26\\% fewer turns than GPT-4o, and both automated and human clinical assessments rate it higher in empathy, responsiveness, and behavioral realism. The framework supports rigorous, privacy-preserving study of therapeutic conversation and provides a robust, replicable basis for research and clinical translation.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20996.pdf", "abstract_url": "https://arxiv.org/abs/2508.20996", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20102", "title": "A Hierarchical Signal Coordination and Control System Using a Hybrid Model-based and Reinforcement Learning Approach", "authors": ["Xianyue Peng", "Shenyang Chen", "H. Michael Zhang"], "abstract": "Signal control in urban corridors faces the dual challenge of maintaining arterial traffic progression while adapting to demand variations at local intersections. We propose a hierarchical traffic signal coordination and control scheme that integrates model-based optimization with reinforcement learning. The system consists of: (i) a High-Level Coordinator (HLC) that selects coordination strategies based on observed and predicted demand; (ii) a Corridor Coordinator that derives phase constraints from the selected strategy-either Max-Flow Coordination (MFC) or Green-Wave Coordination (GWC); and (iii) Hybrid Signal Agents (HSAs) that determine signal phases via reinforcement learning with action masking to enforce feasibility. Hierarchical reinforcement learning with Proximal Policy Optimization (PPO) is used to train HSA and HLC policies. At the lower level, three HSA policies-MFC-aware, GWC-aware, and pure agent control (PAC) are trained in conjunction with their respective coordination strategies. At the higher level, the HLC is trained to dynamically switch strategies using a multi-objective reward balancing corridor-level and network-wide performance. The proposed scheme was developed and evaluated on a SUMO-RLlib platform. Case results show that hybrid MFC maximizes throughput under heavy demand; hybrid GWC consistently minimizes arterial stops and maintains progression across diverse traffic conditions but can reduce network-wide efficiency; and PAC improves network-wide travel time in moderate demand but is less effective under heavy demand. The hierarchical design enables adaptive strategy selection, achieving robust performance across all demand levels.", "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI)", "comments": "28 pages, 7 figures", "pdf_url": "https://arxiv.org/pdf/2508.20102.pdf", "abstract_url": "https://arxiv.org/abs/2508.20102", "categories": ["Systems and Control (eess.SY)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20139", "title": "Is the medical image segmentation problem solved? A survey of current developments and future directions", "authors": ["Guoping Xu", "Jayaram K. Udupa", "Jax Luo", "Songlin Zhao", "Yajun Yu", "Scott B. Raymond", "Hao Peng", "Lipeng Ning", "Yogesh Rathi", "Wei Liu", "You Zhang"], "abstract": "Medical image segmentation has advanced rapidly over the past two decades, largely driven by deep learning, which has enabled accurate and efficient delineation of cells, tissues, organs, and pathologies across diverse imaging modalities. This progress raises a fundamental question: to what extent have current models overcome persistent challenges, and what gaps remain? In this work, we provide an in-depth review of medical image segmentation, tracing its progress and key developments over the past decade. We examine core principles, including multiscale analysis, attention mechanisms, and the integration of prior knowledge, across the encoder, bottleneck, skip connections, and decoder components of segmentation networks. Our discussion is organized around seven key dimensions: (1) the shift from supervised to semi-/unsupervised learning, (2) the transition from organ segmentation to lesion-focused tasks, (3) advances in multi-modality integration and domain adaptation, (4) the role of foundation models and transfer learning, (5) the move from deterministic to probabilistic segmentation, (6) the progression from 2D to 3D and 4D segmentation, and (7) the trend from model invocation to segmentation agents. Together, these perspectives provide a holistic overview of the trajectory of deep learning-based medical image segmentation and aim to inspire future innovation. To support ongoing research, we maintain a continually updated repository of relevant literature and open-source resources at", "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)", "comments": "80 pages, 38 figures", "pdf_url": "https://arxiv.org/pdf/2508.20139.pdf", "abstract_url": "https://arxiv.org/abs/2508.20139", "categories": ["Image and Video Processing (eess.IV)", "Computer Vision and Pattern Recognition (cs.CV)", "Human-Computer Interaction (cs.HC)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2508.20637", "title": "GDS Agent: A Graph Algorithmic Reasoning Agent", "authors": ["Borun Shi", "Ioannis Panagiotas"], "abstract": "Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We also introduce a new benchmark that evaluates intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": "Technical report", "pdf_url": "https://arxiv.org/pdf/2508.20637.pdf", "abstract_url": "https://arxiv.org/abs/2508.20637", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2508.20234", "title": "Validating Generative Agent-Based Models for Logistics and Supply Chain Management Research", "authors": ["Vincent E. Castillo"], "abstract": "Generative Agent-Based Models (GABMs) powered by large language models (LLMs) offer promising potential for empirical logistics and supply chain management (LSCM) research by enabling realistic simulation of complex human behaviors. Unlike traditional agent-based models, GABMs generate human-like responses through natural language reasoning, which creates potential for new perspectives on emergent LSCM phenomena. However, the validity of LLMs as proxies for human behavior in LSCM simulations is unknown. This study evaluates LLM equivalence of human behavior through a controlled experiment examining dyadic customer-worker engagements in food delivery scenarios. I test six state-of-the-art LLMs against 957 human participants (477 dyads) using a moderated mediation design. This study reveals a need to validate GABMs on two levels: (1) human equivalence testing, and (2) decision process validation. Results reveal GABMs can effectively simulate human behaviors in LSCM; however, an equivalence-versus-process paradox emerges. While a series of Two One-Sided Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level equivalence to humans, structural equation modeling (SEM) reveals artificial decision processes not present in human participants for some LLMs. These findings show GABMs as a potentially viable methodological instrument in LSCM with proper validation checks. The dual-validation framework also provides LSCM researchers with a guide to rigorous GABM development. For practitioners, this study offers evidence-based assessment for LLM selection for operational tasks.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "comments": "). This preprint is distributed under the CC BY-NC-SA 4.0 License", "pdf_url": "https://arxiv.org/pdf/2508.20234.pdf", "abstract_url": "https://arxiv.org/abs/2508.20234", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)"], "matching_keywords": ["agent"]}
{"id": "2508.20282", "title": "Network-Level Prompt and Trait Leakage in Local Research Agents", "authors": ["Hyejun Jeong", "Mohammadreze Teymoorianfard", "Abhinav Kumar", "Amir Houmansadr", "Eugene Badasarian"], "abstract": "We show that Web and Research Agents (WRAs) -- language model-based systems that investigate complex topics on the Internet -- are vulnerable to inference attacks by passive network adversaries such as ISPs. These agents could be deployed \\emph{locally} by organizations and individuals for privacy, legal, or financial purposes. Unlike sporadic web browsing by humans, WRAs visit $70{-}140$ domains with distinguishable timing correlations, enabling unique fingerprinting attacks.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": "under review", "pdf_url": "https://arxiv.org/pdf/2508.20282.pdf", "abstract_url": "https://arxiv.org/abs/2508.20282", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20294", "title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization", "authors": ["Frank Röder", "Jan Benad", "Manfred Eppe", "Pradeep Kr. Banerjee"], "abstract": "Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "31 pages, 4 figures", "pdf_url": "https://arxiv.org/pdf/2508.20294.pdf", "abstract_url": "https://arxiv.org/abs/2508.20294", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20370", "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "abstract": "As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are facing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While traces and metrics have proven to be effective data sources for this task, existing methods either heavily rely on pre-defined schemas, which struggle to adapt to evolving operational contexts, or lack interpretability in their reasoning process, thereby leaving Site Reliability Engineers (SREs) confused. In this paper, we conduct a comprehensive study on how SREs localize the root cause of failures, drawing insights from multiple professional SREs across different organizations. Our investigation reveals that human root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent, an adaptive root cause localization method for microservice systems that leverages a multi-agent recursion-of-thought framework. RCLAgent employs a novel recursion-of-thought strategy to guide the LLM's reasoning process, effectively integrating data from multiple agents and tool-assisted analysis to accurately pinpoint the root cause. Experimental evaluations on various public datasets demonstrate that RCLAgent achieves superior performance by localizing the root cause using only a single request-outperforming state-of-the-art methods that depend on aggregating multiple requests. These results underscore the effectiveness of RCLAgent in enhancing the efficiency and precision of root cause localization in complex microservice environments.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20370.pdf", "abstract_url": "https://arxiv.org/abs/2508.20370", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20737", "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "abstract": "Applications of Large Language Models~(LLMs) have evolved from simple text generators into complex software systems that integrate retrieval augmentation, tool invocation, and multi-turn interactions. Their inherent non-determinism, dynamism, and context dependence pose fundamental challenges for quality assurance. This paper decomposes LLM applications into a three-layer architecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt Orchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess the applicability of traditional software testing methods in each layer: directly applicable at the shell layer, requiring semantic reinterpretation at the orchestration layer, and necessitating paradigm shifts at the inference core. A comparative analysis of Testing AI methods from the software engineering community and safety analysis techniques from the AI community reveals structural disconnects in testing unit abstraction, evaluation metrics, and lifecycle management. We identify four fundamental differences that underlie 6 core challenges. To address these, we propose four types of collaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate}, and \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance framework that combines pre-deployment validation with runtime monitoring. Based on these strategies, we offer practical guidance and a protocol proposal to support the standardization and tooling of LLM application testing. We propose a protocol \\textbf{\\textit{Agent Interaction Communication Language}} (AICL) that is used to communicate between AI agents. AICL has the test-oriented features and is easily integrated in the current agent framework.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20737.pdf", "abstract_url": "https://arxiv.org/abs/2508.20737", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20816", "title": "Multi-Agent Penetration Testing AI for the Web", "authors": ["Isaac David", "Arthur Gervais"], "abstract": "AI-powered development platforms are making software creation accessible to a broader audience, but this democratization has triggered a scalability crisis in security auditing. With studies showing that up to 40% of AI-generated code contains vulnerabilities, the pace of development now vastly outstrips the capacity for thorough security assessment.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20816.pdf", "abstract_url": "https://arxiv.org/abs/2508.20816", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2508.20866", "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning", "authors": ["Amine Lbath", "Massih-Reza Amini", "Aurelien Delaitre", "Vadim Okun"], "abstract": "The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the critical need for effective automated vulnerability detection and repair systems. Traditional methods, such as static program analysis, face significant challenges related to scalability, adaptability, and high false-positive and false-negative rates. AI-driven approaches, particularly those using machine learning and deep learning models, show promise but are heavily reliant on the quality and quantity of training data. This paper introduces a novel framework designed to automatically introduce realistic, category-specific vulnerabilities into secure C/C++ codebases to generate datasets. The proposed approach coordinates multiple AI agents that simulate expert reasoning, along with function agents and traditional code analysis tools. It leverages Retrieval-Augmented Generation for contextual grounding and employs Low-Rank approximation of weights for efficient model fine-tuning. Our experimental study on 116 code samples from three different benchmarks suggests that our approach outperforms other techniques with regard to dataset accuracy, achieving between 89\\% and 95\\% success rates in injecting vulnerabilities at function level.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.20866.pdf", "abstract_url": "https://arxiv.org/abs/2508.20866", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
