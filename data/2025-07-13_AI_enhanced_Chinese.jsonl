{"id": "2507.07280", "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "abstract": "Interruption plays a crucial role in collaborative learning, shaping group interactions and influencing knowledge construction. AI-driven support can assist teachers in monitoring these interactions. However, most previous work on interruption detection and interpretation has been conducted in single-conversation environments with relatively clean audio. AI agents deployed in classrooms for collaborative learning within small groups will need to contend with multiple concurrent conversations -- in this context, overlapping speech will be ubiquitous, and interruptions will need to be identified in other ways. In this work, we analyze interruption detection in single-conversation and multi-group dialogue settings. We then create a state-of-the-art method for interruption identification that is robust to overlapping speech, and thus could be deployed in classrooms. Further, our work highlights meaningful linguistic and prosodic information about how interruptions manifest in collaborative group interactions. Our investigation also paves the way for future works to account for the influence of overlapping speech from multiple groups when tracking group dialog.", "subjects": "Computation and Language (cs.CL)", "comments": "Long Paper AIED 2025", "pdf_url": "https://arxiv.org/pdf/2507.07280.pdf", "abstract_url": "https://arxiv.org/abs/2507.07280", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文研究了背景语音对协作小组中打断检测的影响，提出了一种在重叠语音环境下仍能有效识别打断的最先进方法，并探讨了打断在协作小组互动中的语言和韵律特征。", "motivation": "解决在多人同时对话的环境中，如何准确检测和解释打断行为的问题，特别是在教室等协作学习场景中，AI代理需要处理重叠语音的挑战。", "method": "分析了单对话和多组对话设置中的打断检测，开发了一种对重叠语音具有鲁棒性的打断识别方法。", "result": "提出了一种在重叠语音环境下仍能有效识别打断的方法，并揭示了打断在协作小组互动中的语言和韵律特征。", "conclusion": "本研究为未来工作在跟踪小组对话时考虑多组重叠语音的影响奠定了基础，为AI在教育领域的应用提供了新的可能性。"}}
{"id": "2507.07307", "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "abstract": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation (RAG) have demonstrated powerful capabilities in generating counterspeech against misinformation. However, current studies rely on limited evidence and offer less control over final outputs. To address these challenges, we propose a Multi-agent Retrieval-Augmented Framework to generate counterspeech against health misinformation, incorporating multiple LLMs to optimize knowledge retrieval, evidence enhancement, and response refinement. Our approach integrates both static and dynamic evidence, ensuring that the generated counterspeech is relevant, well-grounded, and up-to-date. Our method outperforms baseline approaches in politeness, relevance, informativeness, and factual accuracy, demonstrating its effectiveness in generating high-quality counterspeech. To further validate our approach, we conduct ablation studies to verify the necessity of each component in our framework. Furthermore, human evaluations reveal that refinement significantly enhances counterspeech quality and obtains human preference.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07307.pdf", "abstract_url": "https://arxiv.org/abs/2507.07307", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本文提出了一种多代理检索增强框架，用于生成针对健康错误信息的基于证据的反驳言论，通过整合多个大型语言模型优化知识检索、证据增强和响应精炼，显著提高了反驳言论的质量。", "motivation": "当前利用大型语言模型（LLMs）和检索增强生成（RAG）生成反驳错误信息的言论存在证据有限和对最终输出控制不足的问题。", "method": "采用多代理检索增强框架，结合静态和动态证据，利用多个LLMs优化知识检索、证据增强和响应精炼。", "result": "该方法在礼貌性、相关性、信息量和事实准确性方面优于基线方法，并通过消融研究和人类评估验证了各组成部分的必要性和精炼过程对提高反驳言论质量的贡献。", "conclusion": "提出的框架能有效生成高质量的反驳言论，为对抗健康错误信息提供了有力的工具。"}}
{"id": "2507.07115", "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation", "authors": ["Javal Vyas", "Mehmet Mercangoz"], "abstract": "The increasing complexity of modern chemical processes, coupled with workforce shortages and intricate fault scenarios, demands novel automation paradigms that blend symbolic reasoning with adaptive control. In this work, we introduce a unified agentic framework that leverages large language models (LLMs) for both discrete fault-recovery planning and continuous process control within a single architecture. We adopt Finite State Machines (FSMs) as interpretable operating envelopes: an LLM-driven planning agent proposes recovery sequences through the FSM, a Simulation Agent executes and checks each transition, and a Validator-Reprompting loop iteratively refines invalid plans. In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25 states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path success within five reprompts-outperforming open-source LLMs in both accuracy and latency. In Case Study 2, the same framework modulates dual-heater inputs on a laboratory TCLab platform (and its digital twin) to maintain a target average temperature under persistent asymmetric disturbances. Compared to classical PID control, our LLM-based controller attains similar performance, while ablation of the prompting loop reveals its critical role in handling nonlinear dynamics. We analyze key failure modes-such as instruction following lapses and coarse ODE approximations. Our results demonstrate that, with structured feedback and modular agents, LLMs can unify high-level symbolic planningand low-level continuous control, paving the way towards resilient, language-driven automation in chemical engineering.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07115.pdf", "abstract_url": "https://arxiv.org/abs/2507.07115", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了一种利用大型语言模型（LLMs）的统一代理框架，用于化学工程中的离散故障恢复规划和连续过程控制，通过案例研究展示了其在提高准确性和延迟方面的优越性，以及在与传统PID控制相比的类似性能。", "motivation": "现代化学过程的复杂性增加、劳动力短缺和复杂的故障场景需要新的自动化范式，结合符号推理和自适应控制。", "method": "采用有限状态机（FSMs）作为可解释的操作框架，通过LLM驱动的规划代理提出恢复序列，模拟代理执行和检查每个转换，以及验证-重新提示循环迭代优化无效计划。", "result": "在案例研究1中，GPT-4o和GPT-4o-mini在180个随机生成的FSMs上实现了100%的有效路径成功率；在案例研究2中，LLM-based控制器在维持目标平均温度方面与传统PID控制表现相似，且提示循环在处理非线性动态中发挥关键作用。", "conclusion": "研究表明，通过结构化反馈和模块化代理，LLMs可以统一高级符号规划和低级连续控制，为化学工程中的弹性、语言驱动自动化铺平道路。"}}
{"id": "2507.07302", "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation", "authors": ["Ashish Kumar"], "abstract": "Efficient exploration is a well known problem in deep reinforcement learning and this problem is exacerbated in multi-agent reinforcement learning due the intrinsic complexities of such algorithms. There are several approaches to efficiently explore an environment to learn to solve tasks by multi-agent operating in that environment, of which, the idea of expert exploration is investigated in this work. More specifically, this work investigates the application of large-language models as expert planners for efficient exploration in planning based tasks for multiple agents.", "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07302.pdf", "abstract_url": "https://arxiv.org/abs/2507.07302", "categories": ["Artificial Intelligence (cs.AI)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了在多智能体强化学习中应用大型语言模型作为专家规划器，以解决高效探索的问题。", "motivation": "多智能体强化学习中的高效探索是一个已知的难题，由于这类算法的内在复杂性，问题更加严重。", "method": "研究调查了大型语言模型作为专家规划器在多智能体环境中的高效探索应用。", "result": "研究表明，大型语言模型可以作为专家规划器，有效指导多智能体在规划任务中的探索。", "conclusion": "大型语言模型在多智能体路径规划和任务分配中的应用，为解决高效探索问题提供了新的可能性。"}}
{"id": "2507.07441", "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "authors": ["Yu Xia", "Yiran Jenny Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07441.pdf", "abstract_url": "https://arxiv.org/abs/2507.07441", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了SAND框架，通过自我教导的行动审议来提升LLM代理的性能，解决了现有方法在行动空间探索不足和过度承诺于看似合理但次优行动的问题。", "motivation": "解决LLM代理在有限行动空间探索中可能过度承诺于看似合理但次优行动的问题。", "method": "采用自我一致性行动采样和执行引导的行动批评，结合基础模型合成步骤级行动审议思想，并通过迭代方式使用审议轨迹对LLM代理进行微调。", "result": "在两个代表性交互代理任务上，SAND相比初始监督微调平均提升了20%的性能，并优于最先进的代理调优方法。", "conclusion": "SAND框架通过明确的行动审议显著提升了LLM代理的性能，为代理调优提供了新的方向。"}}
{"id": "2507.07257", "title": "Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery", "authors": ["Licong Xu", "Milind Sarkar", "Anto I. Lonappan", "Íñigo Zubeldia", "Pablo Villanueva-Domingo", "Santiago Casas", "Christian Fidler", "Chetana Amancharla", "Ujjwal Tiwari", "Adrian Bayer", "Chadi Ait Ekiou", "Miles Cranmer", "Adrian Dimitrov", "James Fergusson", "Kahaan Gandhi", "Sven Krippendorf", "Andrew Laverick", "Julien Lesgourgues", "Antony Lewis", "Thomas Meier", "Blake Sherwin", "Kristen Surrao", "Francisco Villaescusa-Navarro", "Chi Wang", "Xueqing Xu", "Boris Bolliet"], "abstract": "We present a multi-agent system for automation of scientific research tasks, cmbagent. The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud.", "subjects": "Artificial Intelligence (cs.AI); Instrumentation and Methods for Astrophysics (astro-ph.IM); Computation and Language (cs.CL); Multiagent Systems (cs.MA)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07257.pdf", "abstract_url": "https://arxiv.org/abs/2507.07257", "categories": ["Artificial Intelligence (cs.AI)", "Instrumentation and Methods for Astrophysics (astro-ph.IM)", "Computation and Language (cs.CL)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "介绍了一个名为cmbagent的多智能体系统，用于自动化科学研究任务，该系统由约30个大型语言模型（LLM）智能体组成，采用规划与控制策略协调工作流程，无需人工干预。", "motivation": "解决科学研究任务自动化的问题，特别是在需要高度专业知识和复杂工作流程的领域。", "method": "使用多智能体系统，每个智能体专注于不同的任务（如检索科学论文和代码库、编写代码、解释结果、评审其他智能体的输出），并能在本地执行代码。", "result": "在博士级别的宇宙学任务（使用超新星数据测量宇宙学参数）上成功应用，并在两个基准测试中表现出优于现有最先进的LLMs的性能。", "conclusion": "cmbagent系统展示了自动化科学研究的潜力，其开源代码和演示视频的可用性，以及在HuggingFace上的部署和即将在云上的可用性，为科学社区提供了强大的工具。"}}
{"id": "2507.07426", "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "abstract": "Recent advances in large language models have demonstrated considerable potential in scientific domains such as drug discovery. However, their effectiveness remains constrained when reasoning extends beyond the knowledge acquired during pretraining. Conventional approaches, such as fine-tuning or retrieval-augmented generation, face limitations in either imposing high computational overhead or failing to fully exploit structured scientific data. To overcome these challenges, we propose DrugMCTS, a novel framework that synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. The framework employs five specialized agents tasked with retrieving and analyzing molecular and protein information, thereby enabling structured and iterative reasoning. Without requiring domain-specific fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by over 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially higher recall and robustness compared to both general-purpose LLMs and deep learning baselines. Our results highlight the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in advancing LLM applications for drug discovery.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07426.pdf", "abstract_url": "https://arxiv.org/abs/2507.07426", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "DrugMCTS是一个结合多智能体、RAG和蒙特卡洛树搜索的药物再利用框架，旨在克服大型语言模型在药物发现领域的限制。", "motivation": "解决大型语言模型在超出预训练知识范围时的推理能力限制，以及传统方法在计算开销和利用结构化科学数据方面的不足。", "method": "整合RAG、多智能体协作和蒙特卡洛树搜索，使用五个专门化的智能体来检索和分析分子及蛋白质信息，实现结构化和迭代推理。", "result": "在不进行领域特定微调的情况下，DrugMCTS使Qwen2.5-7B-Instruct的性能超过Deepseek-R1 20%以上，在DrugBank和KIBA数据集上表现出更高的召回率和鲁棒性。", "conclusion": "结构化推理、基于智能体的协作和反馈驱动的搜索机制对于推进LLM在药物发现中的应用至关重要。"}}
{"id": "2507.07445", "title": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley", "authors": ["Weihao Tan", "Changjiu Jiang", "Yu Duan", "Mingcong Lei", "Jiageng Li", "Yitian Hong", "Xinrun Wang", "Bo An"], "abstract": "Autonomous agents navigating human society must master both production activities and social interactions, yet existing benchmarks rarely evaluate these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel benchmark based on Stardew Valley, designed to assess AI agents in open-ended production-living simulations. In StarDojo, agents are tasked to perform essential livelihood activities such as farming and crafting, while simultaneously engaging in social interactions to establish relationships within a vibrant community. StarDojo features 1,000 meticulously curated tasks across five key domains: farming, crafting, exploration, combat, and social interactions. Additionally, we provide a compact subset of 100 representative tasks for efficient model evaluation. The benchmark offers a unified, user-friendly interface that eliminates the need for keyboard and mouse control, supports all major operating systems, and enables the parallel execution of multiple environment instances, making it particularly well-suited for evaluating the most capable foundation agents, powered by multimodal large language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents demonstrate substantial limitations, with the best-performing model, GPT-4.1, achieving only a 12.7% success rate, primarily due to challenges in visual understanding, multimodal reasoning and low-level manipulation. As a user-friendly environment and benchmark, StarDojo aims to facilitate further research towards robust, open-ended agents in complex production-living environments.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07445.pdf", "abstract_url": "https://arxiv.org/abs/2507.07445", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "StarDojo是一个基于《星露谷物语》的新基准测试，旨在评估AI代理在开放式生产生活模拟中的表现，包括农业、手工艺、探索、战斗和社交互动等多个领域。", "motivation": "现有的基准测试很少同时评估AI代理在生产活动和社会互动中的技能，StarDojo旨在填补这一空白。", "method": "StarDojo提供了1,000个精心策划的任务，覆盖五个关键领域，并提供了一个包含100个代表性任务的紧凑子集，用于高效模型评估。", "result": "对最先进的多模态大型语言模型（MLLMs）代理的广泛评估显示，表现最佳的模型GPT-4.1的成功率仅为12.7%，主要由于视觉理解、多模态推理和低级操作方面的挑战。", "conclusion": "StarDojo作为一个用户友好的环境和基准测试，旨在促进在复杂生产生活环境中对稳健、开放式代理的进一步研究。"}}
{"id": "2507.07306", "title": "ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning", "authors": ["Yichen Lu", "Wei Dai", "Jiaen Liu", "Ching Wing Kwok", "Zongheng Wu", "Xudong Xiao", "Ao Sun", "Sheng Fu", "Jianyuan Zhan", "Yian Wang", "Takatomo Saito", "Sicheng Lai"], "abstract": "LLM-based translation agents have achieved highly human-like translation results and are capable of handling longer and more complex contexts with greater efficiency. However, they are typically limited to text-only inputs. In this paper, we introduce ViDove, a translation agent system designed for multimodal input. Inspired by the workflow of human translators, ViDove leverages visual and contextual background information to enhance the translation process. Additionally, we integrate a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge, enabling the agent to perform more accurately and adaptively in real-world scenarios. As a result, ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark for long-form automatic video subtitling and translation, featuring 17 hours of high-quality, human-annotated data. Our code is available here:", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07306.pdf", "abstract_url": "https://arxiv.org/abs/2507.07306", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Audio and Speech Processing (eess.AS)"], "matching_keywords": ["agent"], "AI": {"tldr": "ViDove是一个基于LLM的多模态翻译代理系统，通过结合视觉和上下文背景信息提升翻译质量，并在字幕生成和一般翻译任务中显著优于现有技术。", "motivation": "解决现有LLM-based翻译代理仅限于文本输入的问题，通过引入多模态输入和记忆增强推理来提高翻译的准确性和适应性。", "method": "采用多模态记忆系统和长短时记忆模块，结合领域特定知识，模拟人类翻译工作流程。", "result": "在BLEU分数上提高了28%，在SubER上提高了15%，并引入了新的长视频自动字幕和翻译基准DoveBench。", "conclusion": "ViDove通过多模态上下文和记忆增强推理显著提升了翻译质量，为多模态翻译领域提供了新的研究方向和实践工具。"}}
{"id": "2507.07505", "title": "Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models", "authors": ["Varin Sikka", "Vishal Sikka"], "abstract": "With widespread adoption of transformer-based language models in AI, there is significant interest in the limits of LLMs capabilities, specifically so-called hallucinations, occurrences in which LLMs provide spurious, factually incorrect or nonsensical information when prompted on certain subjects. Furthermore, there is growing interest in agentic uses of LLMs - that is, using LLMs to create agents that act autonomously or semi-autonomously to carry out various tasks, including tasks with applications in the real world. This makes it important to understand the types of tasks LLMs can and cannot perform. We explore this topic from the perspective of the computational complexity of LLM inference. We show that LLMs are incapable of carrying out computational and agentic tasks beyond a certain complexity, and further that LLMs are incapable of verifying the accuracy of tasks beyond a certain complexity. We present examples of both, then discuss some consequences of this work.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "6 pages; to be submitted to AAAI-26 after reviews", "pdf_url": "https://arxiv.org/pdf/2507.07505.pdf", "abstract_url": "https://arxiv.org/abs/2507.07505", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文探讨了基于Transformer的语言模型（LLMs）在计算复杂性和代理任务执行能力上的基本限制，特别是所谓的‘幻觉’现象，即LLMs在特定主题上提供虚假、事实错误或无意义信息的情况。", "motivation": "随着基于Transformer的语言模型在AI中的广泛应用，人们对其能力的限制，尤其是‘幻觉’现象，以及LLMs在自主或半自主代理任务中的应用潜力产生了浓厚兴趣。理解LLMs能执行和不能执行的任务类型变得尤为重要。", "method": "从LLM推理的计算复杂性角度出发，本文展示了LLMs在执行超出一定复杂度的计算和代理任务上的无能，以及LLMs在验证超出一定复杂度任务准确性上的无能。", "result": "研究结果表明，LLMs在执行和验证高复杂度任务方面存在固有的局限性。", "conclusion": "本文的工作揭示了LLMs在计算和代理任务中的基本限制，这对于理解LLMs的实际应用潜力和限制具有重要意义。"}}
{"id": "2507.07509", "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System", "authors": ["Yuanchen Shi", "Longyin Zhang", "Fang Kong"], "abstract": "The growing need for psychological support due to increasing pressures has exposed the scarcity of relevant datasets, particularly in non-English languages. To address this, we propose a framework that leverages limited real-world data and expert knowledge to fine-tune two large language models: Dialog Generator and Dialog Modifier. The Generator creates large-scale psychological counseling dialogues based on predefined paths, which guide system response strategies and user interactions, forming the basis for effective support. The Modifier refines these dialogues to align with real-world data quality. Through both automated and manual review, we construct the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K dialogues across 13 groups, 16 psychological problems, 13 causes, and 12 support focuses. Additionally, we introduce the Comprehensive Agent Dialogue Support System (CADSS), where a Profiler analyzes user characteristics, a Summarizer condenses dialogue history, a Planner selects strategies, and a Supporter generates empathetic responses. The experimental results of the Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate that CADSS achieves state-of-the-art performance on both CPsDD and ESConv datasets.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "10pages,8 figures", "pdf_url": "https://arxiv.org/pdf/2507.07509.pdf", "abstract_url": "https://arxiv.org/abs/2507.07509", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一个框架，利用有限的真实世界数据和专家知识来微调两个大型语言模型：对话生成器和对话修改器，以构建中文心理支持对话数据集（CPsDD），并介绍了综合代理对话支持系统（CADSS）。", "motivation": "解决非英语语言中心理支持相关数据集的稀缺问题，特别是在中文环境下。", "method": "通过对话生成器基于预定义路径创建大规模心理咨询对话，对话修改器调整这些对话以符合真实数据质量，构建CPsDD数据集，并开发CADSS系统。", "result": "构建了包含68K对话的CPsDD数据集，CADSS在策略预测和情感支持对话任务上达到了最先进的性能。", "conclusion": "提出的框架和系统有效地支持了中文心理支持对话的生成和优化，为心理支持领域提供了有价值的资源和工具。"}}
{"id": "2507.07544", "title": "Position: We Need An Algorithmic Understanding of Generative AI", "authors": ["Oliver Eberle", "Thomas McGee", "Hamza Giaffar", "Taylor Webb", "Ida Momennejad"], "abstract": "What algorithms do LLMs actually learn and use to solve problems? Studies addressing this question are sparse, as research priorities are focused on improving performance through scale, leaving a theoretical and empirical gap in understanding emergent algorithms. This position paper proposes AlgEval: a framework for systematic research into the algorithms that LLMs learn and use. AlgEval aims to uncover algorithmic primitives, reflected in latent representations, attention, and inference-time compute, and their algorithmic composition to solve task-specific problems. We highlight potential methodological paths and a case study toward this goal, focusing on emergent search algorithms. Our case study illustrates both the formation of top-down hypotheses about candidate algorithms, and bottom-up tests of these hypotheses via circuit-level analysis of attention patterns and hidden states. The rigorous, systematic evaluation of how LLMs actually solve tasks provides an alternative to resource-intensive scaling, reorienting the field toward a principled understanding of underlying computations. Such algorithmic explanations offer a pathway to human-understandable interpretability, enabling comprehension of the model's internal reasoning performance measures. This can in turn lead to more sample-efficient methods for training and improving performance, as well as novel architectures for end-to-end and multi-agent systems.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "Accepted at ICML 2025 as a Spotlight Position Paper", "pdf_url": "https://arxiv.org/pdf/2507.07544.pdf", "abstract_url": "https://arxiv.org/abs/2507.07544", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出AlgEval框架，旨在系统研究大型语言模型（LLMs）学习并用于解决问题的算法。通过揭示算法原语及其组合方式，该框架寻求理解模型内部的计算机制，为人类可理解的解释性提供途径。", "motivation": "当前研究多集中于通过扩大规模提升性能，而对LLMs学习并使用的算法理解存在理论和实证空白。本文旨在填补这一空白，推动对模型内部计算原理的系统理解。", "method": "提出AlgEval框架，结合自上而下的假设形成与自下而上的电路级分析（如注意力模式和隐藏状态），系统地评估LLMs如何解决任务。", "result": "案例研究展示了通过AlgEval框架识别和验证LLMs中涌现的搜索算法，证明了该框架在揭示模型内部算法方面的潜力。", "conclusion": "AlgEval框架为理解LLMs的内部计算提供了系统方法，不仅促进了模型的可解释性，还可能引导更高效的训练方法和新颖架构设计，减少对资源密集型扩展的依赖。"}}
{"id": "2507.07543", "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora", "authors": ["Chen Amiraz", "Yaroslav Fyodorov", "Elad Haramaty", "Zohar Karnin", "Liane Lewin-Eytan"], "abstract": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability for retrieving and generating answers across languages. Prior work in this context has mostly focused on generation and relied on benchmarks derived from open-domain sources, most notably Wikipedia. In such settings, retrieval challenges often remain hidden due to language imbalances, overlap with pretraining data, and memorized content. To address this gap, we study Arabic-English RAG in a domain-specific setting using benchmarks derived from real-world corporate datasets. Our benchmarks include all combinations of languages for the user query and the supporting document, drawn independently and uniformly at random. This enables a systematic study of multilingual retrieval behavior.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07543.pdf", "abstract_url": "https://arxiv.org/abs/2507.07543", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文研究了阿拉伯语-英语跨语言检索增强生成（RAG）在特定领域设置中的检索偏差问题，使用了来自真实世界企业数据集的基准测试。", "motivation": "解决跨语言检索增强生成（RAG）在阿拉伯语-英语语料库中的检索偏差问题，特别是在特定领域设置中，这些问题在开放领域源（如维基百科）的基准测试中往往被隐藏。", "method": "使用从真实世界企业数据集中独立且均匀随机抽取的用户查询和支持文档的所有语言组合的基准测试，系统地研究多语言检索行为。", "result": "研究发现，在特定领域设置中，跨语言RAG存在检索偏差，这为理解多语言检索行为提供了新的见解。", "conclusion": "本研究强调了在特定领域设置中跨语言RAG的检索偏差问题，为未来的研究和应用提供了重要的参考和方向。"}}
{"id": "2507.07634", "title": "FrugalRAG: Learning to retrieve and reason for multi-hop QA", "authors": ["Abhinav Java", "Srivathsan Koundinyan", "Nagarajan Natarajan", "Amit Sharma"], "abstract": "We consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-of-thought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples).", "subjects": "Computation and Language (cs.CL)", "comments": "Accepted at ICML Workshop: Efficient Systems for Foundation Models", "pdf_url": "https://arxiv.org/pdf/2507.07634.pdf", "abstract_url": "https://arxiv.org/abs/2507.07634", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了FrugalRAG，一种旨在通过改进提示和微调技术来提高多跳问答系统中检索和推理效率的方法。", "motivation": "解决在大型非结构化文档库中回答复杂问题时，检索增强生成（RAG）方法的效率问题，特别是在减少检索搜索次数方面的需求。", "method": "采用改进提示的标准ReAct管道，以及监督和基于强化学习（RL）的微调技术，以提高RAG的效率和节俭性。", "result": "在HotPotQA等基准测试中，该方法以较少的检索搜索次数（近乎半减）实现了竞争力的RAG指标，且训练成本低（1000个示例）。", "conclusion": "研究表明，大规模微调并非提高RAG指标的必要条件，而通过改进提示和微调技术可以在保持性能的同时显著提高效率。"}}
{"id": "2507.07695", "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities", "authors": ["Hruday Markondapatnaikuni", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "abstract": "Fine-tuning is an immensely resource-intensive process when retraining Large Language Models (LLMs) to incorporate a larger body of knowledge. Although many fine-tuning techniques have been developed to reduce the time and computational cost involved, the challenge persists as LLMs continue to grow in size and complexity. To address this, a new approach to knowledge expansion in LLMs is needed. Retrieval-Augmented Generation (RAG) offers one such alternative by storing external knowledge in a database and retrieving relevant chunks to support question answering. However, naive implementations of RAG face significant limitations in scalability and answer accuracy. This paper introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome these limitations. Inspired by the divide-and-conquer paradigm, K2RAG integrates dense and sparse vector search, knowledge graphs, and text summarization to improve retrieval quality and system efficiency. The framework also includes a preprocessing step that summarizes the training data, significantly reducing the training time. K2RAG was evaluated using the MultiHopRAG dataset, where the proposed pipeline was trained on the document corpus and tested on a separate evaluation set. Results demonstrated notable improvements over common naive RAG implementations. K2RAG achieved the highest mean answer similarity score of 0.57, and reached the highest third quartile (Q3) similarity of 0.82, indicating better alignment with ground-truth answers. In addition to improved accuracy, the framework proved highly efficient. The summarization step reduced the average training time of individual components by 93%, and execution speed was up to 40% faster than traditional knowledge graph-based RAG systems. K2RAG also demonstrated superior scalability, requiring three times less VRAM than several naive RAG implementations tested in this study.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "21 pages, 14 figures", "pdf_url": "https://arxiv.org/pdf/2507.07695.pdf", "abstract_url": "https://arxiv.org/abs/2507.07695", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "K^2RAG是一种改进的RAG方法，通过结合密集和稀疏向量搜索、知识图谱和文本摘要技术，提高了LLM问答能力的效果和效率。", "motivation": "解决大型语言模型(LLMs)在知识扩展时微调过程资源密集的问题，以及传统RAG方法在可扩展性和答案准确性上的限制。", "method": "提出KeyKnowledgeRAG (K^2RAG)框架，整合密集和稀疏向量搜索、知识图谱和文本摘要技术，并包括一个预处理步骤来总结训练数据。", "result": "在MultiHopRAG数据集上的评估显示，K^2RAG在答案相似度得分和训练效率上显著优于传统RAG实现，训练时间减少93%，执行速度提高40%。", "conclusion": "K^2RAG框架不仅提高了问答的准确性和效率，还展示了优异的可扩展性，为LLMs的知识扩展提供了一种有效的替代方案。"}}
{"id": "2507.07902", "title": "MIRA: A Novel Framework for Fusing Modalities in Medical RAG", "authors": ["Jinhong Wang", "Tajamul Ashraf", "Zongyan Han", "Jorma Laaksonen", "Rao Mohammad Anwer"], "abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced AI-assisted medical diagnosis, but they often generate factually inconsistent responses that deviate from established medical knowledge. Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external sources, but it presents two key challenges. First, insufficient retrieval can miss critical information, whereas excessive retrieval can introduce irrelevant or misleading content, disrupting model output. Second, even when the model initially provides correct answers, over-reliance on retrieved data can lead to factual errors. To address these issues, we introduce the Multimodal Intelligent Retrieval and Augmentation (MIRA) framework, designed to optimize factual accuracy in MLLM. MIRA consists of two key components: (1) a calibrated Rethinking and Rearrangement module that dynamically adjusts the number of retrieved contexts to manage factual risk, and (2) A medical RAG framework integrating image embeddings and a medical knowledge base with a query-rewrite module for efficient multimodal reasoning. This enables the model to effectively integrate both its inherent knowledge and external references. Our evaluation of publicly available medical VQA and report generation benchmarks demonstrates that MIRA substantially enhances factual accuracy and overall performance, achieving new state-of-the-art results. Code is released at", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "ACM Multimedia 2025", "pdf_url": "https://arxiv.org/pdf/2507.07902.pdf", "abstract_url": "https://arxiv.org/abs/2507.07902", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了MIRA框架，旨在解决多模态大语言模型在医疗诊断中生成事实不一致响应的问题，通过优化检索增强生成技术，提高事实准确性。", "motivation": "多模态大语言模型在医疗诊断中生成的事实不一致响应，以及检索增强生成技术中检索不足或过度检索带来的问题。", "method": "提出了MIRA框架，包括一个校准的重新思考和重排模块，以及一个整合图像嵌入和医疗知识库的医疗RAG框架，带有查询重写模块。", "result": "在公开的医疗VQA和报告生成基准测试中，MIRA显著提高了事实准确性和整体性能，达到了新的最先进水平。", "conclusion": "MIRA框架通过动态调整检索上下文和有效整合内在知识与外部参考，优化了多模态大语言模型在医疗诊断中的事实准确性。"}}
{"id": "2507.07984", "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "authors": ["JingLi Lin", "Chenming Zhu", "Runsen Xu", "Xiaohan Mao", "Xihui Liu", "Tai Wang", "Jiangmiao Pang"], "abstract": "Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is:", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07984.pdf", "abstract_url": "https://arxiv.org/abs/2507.07984", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "OST-Bench是一个旨在评估多模态大型语言模型（MLLMs）在线时空场景理解能力的基准测试，通过1.4k场景和10k问答对，揭示了现有模型在复杂时空推理任务上的不足。", "motivation": "解决现有基准测试在离线设置下评估MLLMs的局限性，更真实地反映现实世界中的具身感知挑战。", "method": "构建OST-Bench基准测试，包含从ScanNet、Matterport3D和ARKitScenes收集的数据，评估MLLMs在在线时空理解任务上的表现。", "result": "现有MLLMs在需要复杂时空推理的任务上表现不佳，特别是在探索范围扩大和记忆增长时，准确率下降。", "conclusion": "研究揭示了提升在线具身推理能力需解决的核心挑战，为未来研究和发展提供了方向和资源。"}}
{"id": "2507.07870", "title": "DocCHA: Towards LLM-Augmented Interactive Online diagnosis System", "authors": ["Xinyi Liu", "Dachun Sun", "Yi R. Fung", "Dilek Hakkani-Tür", "Tarek Abdelzaher"], "abstract": "Despite the impressive capabilities of Large Language Models (LLMs), existing Conversational Health Agents (CHAs) remain static and brittle, incapable of adaptive multi-turn reasoning, symptom clarification, or transparent decision-making. This hinders their real-world applicability in clinical diagnosis, where iterative and structured dialogue is essential. We propose DocCHA, a confidence-aware, modular framework that emulates clinical reasoning by decomposing the diagnostic process into three stages: (1) symptom elicitation, (2) history acquisition, and (3) causal graph construction. Each module uses interpretable confidence scores to guide adaptive questioning, prioritize informative clarifications, and refine weak reasoning links.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07870.pdf", "abstract_url": "https://arxiv.org/abs/2507.07870", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "DocCHA是一个基于大型语言模型（LLMs）的交互式在线诊断系统，旨在通过模块化和信心感知的框架模拟临床推理，解决现有对话式健康代理（CHAs）在适应性多轮推理、症状澄清和透明决策方面的不足。", "motivation": "现有的对话式健康代理（CHAs）在临床诊断中的实际应用受限，因为它们缺乏适应性多轮推理、症状澄清和透明决策能力，而这些在临床诊断中是必不可少的。", "method": "DocCHA采用了一个信心感知的模块化框架，将诊断过程分解为三个阶段：症状引发、病史获取和因果图构建，每个阶段都使用可解释的信心分数来指导适应性提问、优先考虑信息性澄清和优化弱推理链接。", "result": "DocCHA框架通过模拟临床推理，提高了对话式健康代理在临床诊断中的适应性和透明度。", "conclusion": "DocCHA通过其模块化和信心感知的框架，为交互式在线诊断系统提供了一种新的方法，有望提高临床诊断的效率和准确性。"}}
{"id": "2507.07847", "title": "From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems", "authors": ["Youngjoon Jang", "Seongtae Hong", "Junyoung Son", "Sungjin Park", "Chanjun Park", "Heuiseok Lim"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in natural language processing (NLP), improving factual consistency and reducing hallucinations by integrating external document retrieval with large language models (LLMs). However, the effectiveness of RAG is often hindered by coreferential complexity in retrieved documents, introducing ambiguity that disrupts in-context learning. In this study, we systematically investigate how entity coreference affects both document retrieval and generative performance in RAG-based systems, focusing on retrieval relevance, contextual understanding, and overall response quality. We demonstrate that coreference resolution enhances retrieval effectiveness and improves question-answering (QA) performance. Through comparative analysis of different pooling strategies in retrieval tasks, we find that mean pooling demonstrates superior context capturing ability after applying coreference resolution. In QA tasks, we discover that smaller models benefit more from the disambiguation process, likely due to their limited inherent capacity for handling referential ambiguity. With these findings, this study aims to provide a deeper understanding of the challenges posed by coreferential complexity in RAG, providing guidance for improving retrieval and generation in knowledge-intensive AI applications.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07847.pdf", "abstract_url": "https://arxiv.org/abs/2507.07847", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文研究了指代消解在检索增强生成（RAG）系统中的重要性，展示了其如何通过减少检索文档中的指代复杂性来提高检索效果和问答性能。", "motivation": "检索增强生成（RAG）框架在自然语言处理（NLP）中虽能提高事实一致性和减少幻觉，但其效果常因检索文档中的指代复杂性而受到限制，引入歧义，影响上下文学习。", "method": "本研究系统地探讨了实体指代如何影响RAG系统中的文档检索和生成性能，重点关注检索相关性、上下文理解和整体回答质量，并通过比较分析不同的池化策略来评估指代消解的效果。", "result": "研究发现，指代消解提高了检索的有效性并改善了问答性能；在检索任务中，应用指代消解后，均值池化展现出更优的上下文捕捉能力；在问答任务中，较小的模型从消歧过程中获益更多。", "conclusion": "本研究旨在深入理解RAG中指代复杂性带来的挑战，为改进知识密集型AI应用中的检索和生成提供指导。"}}
{"id": "2507.07887", "title": "Automating MD simulations for Proteins using Large language Models: NAMD-Agent", "authors": ["Achuth Chandrasekhar", "Amir Barati Farimani"], "abstract": "Molecular dynamics simulations are an essential tool in understanding protein structure, dynamics, and function at the atomic level. However, preparing high quality input files for MD simulations can be a time consuming and error prone process. In this work, we introduce an automated pipeline that leverages Large Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with python scripting and Selenium based web automation to streamline the generation of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based interface for preparing simulation-ready inputs for NAMD. By integrating Gemini's code generation and iterative refinement capabilities, simulation scripts are automatically written, executed, and revised to navigate CHARMM GUI, extract appropriate parameters, and produce the required NAMD input files. Post processing is performed using additional software to further refine the simulation outputs, thereby enabling a complete and largely hands free workflow. Our results demonstrate that this approach reduces setup time, minimizes manual errors, and offers a scalable solution for handling multiple protein systems in parallel. This automated framework paves the way for broader application of LLMs in computational structural biology, offering a robust and adaptable platform for future developments in simulation automation.", "subjects": "Computation and Language (cs.CL); Computational Engineering, Finance, and Science (cs.CE); Biomolecules (q-bio.BM)", "comments": "34 pages", "pdf_url": "https://arxiv.org/pdf/2507.07887.pdf", "abstract_url": "https://arxiv.org/abs/2507.07887", "categories": ["Computation and Language (cs.CL)", "Computational Engineering, Finance, and Science (cs.CE)", "Biomolecules (q-bio.BM)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种利用大型语言模型（LLMs）自动化生成蛋白质分子动力学（MD）模拟输入文件的流程，旨在减少准备时间和人为错误。", "motivation": "准备高质量的MD模拟输入文件是一个耗时且容易出错的过程，需要自动化解决方案来提高效率和准确性。", "method": "结合Gemini 2.0 Flash大型语言模型、Python脚本和基于Selenium的网页自动化技术，利用CHARMM GUI的网络界面自动生成NAMD模拟所需的输入文件。", "result": "该方法显著减少了设置时间，最小化了手动错误，并为并行处理多个蛋白质系统提供了可扩展的解决方案。", "conclusion": "这一自动化框架为计算结构生物学中LLMs的更广泛应用铺平了道路，为模拟自动化未来的发展提供了一个强大且适应性强的平台。"}}
{"id": "2507.07155", "title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics", "authors": ["Xueqing Xu", "Boris Bolliet", "Adrian Dimitrov", "Andrew Laverick", "Francisco Villaescusa-Navarro", "Licong Xu", "Íñigo Zubeldia"], "abstract": "We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this", "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07155.pdf", "abstract_url": "https://arxiv.org/abs/2507.07155", "categories": ["Instrumentation and Methods for Astrophysics (astro-ph.IM)", "Cosmology and Nongalactic Astrophysics (astro-ph.CO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "评估了9种检索增强生成（RAG）代理配置在105个专为此次研究构建的宇宙学问答对上的表现。", "motivation": "解决在天体物理学领域进行自主科学发现时，如何有效利用检索增强生成技术来提高问答系统的准确性和效率的问题。", "method": "使用了9种不同的检索增强生成（RAG）代理配置，并在105个专门构建的宇宙学问答对上进行了评估。", "result": "关键发现或结果未在摘要中明确提及，但研究旨在评估不同RAG配置在特定QA任务上的表现。", "conclusion": "主要结论和意义未在摘要中详细说明，但研究可能为天体物理学领域的自主科学发现提供了技术支持和评估方法。"}}
{"id": "2507.07299", "title": "LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation", "authors": ["Sonia Raychaudhuri", "Enrico Cancelli", "Tommaso Campari", "Lamberto Ballan", "Manolis Savva", "Angel X. Chang"], "abstract": "Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Despite these advances, we still lack a clear, language-focused benchmark for testing how well such agents ground the words in their instructions. We address this gap with LangNav, an open-set dataset specifically created to test an agent's ability to locate objects described at different levels of detail, from broad category names to fine attributes and object-object relations. Every description in LangNav was manually checked, yielding a lower error rate than existing lifelong- and semantic-navigation datasets. On top of LangNav we build LangNavBench, a benchmark that measures how well current semantic-navigation methods understand and act on these descriptions while moving toward their targets. LangNavBench allows us to systematically compare models on their handling of attributes, spatial and relational cues, and category hierarchies, offering the first thorough, language-centric evaluation of embodied navigation systems. We also present Multi-Layered Feature Map (MLFM), a method that builds a queryable multi-layered semantic map, particularly effective when dealing with small objects or instructions involving spatial relations. MLFM outperforms state-of-the-art mapping-based navigation baselines on the LangNav dataset.", "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07299.pdf", "abstract_url": "https://arxiv.org/abs/2507.07299", "categories": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "LangNavBench是一个专门用于测试基于语言的语义导航代理能力的开放数据集和基准，旨在评估代理对不同详细程度语言指令的理解和响应能力。", "motivation": "当前缺乏一个专注于语言理解的基准来评估语义导航代理如何将语言指令中的词汇与实际环境中的对象对应起来。", "method": "提出了LangNav数据集和LangNavBench基准，以及Multi-Layered Feature Map (MLFM)方法，该方法构建了一个可查询的多层语义地图，特别适用于处理小对象或涉及空间关系的指令。", "result": "MLFM在LangNav数据集上优于现有的基于地图的导航基线方法。", "conclusion": "LangNavBench为评估体现导航系统的语言理解能力提供了一个全面、以语言为中心的框架，MLFM方法在特定情况下表现出色。"}}
{"id": "2507.07197", "title": "Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning", "authors": ["Elia Piccoli", "Malio Li", "Giacomo Carfì", "Vincenzo Lomonaco", "Davide Bacciu"], "abstract": "The recent focus and release of pre-trained models have been a key components to several advancements in many fields (e.g. Natural Language Processing and Computer Vision), as a matter of fact, pre-trained models learn disparate latent embeddings sharing insightful representations. On the other hand, Reinforcement Learning (RL) focuses on maximizing the cumulative reward obtained via agent's interaction with the environment. RL agents do not have any prior knowledge about the world, and they either learn from scratch an end-to-end mapping between the observation and action spaces or, in more recent works, are paired with monolithic and computationally expensive Foundational Models. How to effectively combine and leverage the hidden information of different pre-trained models simultaneously in RL is still an open and understudied question. In this work, we propose Weight Sharing Attention (WSA), a new architecture to combine embeddings of multiple pre-trained models to shape an enriched state representation, balancing the tradeoff between efficiency and performance. We run an extensive comparison between several combination modes showing that WSA obtains comparable performance on multiple Atari games compared to end-to-end models. Furthermore, we study the generalization capabilities of this approach and analyze how scaling the number of models influences agents' performance during and after training.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "Published at 4th Conference on Lifelong Learning Agents (CoLLAs), 2025", "pdf_url": "https://arxiv.org/pdf/2507.07197.pdf", "abstract_url": "https://arxiv.org/abs/2507.07197", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为权重共享注意力（WSA）的新架构，旨在通过结合多个预训练模型的嵌入来丰富强化学习中的状态表示，平衡效率与性能之间的权衡。", "motivation": "强化学习（RL）代理通常从零开始学习观察与动作空间之间的端到端映射，或与计算成本高的基础模型配对。如何有效结合和利用不同预训练模型的隐藏信息在RL中仍是一个开放且研究不足的问题。", "method": "提出权重共享注意力（WSA）架构，结合多个预训练模型的嵌入，形成丰富的状态表示。", "result": "WSA在多个Atari游戏上获得了与端到端模型相当的性能，并研究了该方法的泛化能力及模型数量对代理性能的影响。", "conclusion": "WSA架构有效地结合了预训练模型的嵌入，为强化学习提供了丰富的状态表示，同时在效率和性能之间取得了良好的平衡。"}}
{"id": "2507.07983", "title": "Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology", "authors": ["Sabine Felde", "Rüdiger Buchkremer", "Gamal Chehab", "Christian Thielscher", "Jörg HW Distler", "Matthias Schneider", "Jutta G. Richter"], "abstract": "Large language models (LLMs) show promise for supporting clinical decision-making in complex fields such as rheumatology. Our evaluation shows that smaller language models (SLMs), combined with retrieval-augmented generation (RAG), achieve higher diagnostic and therapeutic performance than larger models, while requiring substantially less energy and enabling cost-efficient, local deployment. These features are attractive for resource-limited healthcare. However, expert oversight remains essential, as no model consistently reached specialist-level accuracy in rheumatology.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07983.pdf", "abstract_url": "https://arxiv.org/abs/2507.07983", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "大型语言模型（LLMs）在复杂领域如风湿病学中支持临床决策显示出潜力。我们的评估显示，结合检索增强生成（RAG）的小型语言模型（SLMs）在诊断和治疗性能上优于大型模型，同时能耗显著降低，支持成本效益高的本地部署。这些特点对资源有限的医疗保健具有吸引力。然而，专家监督仍然必不可少，因为没有模型能持续达到风湿病学专家的准确度。", "motivation": "解决在资源有限的医疗保健环境中，如何高效、准确地支持临床决策的问题。", "method": "评估大型语言模型（LLMs）与结合检索增强生成（RAG）的小型语言模型（SLMs）在风湿病学临床决策支持中的性能和实用性。", "result": "小型语言模型（SLMs）结合RAG在诊断和治疗性能上优于大型模型，且能耗更低，适合本地部署。", "conclusion": "尽管小型语言模型（SLMs）结合RAG在性能和能效方面表现优异，但在风湿病学临床决策支持中仍需专家监督以确保准确性。"}}
{"id": "2507.07998", "title": "PyVision: Agentic Vision with Dynamic Tooling", "authors": ["Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Ming Li", "Qilong Wu", "Kaipeng Zhang", "Chen Wei"], "abstract": "LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "comments": "26 Pages, 10 Figures, Technical report", "pdf_url": "https://arxiv.org/pdf/2507.07998.pdf", "abstract_url": "https://arxiv.org/abs/2507.07998", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "PyVision是一个交互式、多轮次的框架，使MLLMs能够自主生成、执行和优化基于Python的工具，以实现灵活和可解释的问题解决。", "motivation": "解决视觉推理中预定义工作流和静态工具集的限制，提升模型的灵活性和问题解决能力。", "method": "开发了一个交互式、多轮次的框架，允许MLLMs自主生成、执行和优化基于Python的工具。", "result": "PyVision在多个基准测试中实现了性能提升，GPT-4.1在V*上提升了7.8%，Claude-4.0-Sonnet在VLMsAreBlind-mini上提升了31.1%。", "conclusion": "动态工具化不仅使模型能够使用工具，还能发明工具，推动了更高级的视觉推理。"}}
{"id": "2507.07957", "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents", "authors": ["Yu Wang", "Xi Chen"], "abstract": "Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the field's most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07957.pdf", "abstract_url": "https://arxiv.org/abs/2507.07957", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "MIRIX是一个模块化的多代理记忆系统，旨在解决AI代理记忆能力的根本限制，通过引入六种不同的记忆类型和多代理框架，实现了对多样化、长期用户数据的高效存储和检索。", "motivation": "现有的AI代理记忆解决方案主要依赖于扁平、范围狭窄的记忆组件，限制了它们在个性化、抽象和可靠回忆用户特定信息方面的能力。MIRIX旨在解决这一领域的关键挑战：使语言模型能够真正记住。", "method": "MIRIX采用了一个模块化的多代理记忆系统，包括六种不同的记忆类型：核心记忆、情景记忆、语义记忆、程序记忆、资源记忆和知识库，以及一个动态控制和协调更新和检索的多代理框架。", "result": "在ScreenshotVQA和LOCOMO两个具有挑战性的基准测试中，MIRIX分别实现了比RAG基线高35%的准确率和85.4%的最先进性能，同时存储需求减少了99.9%。", "conclusion": "MIRIX为记忆增强的LLM代理设定了新的性能标准，并通过提供打包应用程序，使用户能够体验到其记忆系统，确保了隐私和安全。"}}
{"id": "2507.07906", "title": "Agentic Retrieval of Topics and Insights from Earnings Calls", "authors": ["Anant Gupta", "Rajarshi Bhowmik", "Geoffrey Gunow"], "abstract": "Tracking the strategic focus of companies through topics in their earnings calls is a key task in financial analysis. However, as industries evolve, traditional topic modeling techniques struggle to dynamically capture emerging topics and their relationships. In this work, we propose an LLM-agent driven approach to discover and retrieve emerging topics from quarterly earnings calls. We propose an LLM-agent to extract topics from documents, structure them into a hierarchical ontology, and establish relationships between new and existing topics through a topic ontology. We demonstrate the use of extracted topics to infer company-level insights and emerging trends over time. We evaluate our approach by measuring ontology coherence, topic evolution accuracy, and its ability to surface emerging financial trends.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "The 2nd Workshop on Financial Information Retrieval in the Era of Generative AI, The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval July 13-17, 2025 | Padua, Italy", "pdf_url": "https://arxiv.org/pdf/2507.07906.pdf", "abstract_url": "https://arxiv.org/abs/2507.07906", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了一种基于LLM-agent的方法，用于从季度收益电话会议中发现和检索新兴主题，通过构建层次化本体来捕捉主题之间的关系，并展示了如何利用提取的主题推断公司层面的见解和新兴趋势。", "motivation": "传统主题建模技术在动态捕捉新兴主题及其关系方面存在困难，尤其是在行业不断演变的情况下。", "method": "使用LLM-agent从文档中提取主题，将其结构化为层次化本体，并通过主题本体建立新旧主题之间的关系。", "result": "通过测量本体一致性、主题演化准确性及其表面新兴金融趋势的能力，评估了所提方法的有效性。", "conclusion": "该方法能够有效发现和检索新兴主题，为财务分析提供了新的工具和见解。"}}
{"id": "2507.07376", "title": "PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments", "authors": ["Hengrui Liu", "Yi Feng", "Qilong Zhang"], "abstract": "Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster response, exploration, and reconnaissance. However, dynamic and unknown environments pose significant challenges due to target unpredictability and environmental uncertainty. To tackle these issues, we propose PILOC, a framework that operates without global prior knowledge, leveraging local perception and communication. It introduces a pheromone inverse guidance mechanism to enable efficient coordination and dynamic target localization. PILOC promotes decentralized cooperation through local communication, significantly reducing reliance on global channels. Unlike conventional heuristics, the pheromone mechanism is embedded into the observation space of Deep Reinforcement Learning (DRL), supporting indirect agent coordination based on environmental cues. We further integrate this strategy into a DRL-based multi-agent architecture and conduct extensive experiments. Results show that combining local communication with pheromone-based guidance significantly boosts search efficiency, adaptability, and system robustness. Compared to existing methods, PILOC performs better under dynamic and communication-constrained scenarios, offering promising directions for future MASAR applications.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07376.pdf", "abstract_url": "https://arxiv.org/abs/2507.07376", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为PILOC的框架，用于在未知环境中进行多智能体动态目标搜索，通过局部感知和通信以及信息素逆向引导机制，提高了搜索效率和系统鲁棒性。", "motivation": "多智能体搜索与救援（MASAR）在灾难响应、探索和侦察中扮演重要角色，但动态和未知环境中的目标不可预测性和环境不确定性带来了重大挑战。", "method": "PILOC框架利用局部感知和通信，引入信息素逆向引导机制，通过将信息素机制嵌入深度强化学习（DRL）的观察空间，支持基于环境线索的间接智能体协调。", "result": "实验结果表明，结合局部通信和信息素引导显著提高了搜索效率、适应性和系统鲁棒性，PILOC在动态和通信受限的场景下表现优于现有方法。", "conclusion": "PILOC为未来的MASAR应用提供了有前景的方向，特别是在动态和通信受限的环境中。"}}
{"id": "2507.07969", "title": "Reinforcement Learning with Action Chunking", "authors": ["Qiyang Li", "Zhiyuan Zhou", "Sergey Levine"], "abstract": "We present Q-chunking, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks. Our recipe is designed for the offline-to-online RL setting, where the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. Effective exploration and sample-efficient learning remain central challenges in this setting, as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. Our key insight is that action chunking, a technique popularized in imitation learning where sequences of future actions are predicted rather than a single action at each timestep, can be applied to temporal difference (TD)-based RL methods to mitigate the exploration challenge. Q-chunking adopts action chunking by directly running RL in a 'chunked' action space, enabling the agent to (1) leverage temporally consistent behaviors from offline data for more effective online exploration and (2) use unbiased $n$-step backups for more stable and efficient TD learning. Our experimental results demonstrate that Q-chunking exhibits strong offline performance and online sample efficiency, outperforming prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)", "comments": "25 pages, 15 figures", "pdf_url": "https://arxiv.org/pdf/2507.07969.pdf", "abstract_url": "https://arxiv.org/abs/2507.07969", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)", "Machine Learning (stat.ML)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了Q-chunking，一种通过动作分块（action chunking）技术改进强化学习算法的方法，特别适用于长周期、稀疏奖励的任务，旨在提高离线到在线强化学习设置的样本效率。", "motivation": "解决在离线到在线强化学习设置中，如何有效利用离线数据来提升在线学习的样本效率和探索效率的问题。", "method": "采用动作分块技术，将未来的动作序列而非单个动作预测应用于基于时间差分（TD）的强化学习方法，以缓解探索挑战。", "result": "实验结果表明，Q-chunking在离线性能和在线样本效率方面表现出色，在一系列长周期、稀疏奖励的操作任务上优于之前的最佳离线到在线方法。", "conclusion": "Q-chunking通过动作分块技术有效提升了强化学习在长周期、稀疏奖励任务中的性能，为离线到在线强化学习提供了一种简单而有效的解决方案。"}}
