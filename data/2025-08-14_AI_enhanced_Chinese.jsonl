{"id": "2508.09210", "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models", "authors": ["Fan Zhang", "Zebang Cheng", "Chong Deng", "Haoxuan Li", "Zheng Lian", "Qian Chen", "Huadai Liu", "Wen Wang", "Yi-Fan Zhang", "Renrui Zhang", "Ziyu Guo", "Zhihong Zhu", "Hao Wu", "Haixin Wang", "Yefeng Zheng", "Xiaojiang Peng", "Xian Wu", "Kun Wang", "Xiangang Li", "Jieping Ye", "Pheng-Ann Heng"], "abstract": "Recent advances in multimodal large language models (MLLMs) have catalyzed transformative progress in affective computing, enabling models to exhibit emergent emotional intelligence. Despite substantial methodological progress, current emotional benchmarks remain limited, as it is still unknown: (a) the generalization abilities of MLLMs across distinct scenarios, and (b) their reasoning capabilities to identify the triggering factors behind emotional states. To bridge these gaps, we present \\textbf{MME-Emotion}, a systematic benchmark that assesses both emotional understanding and reasoning capabilities of MLLMs, enjoying \\textit{scalable capacity}, \\textit{diverse settings}, and \\textit{unified protocols}. As the largest emotional intelligence benchmark for MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific questioning-answering (QA) pairs, spanning broad scenarios to formulate eight emotional tasks. It further incorporates a holistic evaluation suite with hybrid metrics for emotion recognition and reasoning, analyzed through a multi-agent system framework. Through a rigorous evaluation of 20 advanced MLLMs, we uncover both their strengths and limitations, yielding several key insights: \\ding{182} Current MLLMs exhibit unsatisfactory emotional intelligence, with the best-performing model achieving only $39.3\\%$ recognition score and $56.0\\%$ Chain-of-Thought (CoT) score on our benchmark. \\ding{183} Generalist models (\\emph{e.g.}, Gemini-2.5-Pro) derive emotional intelligence from generalized multimodal understanding capabilities, while specialist models (\\emph{e.g.}, R1-Omni) can achieve comparable performance through domain-specific post-training adaptation. By introducing MME-Emotion, we hope that it can serve as a foundation for advancing MLLMs' emotional intelligence in the future.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09210.pdf", "abstract_url": "https://arxiv.org/abs/2508.09210", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "MME-Emotion是一个针对多模态大语言模型（MLLMs）情感智能的系统性评估基准，包含超过6,000个视频片段和任务特定的问答对，旨在评估MLLMs的情感理解和推理能力。", "motivation": "当前的情感基准在评估多模态大语言模型的泛化能力和情感状态触发因素识别能力方面存在局限，MME-Emotion旨在填补这一空白。", "method": "通过构建一个包含广泛场景和八项情感任务的基准，结合混合度量标准和多代理系统框架，对20种先进MLLMs进行严格评估。", "result": "评估显示当前MLLMs的情感智能表现不佳，最佳模型在识别和推理任务上的得分分别为39.3%和56.0%。通用模型和专用模型在情感智能方面各有优势。", "conclusion": "MME-Emotion的引入为未来提升MLLMs的情感智能奠定了基础，揭示了当前模型的局限性和潜在改进方向。"}}
{"id": "2508.09241", "title": "FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents", "authors": ["Fengxian Ji", "Jingpu Yang", "Zirui Song", "Yuanxi Wang", "Zhexuan Cui", "Yuke Li", "Qian Jiang", "Miao Fang", "Xiuying Chen"], "abstract": "With the rapid advancement of generative artificial intelligence technology, Graphical User Interface (GUI) agents have demonstrated tremendous potential for autonomously managing daily tasks through natural language instructions. However, current evaluation frameworks for GUI agents suffer from fundamental flaws: existing benchmarks overly focus on coarse-grained task completion while neglecting fine-grained control capabilities crucial for real-world applications. To address this, we introduce FineState-Bench, the first evaluation and diagnostic standard for fine-grained GUI proxy operations, designed to quantify fine-grained control. This multi-platform (desktop, Web, mobile) framework includes 2257 task benchmarks in four components and uses a four-phase indicator for comprehensive perception-to-control assessment. To analyze perception and positioning for refined operations, we developed the plug-and-play Visual Diagnostic Assistant (VDA), enabling the first quantitative decoupling analysis of these capabilities. Experimental results on our benchmark show that the most advanced models achieve only 32.8% fine-grained interaction accuracy. Using our VDA in controlled experiments, quantifying the impact of visual capabilities, we showed that ideal visual localization boosts Gemini-2.5-Flash's success rate by 14.9\\%. Our diagnostic framework confirms for the first time that the primary bottleneck for current GUI proxies is basic visual positioning", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "submit/6682470 (Fengxian Ji)", "pdf_url": "https://arxiv.org/pdf/2508.09241.pdf", "abstract_url": "https://arxiv.org/abs/2508.09241", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了FineState-Bench，这是一个用于评估图形用户界面(GUI)代理细粒度控制能力的综合基准测试框架，旨在解决现有评估框架忽视细粒度控制能力的问题。", "motivation": "当前GUI代理的评估框架存在根本性缺陷，过于关注粗粒度任务完成而忽视了细粒度控制能力，这对于实际应用至关重要。", "method": "提出了FineState-Bench，一个多平台(桌面、网页、移动)框架，包含2257个任务基准和四个组件，使用四阶段指标进行全面感知到控制的评估。并开发了视觉诊断助手(VDA)进行感知和定位的定量解耦分析。", "result": "实验结果显示，最先进的模型在细粒度交互准确率上仅达到32.8%。使用VDA在控制实验中量化视觉能力的影响，理想视觉定位将Gemini-2.5-Flash的成功率提高了14.9%。", "conclusion": "诊断框架首次证实，当前GUI代理的主要瓶颈是基本的视觉定位能力。"}}
{"id": "2508.09186", "title": "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System", "authors": ["Abdolazim Rezaei", "Mehdi Sookhak", "Mahboobeh Haghparast"], "abstract": "The proliferation of AI-powered cameras in Intelligent Transportation Systems (ITS) creates a severe conflict between the need for rich visual data and the fundamental right to privacy. Existing privacy-preserving mechanisms, such as blurring or encryption, are often insufficient, creating an undesirable trade-off where either privacy is compromised against advanced reconstruction attacks or data utility is critically degraded. To resolve this impasse, we propose RL-MoE, a novel framework that transforms sensitive visual data into privacy-preserving textual descriptions, eliminating the need for direct image transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture for nuanced, multi-aspect scene decomposition with a Reinforcement Learning (RL) agent that optimizes the generated text for a dual objective of semantic accuracy and privacy preservation. Extensive experiments demonstrate that RL-MoE provides superior privacy protection, reducing the success rate of replay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously generating richer textual content than baseline methods. Our work provides a practical and scalable solution for building trustworthy AI systems in privacy-sensitive domains, paving the way for more secure smart city and autonomous vehicle networks.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09186.pdf", "abstract_url": "https://arxiv.org/abs/2508.09186", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "RL-MoE是一种基于图像的隐私保护方法，用于智能交通系统，通过将敏感视觉数据转换为隐私保护的文本描述，解决了视觉数据丰富性与隐私权之间的冲突。", "motivation": "智能交通系统中AI摄像头的普及引发了视觉数据需求与隐私权保护之间的严重冲突，现有隐私保护机制如模糊化或加密往往不足，导致隐私与数据效用之间的不良权衡。", "method": "RL-MoE框架结合了混合专家（MoE）架构进行多方面的场景分解和强化学习（RL）代理，优化生成的文本以实现语义准确性和隐私保护的双重目标。", "result": "在CFP-FP数据集上的广泛实验表明，RL-MoE提供了卓越的隐私保护，将重放攻击的成功率降低至仅9.4%，同时生成的文本内容比基线方法更丰富。", "conclusion": "RL-MoE为在隐私敏感领域构建可信赖的AI系统提供了实用且可扩展的解决方案，为更安全的智能城市和自动驾驶车辆网络铺平了道路。"}}
{"id": "2508.09262", "title": "Harnessing Input-Adaptive Inference for Efficient VLN", "authors": ["Dongwoo Kang", "Akhil Perincherry", "Zachary Coalson", "Aiden Gabriel", "Stefan Lee", "Sanghyun Hong"], "abstract": "An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "comments": "Accepted to ICCV 2025 [Poster]", "pdf_url": "https://arxiv.org/pdf/2508.09262.pdf", "abstract_url": "https://arxiv.org/abs/2508.09262", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新颖的输入自适应导航方法，以提高视觉与语言导航（VLN）模型的效率。通过三种自适应算法在不同层次上的部署，实现了计算量的大幅减少。", "motivation": "视觉与语言导航（VLN）中使用的历史感知多模态变换器模型虽然性能显著，但其规模在计算资源有限的实际应用中成为瓶颈。", "method": "提出了三种自适应算法：选择性处理全景视图以提高空间效率；基于重要性的自适应阈值用于早期退出方法以提高模型内效率；实现缓存机制以防止重复处理先前观察到的视图以提高时间效率。", "result": "在七个VLN基准测试中，展示了在标准和连续环境下，三种现成代理的计算量减少了超过2倍。", "conclusion": "通过输入自适应推理，可以在不显著降低性能的情况下，有效提高VLN模型的效率，为资源受限的实际应用提供了可行的解决方案。"}}
{"id": "2508.09404", "title": "Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving", "authors": ["Guangxun Zhu", "Shiyu Fan", "Hang Dai", "Edmond S. L. Ho"], "abstract": "Large-scale high-quality 3D motion datasets with multi-person interactions are crucial for data-driven models in autonomous driving to achieve fine-grained pedestrian interaction understanding in dynamic urban environments. However, existing datasets mostly rely on estimating 3D poses from monocular RGB video frames, which suffer from occlusion and lack of temporal continuity, thus resulting in unrealistic and low-quality human motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics, derived from the Waymo Perception dataset. Our key insight is to utilize 3D human body shape and motion priors to enhance the quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The dataset covers over 14,000 seconds across more than 800 real driving scenarios, including rich interactions among an average of 27 agents per scene (with up to 250 agents in the largest scene). Furthermore, we establish 3D pose forecasting benchmarks under varying pedestrian densities, and the results demonstrate its value as a foundational resource for future research on fine-grained human behavior understanding in complex urban environments. The dataset and code will be available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)", "comments": "ACM Multimedia 2025 (Dataset Track) Paper", "pdf_url": "https://arxiv.org/pdf/2508.09404.pdf", "abstract_url": "https://arxiv.org/abs/2508.09404", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Multimedia (cs.MM)"], "matching_keywords": ["agent"], "AI": {"tldr": "Waymo-3DSkelMo是一个大规模高质量的3D骨骼运动数据集，专注于自动驾驶中行人交互建模，通过利用3D人体形状和运动先验提升从LiDAR点云提取的3D姿态序列质量。", "motivation": "解决现有数据集因依赖单目RGB视频帧估计3D姿态而导致的遮挡和时间连续性不足问题，提供更真实、高质量的人类运动数据。", "method": "利用3D人体形状和运动先验，从Waymo Perception数据集的原始LiDAR点云中提取高质量的3D姿态序列。", "result": "数据集覆盖了超过14,000秒的800多个真实驾驶场景，平均每个场景有27个交互主体（最大场景达250个），并建立了不同行人密度下的3D姿态预测基准。", "conclusion": "Waymo-3DSkelMo作为未来研究复杂城市环境中细粒度人类行为理解的基础资源，具有重要价值。数据集和代码将公开提供。"}}
{"id": "2508.09423", "title": "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation", "authors": ["Badi Li", "Ren-jie Lu", "Yu Zhou", "Jingke Meng", "Wei-shi Zheng"], "abstract": "The Object Goal Navigation (ObjectNav) task challenges agents to locate a specified object in an unseen environment by imagining unobserved regions of the scene. Prior approaches rely on deterministic and discriminative models to complete semantic maps, overlooking the inherent uncertainty in indoor layouts and limiting their ability to generalize to unseen environments. In this work, we propose GOAL, a generative flow-based framework that models the semantic distribution of indoor environments by bridging observed regions with LLM-enriched full-scene semantic maps. During training, spatial priors inferred from large language models (LLMs) are encoded as two-dimensional Gaussian fields and injected into target maps, distilling rich contextual knowledge into the flow model and enabling more generalizable completions. Extensive experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D and Gibson, and shows strong generalization in transfer settings to HM3D. Codes and pretrained models are available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09423.pdf", "abstract_url": "https://arxiv.org/abs/2508.09423", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了GOAL，一个基于生成流的框架，用于在对象目标导航（ObjectNav）任务中建模室内环境的语义分布，通过将观察到的区域与LLM丰富的全场景语义地图桥接，提高了在未见环境中的泛化能力。", "motivation": "解决ObjectNav任务中现有方法依赖于确定性和判别性模型，忽视了室内布局的不确定性，限制了在未见环境中的泛化能力的问题。", "method": "提出GOAL框架，利用大型语言模型（LLMs）推断的空间先验作为二维高斯场注入目标地图，将丰富的上下文知识蒸馏到流模型中，实现更通用的完成。", "result": "在MP3D和Gibson上实现了最先进的性能，并在HM3D的转移设置中显示出强大的泛化能力。", "conclusion": "GOAL通过结合LLM的先验知识和生成流模型，显著提高了ObjectNav任务中的泛化能力和性能，为未来的研究提供了新的方向。"}}
{"id": "2508.09303", "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu", "Japinder Singh", "Aaditya Shukla", "Rama Akkiraju"], "abstract": "Reasoning-augmented search agents such as Search-R1, trained via reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable capabilities in multi-step information retrieval from external knowledge sources. These agents address the limitations of their parametric memory by dynamically gathering relevant facts to address complex reasoning tasks. However, existing approaches suffer from a fundamental architectural limitation: they process search queries strictly sequentially, even when handling inherently parallelizable and logically independent comparisons. This sequential bottleneck significantly constrains computational efficiency, particularly for queries that require multiple entity comparisons. To address this critical limitation, we propose ParallelSearch, a novel reinforcement learning framework that empowers large language models (LLMs) to recognize parallelizable query structures and execute multiple search operations concurrently. Our approach introduces dedicated reward functions that incentivize the identification of independent query components while preserving answer accuracy through jointly considering correctness, query decomposition quality, and parallel execution benefits. Comprehensive experiments demonstrate that ParallelSearch outperforms state-of-the-art baselines by an average performance gain of 2.9% across seven question-answering benchmarks. Notably, on parallelizable questions, our method achieves a 12.7% performance improvement while requiring only 69.6% of the LLM calls compared to sequential approaches.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09303.pdf", "abstract_url": "https://arxiv.org/abs/2508.09303", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["agent"], "AI": {"tldr": "ParallelSearch是一种新颖的强化学习框架，旨在通过并行处理可并行化的查询结构来提高大型语言模型（LLMs）在信息检索中的效率和性能。", "motivation": "现有的搜索代理在处理查询时采用严格的顺序处理方式，即使面对可并行化和逻辑上独立的比较时也是如此，这显著限制了计算效率。", "method": "提出了一种强化学习框架，通过专门的奖励函数激励识别独立的查询组件，并在保持答案准确性的同时，考虑正确性、查询分解质量和并行执行的好处。", "result": "ParallelSearch在七个问答基准测试中平均性能提升了2.9%，在可并行化问题上性能提升了12.7%，同时仅需顺序方法69.6%的LLM调用。", "conclusion": "ParallelSearch通过并行执行搜索操作，显著提高了处理复杂查询任务的效率和性能，为未来的信息检索和问答系统提供了新的方向。"}}
{"id": "2508.09277", "title": "Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning", "authors": ["Soumia Mehimeh"], "abstract": "Value function initialization (VFI) is an effective way to achieve a jumpstart in reinforcement learning (RL) by leveraging value estimates from prior tasks. While this approach is well established in tabular settings, extending it to deep reinforcement learning (DRL) poses challenges due to the continuous nature of the state-action space, the noisy approximations of neural networks, and the impracticality of storing all past models for reuse. In this work, we address these challenges and introduce DQInit, a method that adapts value function initialization to DRL. DQInit reuses compact tabular Q-values extracted from previously solved tasks as a transferable knowledge base. It employs a knownness-based mechanism to softly integrate these transferred values into underexplored regions and gradually shift toward the agent's learned estimates, avoiding the limitations of fixed time decay. Our approach offers a novel perspective on knowledge transfer in DRL by relying solely on value estimates rather than policies or demonstrations, effectively combining the strengths of jumpstart RL and policy distillation while mitigating their drawbacks. Experiments across multiple continuous control tasks demonstrate that DQInit consistently improves early learning efficiency, stability, and overall performance compared to standard initialization and existing transfer techniques.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09277.pdf", "abstract_url": "https://arxiv.org/abs/2508.09277", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Logic in Computer Science (cs.LO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为DQInit的方法，通过利用从先前任务中提取的紧凑表格Q值作为可转移的知识库，将价值函数初始化（VFI）适应于深度强化学习（DRL），以提高早期学习效率、稳定性和整体性能。", "motivation": "解决在深度强化学习中，由于状态-动作空间的连续性、神经网络近似的不准确性以及存储所有过去模型的不切实际性，将价值函数初始化从表格设置扩展到DRL所面临的挑战。", "method": "引入DQInit方法，该方法重用从先前解决的任务中提取的紧凑表格Q值作为可转移的知识库，并采用基于已知度的机制将这些转移的值软集成到未充分探索的区域，逐渐转向代理的学习估计。", "result": "在多个连续控制任务上的实验表明，DQInit在早期学习效率、稳定性和整体性能上 consistently优于标准初始化和现有转移技术。", "conclusion": "DQInit为DRL中的知识转移提供了新的视角，仅依赖于价值估计而非策略或演示，有效地结合了跳跃启动RL和策略蒸馏的优势，同时减轻了它们的缺点。"}}
{"id": "2508.09323", "title": "Leveraging Large Language Models for Rare Disease Named Entity Recognition", "authors": ["Nan Miles Xi", "Yu Deng", "Lin Wang"], "abstract": "Named Entity Recognition (NER) in the rare disease domain poses unique challenges due to limited labeled data, semantic ambiguity between entity types, and long-tail distributions. In this study, we evaluate the capabilities of GPT-4o for rare disease NER under low-resource settings, using a range of prompt-based strategies including zero-shot prompting, few-shot in-context learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We design a structured prompting framework that encodes domain-specific knowledge and disambiguation rules for four entity types. We further introduce two semantically guided few-shot example selection methods to improve in-context performance while reducing labeling effort. Experiments on the RareDis Corpus show that GPT-4o achieves competitive or superior performance compared to BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art (SOTA) results. Cost-performance analysis reveals that few-shot prompting delivers high returns at low token budgets, while RAG offers marginal additional benefit. An error taxonomy highlights common failure modes such as boundary drift and type confusion, suggesting opportunities for post-processing and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can serve as effective, scalable alternatives to traditional supervised models in biomedical NER, particularly in rare disease applications where annotated data is scarce.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09323.pdf", "abstract_url": "https://arxiv.org/abs/2508.09323", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "评估GPT-4o在罕见疾病命名实体识别（NER）中的能力，通过多种提示策略在低资源设置下实现竞争性或优越性能。", "motivation": "解决罕见疾病领域NER面临的标注数据有限、实体类型间语义模糊及长尾分布等独特挑战。", "method": "使用零样本提示、少量样本上下文学习、检索增强生成（RAG）和任务级微调等提示策略，设计结构化提示框架，并引入两种语义引导的少量样本选择方法。", "result": "在RareDis Corpus上的实验显示，GPT-4o达到与BioClinicalBERT竞争或更优的性能，任务级微调创造了新的SOTA结果；少量样本提示在低令牌预算下回报高，而RAG提供边际额外效益。", "conclusion": "优化提示的大型语言模型（LLMs）可作为生物医学NER中传统监督模型的有效、可扩展替代方案，尤其在标注数据稀缺的罕见疾病应用中。"}}
{"id": "2508.09784", "title": "Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete", "authors": ["Avijeet Ghosh", "Sujata Ghosh", "François Schwarzentruber"], "abstract": "Logics for reasoning about knowledge and actions have seen many applications in various domains of multi-agent systems, including epistemic planning. Change of knowledge based on observations about the surroundings forms a key aspect in such planning scenarios. Public Observation Logic (POL) is a variant of public announcement logic for reasoning about knowledge that gets updated based on public observations. Each state in an epistemic (Kripke) model is equipped with a set of expected observations. These states evolve as the expectations get matched with the actual observations. In this work, we prove that the satisfiability problem of $\\POL$ is 2EXPTIME-complete.", "subjects": "Artificial Intelligence (cs.AI); Computational Complexity (cs.CC); Logic in Computer Science (cs.LO)", "comments": "Accepted in KR 25", "pdf_url": "https://arxiv.org/pdf/2508.09784.pdf", "abstract_url": "https://arxiv.org/abs/2508.09784", "categories": ["Artificial Intelligence (cs.AI)", "Computational Complexity (cs.CC)", "Logic in Computer Science (cs.LO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文证明了关于正则表达式的知识推理问题是2EXPTIME完全的。", "motivation": "在多智能体系统中，基于知识和行动的逻辑在多个领域有广泛应用，包括认知规划。基于对周围环境的观察来改变知识是此类规划场景中的一个关键方面。公共观察逻辑（POL）是公共宣布逻辑的一个变体，用于推理基于公共观察更新的知识。", "method": "在认知（Kripke）模型中，每个状态都配备了一组预期的观察。这些状态随着预期与实际观察的匹配而演变。本文研究了POL的可满足性问题。", "result": "主要结果是证明了POL的可满足性问题是2EXPTIME完全的。", "conclusion": "本文的主要结论和意义在于确定了POL的可满足性问题的计算复杂性，这对于理解和设计基于知识的推理系统具有重要意义。"}}
{"id": "2508.09507", "title": "An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants", "authors": ["Meiping Wang", "Jian Zhong", "Rongduo Han", "Liming Kang", "Zhengkun Shi", "Xiao Liang", "Xing Lin", "Nan Gao", "Haining Zhang"], "abstract": "With the rapid development of mobile intelligent assistant technologies, multi-modal AI assistants have become essential interfaces for daily user interactions. However, current evaluation methods face challenges including high manual costs, inconsistent standards, and subjective bias. This paper proposes an automated multi-modal evaluation framework based on large language models and multi-agent collaboration. The framework employs a three-tier agent architecture consisting of interaction evaluation agents, semantic verification agents, and experience decision agents. Through supervised fine-tuning on the Qwen3-8B model, we achieve a significant evaluation matching accuracy with human experts. Experimental results on eight major intelligent agents demonstrate the framework's effectiveness in predicting users' satisfaction and identifying generation defects.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09507.pdf", "abstract_url": "https://arxiv.org/abs/2508.09507", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于大型语言模型和多智能体协作的自动化多模态评估框架，用于移动智能助手。该框架通过三层智能体架构，实现了与人类专家评估匹配的高准确率，有效预测用户满意度和识别生成缺陷。", "motivation": "随着移动智能助手技术的快速发展，多模态AI助手已成为用户日常交互的重要界面。然而，当前的评估方法面临高人工成本、标准不一致和主观偏见等挑战。", "method": "本文提出的框架采用三层智能体架构：交互评估智能体、语义验证智能体和体验决策智能体。通过在Qwen3-8B模型上进行监督微调，实现了与人类专家评估匹配的高准确率。", "result": "在八大主要智能助手上的实验结果表明，该框架在预测用户满意度和识别生成缺陷方面具有高效性。", "conclusion": "该自动化多模态评估框架为解决当前评估方法的挑战提供了有效方案，具有重要的实际应用价值。"}}
{"id": "2508.09497", "title": "From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation", "authors": ["Siyuan Meng", "Junming Liu", "Yirong Chen", "Song Mao", "Pinlong Cai", "Guohang Yan", "Botian Shi", "Ding Wang"], "abstract": "Retrieval-augmented generation (RAG) systems are often bottlenecked by their reranking modules, which typically score passages independently and select a fixed Top-K size. This approach struggles with complex multi-hop queries that require synthesizing evidence across multiple documents, creating a trade-off where small K values omit crucial information and large K values introduce noise. To address this, we introduce the Dynamic Passage Selector (DPS), a novel reranking framework that treats passage selection as a supervised learning problem. Unlike traditional point-wise or list-wise methods, DPS is fine-tuned to capture inter-passage dependencies and dynamically select the most relevant set of passages for generation. As a seamless plug-and-play module, DPS requires no modifications to the standard RAG pipeline. Comprehensive evaluations on five benchmarks show that DPS consistently outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results demonstrate that by enabling adaptive evidence selection, DPS substantially enhances reasoning capabilities in complex RAG scenarios.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "9 pages, 4 tables", "pdf_url": "https://arxiv.org/pdf/2508.09497.pdf", "abstract_url": "https://arxiv.org/abs/2508.09497", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了一种名为动态段落选择器（DPS）的新颖重排框架，旨在解决检索增强生成（RAG）系统中传统重排模块在处理复杂多跳查询时的局限性。DPS通过监督学习动态选择最相关的段落集，显著提升了复杂RAG场景下的推理能力。", "motivation": "传统的RAG系统重排模块通常独立评分段落并选择固定的Top-K大小，这在处理需要跨多个文档综合证据的复杂多跳查询时存在困难，小K值会遗漏关键信息，大K值则引入噪音。", "method": "DPS将段落选择视为监督学习问题，通过微调捕捉段落间的依赖关系，动态选择最相关的段落集，无需修改标准RAG流程即可实现即插即用。", "result": "在五个基准测试上的全面评估显示，DPS consistently outperforms state-of-the-art rerankers and fine-tuning methods。特别是在MuSiQue数据集上，DPS的F1-score比Qwen3-reranker和RankingGPT分别提高了30.06%和15.4%。", "conclusion": "通过实现自适应证据选择，DPS显著增强了复杂RAG场景下的推理能力，证明了其在提升RAG系统性能方面的有效性。"}}
{"id": "2508.09893", "title": "RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA", "authors": ["Bhavik Agarwal", "Hemant Sunil Jomraj", "Simone Kaplunov", "Jack Krolick", "Viktoria Rojkova"], "abstract": "Regulatory compliance question answering (QA) requires precise, verifiable information, and domain-specific expertise, posing challenges for Large Language Models (LLMs). In this work, we present a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with Retrieval-Augmented Generation (RAG) to address these demands. First, agents build and maintain an ontology-free KG by extracting subject--predicate--object (SPO) triplets from regulatory documents and systematically cleaning, normalizing, deduplicating, and updating them. Second, these triplets are embedded and stored along with their corresponding textual sections and metadata in a single enriched vector database, allowing for both graph-based reasoning and efficient information retrieval. Third, an orchestrated agent pipeline leverages triplet-level retrieval for question answering, ensuring high semantic alignment between user queries and the factual \"who-did-what-to-whom\" core captured by the graph. Our hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader audit-focused applications.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09893.pdf", "abstract_url": "https://arxiv.org/abs/2508.09893", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本文提出了一种新颖的多智能体框架，结合知识图谱（KG）和检索增强生成（RAG）技术，以提高监管合规问答（QA）的准确性和可验证性。", "motivation": "解决大型语言模型（LLMs）在处理需要精确、可验证信息和领域专业知识的监管合规问答时面临的挑战。", "method": "通过多智能体框架构建和维护一个无本体的知识图谱，从监管文档中提取SPO三元组，并进行清洗、规范化、去重和更新；将这些三元组嵌入并与对应的文本部分和元数据一起存储在富化的向量数据库中；利用三元组级别的检索进行问答。", "result": "混合系统在复杂的监管查询中优于传统方法，确保了事实正确性，通过统一的向量数据库实现了可追溯性，并通过子图可视化增强了理解。", "conclusion": "该系统为合规驱动和更广泛的审计聚焦应用提供了坚实的基础，展示了在监管合规问答领域的潜在广泛应用。"}}
{"id": "2508.09889", "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving", "authors": ["Zhitian Xie", "Qintong Wu", "Chengyue Yu", "Chenyi Zhuang", "Jinjie Gu"], "abstract": "The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, as agents increasingly depend on multiple tools, they encounter new challenges: extended contexts from disparate sources and noisy or irrelevant tool outputs can undermine system reliability and accuracy. These challenges underscore the necessity for enhanced stability in agent-based systems. To address this, we introduce dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. In our approach, the Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process, effectively reducing errors arising from noise and bolstering problem-solving robustness. Extensive experiments on the GAIA test dataset reveal that our dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. As a result, our dynamic MAS system achieved first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight the practical value of collaborative agent roles in developing more reliable and trustworthy intelligent systems.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09889.pdf", "abstract_url": "https://arxiv.org/abs/2508.09889", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了AWorld框架下的动态多代理系统（MAS），通过引入动态监督和机动机制，增强了代理系统的稳定性和问题解决的鲁棒性。在GAIA测试数据集上的实验表明，该系统显著提高了解决方案的有效性和稳定性，超越了单代理系统和标准工具增强系统。", "motivation": "随着大型语言模型（LLMs）的快速发展，智能代理越来越多地依赖多种外部工具来解决复杂的现实世界问题。然而，这种依赖性带来了新的挑战：来自不同来源的扩展上下文以及嘈杂或不相关的工具输出可能削弱系统的可靠性和准确性。这些挑战凸显了增强代理系统稳定性的必要性。", "method": "为了解决这些问题，我们在AWorld框架内构建了一个鲁棒且动态的多代理系统（MAS）架构，引入了动态监督和机动机制。在我们的方法中，执行代理在关键步骤调用守卫代理来验证和纠正推理过程，有效减少由噪声引起的错误，增强问题解决的鲁棒性。", "result": "在GAIA测试数据集上的广泛实验显示，我们的动态机动机制显著提高了解决方案的有效性和稳定性，优于单代理系统（SAS）和标准工具增强系统。因此，我们的动态MAS系统在著名的GAIA排行榜上获得了开源项目的第一名。", "conclusion": "这些发现突出了协作代理角色在开发更可靠和可信赖的智能系统中的实际价值。"}}
{"id": "2508.09932", "title": "Mathematical Computation and Reasoning Errors by Large Language Models", "authors": ["Liang Zhang", "Edith Aurora Graf"], "abstract": "Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09932.pdf", "abstract_url": "https://arxiv.org/abs/2508.09932", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "大型语言模型（LLMs）在数学教育中的应用日益增多，本研究评估了四种LLMs在解决算术、代数和数论任务中的准确性，并识别了其解决方案中的步骤级推理错误。研究发现，增强推理的OpenAI o1模型在所有数学任务类别中表现最佳，而双代理配置显著提高了整体性能。", "motivation": "解决LLMs在数学问题解决任务中生成准确答案和详细解决方案的能力，以确保数学教育实践中的反馈和评估的可靠性和精确性。", "method": "通过故意构建对LLMs具有挑战性且容易出错的任务（通过项目模型），系统地分析和编码最终答案的准确性及个别解决步骤中的错误。测试了单代理和双代理配置。", "result": "增强推理的OpenAI o1模型在所有数学任务类别中表现最佳。程序性失误是最常见且显著影响整体性能的错误类型，而概念性误解较少见。双代理配置显著提高了整体性能。", "conclusion": "这些发现为提高LLMs性能提供了可行的见解，并强调了将LLMs整合到数学教育中的有效策略，从而推动了AI驱动的教学实践和评估精确性的进步。"}}
{"id": "2508.09142", "title": "Bayesian-Driven Graph Reasoning for Active Radio Map Construction", "authors": ["Wenlihan Lu", "Shijian Gao", "Miaowen Wen", "Yuxuan Liang", "Chan-Byoung Chae", "H. Vincent Poor"], "abstract": "With the emergence of the low-altitude economy, radio maps have become essential for ensuring reliable wireless connectivity to aerial platforms. Autonomous aerial agents are commonly deployed for data collection using waypoint-based navigation; however, their limited battery capacity significantly constrains coverage and efficiency. To address this, we propose an uncertainty-aware radio map (URAM) reconstruction framework that explicitly leverages graph-based reasoning tailored for waypoint navigation. Our approach integrates two key deep learning components: (1) a Bayesian neural network that estimates spatial uncertainty in real time, and (2) an attention-based reinforcement learning policy that performs global reasoning over a probabilistic roadmap, using uncertainty estimates to plan informative and energy-efficient trajectories. This graph-based reasoning enables intelligent, non-myopic trajectory planning, guiding agents toward the most informative regions while satisfying safety constraints. Experimental results show that URAM improves reconstruction accuracy by up to 34% over existing baselines.", "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09142.pdf", "abstract_url": "https://arxiv.org/abs/2508.09142", "categories": ["Signal Processing (eess.SP)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于贝叶斯驱动的图推理方法，用于主动无线电地图构建，以提高无人机在低空经济中的无线连接可靠性。", "motivation": "随着低空经济的兴起，无线电地图对于确保空中平台的可靠无线连接变得至关重要。然而，自主空中代理的有限电池容量显著限制了其覆盖范围和数据收集效率。", "method": "本文提出了一个不确定性感知的无线电地图（URAM）重建框架，该框架结合了两个关键的深度学习组件：（1）一个实时估计空间不确定性的贝叶斯神经网络，（2）一个基于注意力的强化学习策略，该策略在概率路图上进行全局推理，利用不确定性估计来规划信息丰富且能量高效的轨迹。", "result": "实验结果表明，URAM比现有基线方法的重建准确率提高了高达34%。", "conclusion": "通过图推理实现的智能、非短视的轨迹规划，能够引导代理朝向信息最丰富的区域，同时满足安全约束，从而显著提高了无线电地图的重建效率和准确性。"}}
{"id": "2508.09632", "title": "Preacher: Paper-to-Video Agentic System", "authors": ["Jingwei Liu", "Ling Yang", "Hao Luo", "Fan Wang Hongyan Li", "Mengdi Wang"], "abstract": "The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a top-down approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at:", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09632.pdf", "abstract_url": "https://arxiv.org/abs/2508.09632", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "Preacher是一个将研究论文转化为结构化视频摘要的代理系统，通过分解、总结和重构论文内容，再自底向上生成视频，解决了现有视频生成模型的局限性。", "motivation": "解决现有视频生成模型在上下文窗口限制、视频时长刚性、风格多样性不足及无法表示领域特定知识方面的局限性。", "method": "采用自上而下的方法分解、总结和重构论文内容，然后自底向上生成视频，使用渐进式思维链（P-CoT）进行细粒度迭代规划。", "result": "Preacher在五个研究领域成功生成了高质量的视频摘要，展示了超越当前视频生成模型的专长。", "conclusion": "Preacher系统通过创新的方法解决了视频生成中的关键挑战，为研究论文的视频摘要提供了有效的解决方案。"}}
{"id": "2508.09755", "title": "Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation", "authors": ["Seokgi Lee"], "abstract": "We introduce a novel retrieval-augmented generation (RAG) framework tailored for multihop question answering. First, our system uses large language model (LLM) to decompose complex multihop questions into a sequence of single-hop subquestions that guide document retrieval. This decomposition mitigates the ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge facets. Second, instead of embedding raw or chunked documents directly, we generate answerable questions from each document chunk using Qwen3-8B, embed these generated questions, and retrieve relevant chunks via question-question embedding similarity. During inference, the retrieved chunks are then fed along with the original question into the RAG pipeline. We evaluate on three multihop question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our method improves RAG performacne compared to baseline systems. Our contributions highlight the benefits of using answerable-question embeddings for RAG, and the effectiveness of LLM-based query decomposition for multihop scenarios.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09755.pdf", "abstract_url": "https://arxiv.org/abs/2508.09755", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了一种新颖的检索增强生成（RAG）框架，专门用于多跳问题回答。该系统利用大型语言模型（LLM）将复杂的多跳问题分解为一系列单跳子问题，以指导文档检索。此外，通过从每个文档块生成可回答问题并使用问题-问题嵌入相似性检索相关块，提高了RAG的性能。", "motivation": "解决多跳问题回答中的模糊性，提高检索增强生成（RAG）框架的性能。", "method": "使用LLM分解多跳问题为单跳子问题，生成可回答问题并嵌入，通过问题-问题嵌入相似性检索相关文档块。", "result": "在三个多跳问题数据集（MuSiQue、2WikiMultiHopQa、HotpotQA）上评估，该方法比基线系统提高了RAG性能。", "conclusion": "使用可回答问题嵌入和基于LLM的查询分解在多跳场景中有效提高了RAG的性能。"}}
{"id": "2508.09147", "title": "Agentic TinyML for Intent-aware Handover in 6G Wireless Networks", "authors": ["Alaa Saleh", "Roberto Morabito", "Sasu Tarkoma", "Anders Lindgren", "Susanna Pirttikangas", "Lauri Lovén"], "abstract": "As 6G networks evolve into increasingly AI-driven, user-centric ecosystems, traditional reactive handover mechanisms demonstrate limitations, especially in mobile edge computing and autonomous agent-based service scenarios. This manuscript introduces WAAN, a cross-layer framework that enables intent-aware and proactive handovers by embedding lightweight TinyML agents as autonomous, negotiation-capable entities across heterogeneous edge nodes that contribute to intent propagation and network adaptation. To ensure continuity across mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points that serve as coordination anchors for context transfer and state preservation. The framework's operational capabilities are demonstrated through a multimodal environmental control case study, highlighting its effectiveness in maintaining user experience under mobility. Finally, the article discusses key challenges and future opportunities associated with the deployment and evolution of WAAN.", "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09147.pdf", "abstract_url": "https://arxiv.org/abs/2508.09147", "categories": ["Networking and Internet Architecture (cs.NI)", "Artificial Intelligence (cs.AI)", "Distributed, Parallel, and Cluster Computing (cs.DC)", "Machine Learning (cs.LG)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了WAAN，一个跨层框架，通过在异构边缘节点中嵌入轻量级TinyML代理作为自主、可协商的实体，实现意图感知和主动切换，以解决6G无线网络中传统反应式切换机制的局限性。", "motivation": "随着6G网络发展成为越来越以AI驱动、用户为中心的生态系统，传统的反应式切换机制在移动边缘计算和基于自主代理的服务场景中显示出局限性。", "method": "WAAN框架通过在异构边缘节点中嵌入轻量级TinyML代理作为自主、可协商的实体，实现意图感知和主动切换，并引入半稳定会合点作为上下文转移和状态保存的协调锚点。", "result": "通过多模式环境控制的案例研究，展示了WAAN在移动性下维持用户体验的有效性。", "conclusion": "文章讨论了与WAAN部署和演进相关的关键挑战和未来机遇，强调了其在6G无线网络中的潜在价值。"}}
{"id": "2508.09159", "title": "Agoran: An Agentic Open Marketplace for 6G RAN Automation", "authors": ["Ilias Chatzistefanidis", "Navid Nikaein", "Andrea Leone", "Ali Maatouk", "Leandros Tassioulas", "Roberto Morabito", "Ioannis Pitsiorlas", "Marios Kountouris"], "abstract": "Next-generation mobile networks must reconcile the often-conflicting goals of multiple service owners. However, today's network slice controllers remain rigid, policy-bound, and unaware of the business context. We introduce Agoran Service and Resource Broker (SRB), an agentic marketplace that brings stakeholders directly into the operational loop. Inspired by the ancient Greek agora, Agoran distributes authority across three autonomous AI branches: a Legislative branch that answers compliance queries using retrieval-augmented Large Language Models (LLMs); an Executive branch that maintains real-time situational awareness through a watcher-updated vector database; and a Judicial branch that evaluates each agent message with a rule-based Trust Score, while arbitrating LLMs detect malicious behavior and apply real-time incentives to restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective optimizer, reaching a consensus intent in a single round, which is then deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and evaluated with realistic traces of vehicle mobility, Agoran achieved significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73% reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3% saving in PRB usage compared to a static baseline. An 1B-parameter Llama model, fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80% of GPT-4.1's decision quality, while operating within 6 GiB of memory and converging in only 1.3 seconds. These results establish Agoran as a concrete, standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks. A live demo is presented", "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI)", "comments": "Pre-print submitted to Computer Networks AI-for-6G", "pdf_url": "https://arxiv.org/pdf/2508.09159.pdf", "abstract_url": "https://arxiv.org/abs/2508.09159", "categories": ["Networking and Internet Architecture (cs.NI)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "Agoran是一个为6G RAN自动化设计的代理开放市场，通过三个自主AI分支（立法、执行、司法）实现多方利益相关者的直接参与，显著提升了网络性能。", "motivation": "解决下一代移动网络中多方服务所有者目标冲突的问题，以及当前网络切片控制器的僵化和缺乏业务上下文意识。", "method": "引入Agoran服务和资源代理（SRB），一个代理市场，通过三个AI分支（立法、执行、司法）和谈判代理与调解代理的协商，实现多方利益相关者的直接参与和共识意图的达成。", "result": "在私有5G测试床上部署Agoran，实现了eMBB切片吞吐量增加37%，URLLC切片延迟减少73%，以及端到端PRB使用节省8.3%。", "conclusion": "Agoran为6G网络提供了一条标准对齐、超灵活、以利益相关者为中心的具体路径，其性能提升和高效决策质量展示了其潜力。"}}
{"id": "2508.09170", "title": "Multimodal RAG Enhanced Visual Description", "authors": ["Amit Kumar Jaiswal", "Haiming Liu", "Ingo Frommholz"], "abstract": "Textual descriptions for multimodal inputs entail recurrent refinement of queries to produce relevant output images. Despite efforts to address challenges such as scaling model size and data volume, the cost associated with pre-training and fine-tuning remains substantial. However, pre-trained large multimodal models (LMMs) encounter a modality gap, characterised by a misalignment between textual and visual representations within a common embedding space. Although fine-tuning can potentially mitigate this gap, it is typically expensive and impractical due to the requirement for extensive domain-driven data. To overcome this challenge, we propose a lightweight training-free approach utilising Retrieval-Augmented Generation (RAG) to extend across the modality using a linear mapping, which can be computed efficiently. During inference, this mapping is applied to images embedded by an LMM enabling retrieval of closest textual descriptions from the training set. These textual descriptions, in conjunction with an instruction, cater as an input prompt for the language model to generate new textual descriptions. In addition, we introduce an iterative technique for distilling the mapping by generating synthetic descriptions via the language model facilitating optimisation for standard utilised image description measures. Experimental results on two benchmark multimodal datasets demonstrate significant improvements.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)", "comments": "Accepted by ACM CIKM 2025. 5 pages, 2 figures", "pdf_url": "https://arxiv.org/pdf/2508.09170.pdf", "abstract_url": "https://arxiv.org/abs/2508.09170", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种轻量级、无需训练的检索增强生成（RAG）方法，通过线性映射跨越模态间隙，利用大型多模态模型（LMM）嵌入图像，检索训练集中最接近的文本描述，结合指令生成新的文本描述，并在两个基准多模态数据集上展示了显著改进。", "motivation": "解决多模态输入中文本描述与视觉表示之间的模态间隙问题，以及预训练和微调的高成本问题。", "method": "采用检索增强生成（RAG）和线性映射的方法，无需训练即可跨越模态间隙，利用LMM嵌入图像并检索最接近的文本描述，结合指令生成新的描述。", "result": "在两个基准多模态数据集上的实验结果表明，该方法在标准图像描述度量上取得了显著改进。", "conclusion": "提出的轻量级方法有效解决了模态间隙问题，减少了预训练和微调的成本，为多模态文本描述生成提供了实用的解决方案。"}}
{"id": "2508.09736", "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory", "authors": ["Lin Long", "Yichen He", "Wentao Ye", "Yiyuan Pan", "Yuan Lin", "Hang Li", "Junbo Zhao", "Wei Li"], "abstract": "We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot's perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09736.pdf", "abstract_url": "https://arxiv.org/abs/2508.09736", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "介绍了M3-Agent，一种新型的多模态代理框架，具备长期记忆能力，能够处理实时视觉和听觉输入，并通过强化学习训练在多个基准测试中表现优异。", "motivation": "解决多模态代理在长期记忆和基于记忆的推理方面的挑战，以更接近人类的方式理解和交互。", "method": "开发了M3-Agent框架，采用实体中心的多模态记忆组织方式，并通过强化学习进行训练。", "result": "M3-Agent在M3-Bench-robot、M3-Bench-web和VideoMME-long上的准确率分别比基线模型高出6.7%、7.7%和5.3%。", "conclusion": "M3-Agent框架在多模态代理的长期记忆和推理方面取得了进展，为实际应用设计提供了见解。"}}
{"id": "2508.09171", "title": "webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design", "authors": ["D. Perera"], "abstract": "Current AI agents create significant barriers for users by requiring extensive processing to understand web pages, making AI-assisted web interaction slow and expensive. This paper introduces webMCP (Web Machine Context & Procedure), a client-side standard that embeds structured interaction metadata directly into web pages, enabling more efficient human-AI collaboration on existing websites. webMCP transforms how AI agents understand web interfaces by providing explicit mappings between page elements and user actions. Instead of processing entire HTML documents, agents can access pre-structured interaction data, dramatically reducing computational overhead while maintaining task accuracy. A comprehensive evaluation across 1,890 real API calls spanning online shopping, authentication, and content management scenarios demonstrates webMCP reduces processing requirements by 67.6% while maintaining 97.9% task success rates compared to 98.8% for traditional approaches. Users experience significantly lower costs (34-63% reduction) and faster response times across diverse web interactions. Statistical analysis confirms these improvements are highly significant across multiple AI models. An independent WordPress deployment study validates practical applicability, showing consistent improvements across real-world content management workflows. webMCP requires no server-side modifications, making it deployable across millions of existing websites without technical barriers. These results establish webMCP as a viable solution for making AI web assistance more accessible and sustainable, addressing the critical gap between user interaction needs and AI computational requirements in production environments.", "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09171.pdf", "abstract_url": "https://arxiv.org/abs/2508.09171", "categories": ["Networking and Internet Architecture (cs.NI)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了webMCP，一种客户端标准，通过在网页中嵌入结构化的交互元数据，显著提高了AI代理理解网页界面的效率，减少了计算开销，同时保持了高任务成功率。", "motivation": "当前的AI代理在理解网页时需要大量处理，导致AI辅助的网页交互既慢又昂贵。webMCP旨在解决这一问题，通过提供明确的页面元素与用户动作之间的映射，使AI代理能够更高效地与人类协作。", "method": "webMCP通过在网页中直接嵌入结构化的交互元数据，使AI代理能够访问预结构化的交互数据，而不是处理整个HTML文档，从而减少了计算开销。", "result": "在1,890个真实API调用的综合评估中，webMCP将处理需求减少了67.6%，同时保持了97.9%的任务成功率，与传统方法的98.8%相近。用户在不同网页交互中体验到了显著降低的成本（34-63%减少）和更快的响应时间。", "conclusion": "webMCP无需服务器端修改，可在数百万现有网站上部署，为解决用户交互需求与AI计算需求之间的关键差距提供了可行的解决方案，使AI网页辅助更加可访问和可持续。"}}
{"id": "2508.09848", "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts", "authors": ["Mo Yu", "Tsz Ting Chung", "Chulun Zhou", "Tong Li", "Rui Lu", "Jiangnan Li", "Liyan Xu", "Haoshu Lu", "Ning Zhang", "Jing Li", "Jie Zhou"], "abstract": "We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2508.09848.pdf", "abstract_url": "https://arxiv.org/abs/2508.09848", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "介绍了PRELUDE基准，用于评估通过判断角色的前传故事是否与原书正典叙事一致来评估长上下文理解能力。该任务对全局理解和深度推理提出了更高要求。", "motivation": "解决现有基准在评估长上下文理解和深度推理能力方面的不足，特别是在需要从叙事中搜索和整合间接相关信息的情况下。", "method": "通过创建PRELUDE基准，要求评估角色前传故事与原始叙事的一致性，实证表明88%的实例需要来自叙事多个部分的证据。", "result": "实验结果显示，最先进的LLMs和商业DeepResearch服务在长上下文理解和推理任务上落后人类超过15%，且模型常以错误推理得出正确答案，推理准确率比人类低30%以上。", "conclusion": "研究强调了在长上下文理解和推理方面仍有巨大改进空间，特别是在需要全局理解和深度推理的任务上。"}}
{"id": "2508.09874", "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models", "authors": ["Jiaqi Cao", "Jiarui Wang", "Rubin Wei", "Qipeng Guo", "Kai Chen", "Bowen Zhou", "Zhouhan Lin"], "abstract": "Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09874.pdf", "abstract_url": "https://arxiv.org/abs/2508.09874", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了Memory Decoder，一种即插即用的预训练记忆模块，旨在解决大型语言模型在特定领域适应中的挑战，无需修改原始模型参数即可实现高效领域适应。", "motivation": "大型语言模型在通用语言任务中表现出色，但在特定领域的适应仍面临挑战，如全参数训练成本高、灾难性遗忘问题，以及检索增强生成带来的推理延迟。", "method": "Memory Decoder采用小型变压器解码器学习模仿外部非参数检索器的行为，训练后可无缝集成到任何共享相同分词器的预训练语言模型中。", "result": "实验结果表明，Memory Decoder能够有效适应生物医学、金融和法律三个不同领域，平均降低困惑度6.17点。", "conclusion": "Memory Decoder引入了一种以专门预训练的记忆组件为中心的新范式，支持即插即用方式集成，持续提升目标领域内多个模型的性能。"}}
{"id": "2508.09535", "title": "AI Blob! LLM-Driven Recontextualization of Italian Television Archives", "authors": ["Roberto Balestri"], "abstract": "This paper introduces AI Blob!, an experimental system designed to explore the potential of semantic cataloging and Large Language Models (LLMs) for the retrieval and recontextualization of archival television footage. Drawing methodological inspiration from Italian television programs such as Blob (RAI Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic embeddings, and retrieval-augmented generation (RAG) to organize and reinterpret archival content. The system processes a curated dataset of 1,547 Italian television videos by transcribing audio, segmenting it into sentence-level units, and embedding these segments into a vector database for semantic querying. Upon user input of a thematic prompt, the LLM generates a range of linguistically and conceptually related queries, guiding the retrieval and recombination of audiovisual fragments. These fragments are algorithmically selected and structured into narrative sequences producing montages that emulate editorial practices of ironic juxtaposition and thematic coherence. By foregrounding dynamic, content-aware retrieval over static metadata schemas, AI Blob! demonstrates how semantic technologies can facilitate new approaches to archival engagement, enabling novel forms of automated narrative construction and cultural analysis. The project contributes to ongoing debates in media historiography and AI-driven archival research, offering both a conceptual framework and a publicly available dataset to support further interdisciplinary experimentation.", "subjects": "Multimedia (cs.MM); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Digital Libraries (cs.DL)", "comments": "Preprint", "pdf_url": "https://arxiv.org/pdf/2508.09535.pdf", "abstract_url": "https://arxiv.org/abs/2508.09535", "categories": ["Multimedia (cs.MM)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Digital Libraries (cs.DL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "AI Blob!是一个实验性系统，利用大型语言模型（LLMs）和语义编目技术，对意大利电视档案进行检索和重新语境化。", "motivation": "解决传统静态元数据模式在电视档案检索和重新语境化中的局限性，探索语义技术在档案参与和文化分析中的新方法。", "method": "结合自动语音识别（ASR）、语义嵌入和检索增强生成（RAG）技术，对意大利电视视频进行转录、分段和向量数据库嵌入，实现语义查询和叙事序列的自动构建。", "result": "AI Blob!能够根据用户输入的主题提示，生成相关查询并检索、重组视听片段，创造出具有讽刺性并列和主题一致性的叙事序列。", "conclusion": "AI Blob!展示了语义技术如何促进档案参与的新方法，为媒体史学和AI驱动的档案研究提供了概念框架和公开数据集，支持进一步的跨学科实验。"}}
{"id": "2508.09197", "title": "MX-AI: Agentic Observability and Control Platform for Open and AI-RAN", "authors": ["Ilias Chatzistefanidis", "Andrea Leone", "Ali Yaghoubian", "Mikel Irazabal", "Sehad Nassim", "Lina Bariah", "Merouane Debbah", "Navid Nikaein"], "abstract": "Future 6G radio access networks (RANs) will be artificial intelligence (AI)-native: observed, reasoned about, and re-configured by autonomous agents cooperating across the cloud-edge continuum. We introduce MX-AI, the first end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of Large-Language-Model (LLM)-powered agents inside the Service Management and Orchestration (SMO) layer, and (iii) exposes both observability and control functions for 6G RAN resources through natural-language intents. On 50 realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0 and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end latency when backed by GPT-4.1. Thus, it matches human-expert performance, validating its practicality in real settings. We publicly release the agent graph, prompts, and evaluation harness to accelerate open research on AI-native RANs. A live demo is presented here:", "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI)", "comments": "This work has been submitted to the IEEE for possible publication", "pdf_url": "https://arxiv.org/pdf/2508.09197.pdf", "abstract_url": "https://arxiv.org/abs/2508.09197", "categories": ["Networking and Internet Architecture (cs.NI)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "MX-AI是首个端到端的代理系统，用于未来6G无线接入网络（RAN）的观察、推理和重新配置，通过在服务管理和编排（SMO）层部署大型语言模型（LLM）驱动的代理图，并通过自然语言意图暴露6G RAN资源的可观察性和控制功能。", "motivation": "解决未来6G无线接入网络（RAN）如何通过自主代理在云边缘连续体中合作进行观察、推理和重新配置的问题。", "method": "基于OpenAirInterface（OAI）和FlexRIC的实时5G Open RAN测试床，部署LLM驱动的代理图，并通过自然语言意图暴露6G RAN资源的可观察性和控制功能。", "result": "在50个实际操作查询中，MX-AI的平均回答质量为4.1/5.0，决策-行动准确率为100%，端到端延迟仅为8.8秒，与人类专家性能相匹配。", "conclusion": "MX-AI在实际设置中的实用性得到验证，公开发布了代理图、提示和评估工具以加速AI原生RAN的开放研究。"}}
{"id": "2508.09858", "title": "HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics", "authors": ["Weiqi Li", "Zehao Zhang", "Liang Lin", "Guangrun Wang"], "abstract": "\\textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \\emph{geometric inconsistency} and \\emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \\emph{motion generalization limitations} and \\emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \\textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \\textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \\textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \\textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \\textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09858.pdf", "abstract_url": "https://arxiv.org/abs/2508.09858", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "HumanGenesis是一个集成几何和生成建模的框架，通过四个协作代理解决合成人类动态中的几何不一致和运动泛化限制问题，实现了在文本引导合成、视频重演和新姿势泛化等任务上的最先进性能。", "motivation": "当前合成人类动态的方法面临几何不一致、重建粗糙以及运动泛化能力弱和场景不协调等核心挑战。", "method": "HumanGenesis框架通过四个协作代理：Reconstructor构建3D一致的人类场景表示；Critique Agent通过多轮MLLM-based反射增强重建保真度；Pose Guider使用时间感知参数编码器生成富有表现力的姿势序列；Video Harmonizer通过混合渲染管道合成逼真连贯的视频。", "result": "HumanGenesis在文本引导合成、视频重演和新姿势泛化等任务上实现了最先进的性能，显著提高了表现力、几何保真度和场景整合。", "conclusion": "HumanGenesis通过集成几何和生成建模，有效解决了合成人类动态中的关键挑战，为生成逼真人类动态视频提供了强有力的工具。"}}
{"id": "2508.09230", "title": "Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems", "authors": ["Yutong Wu", "Jie Zhang", "Yiming Li", "Chao Zhang", "Qing Guo", "Nils Lukas", "Tianwei Zhang"], "abstract": "Vision Language Model (VLM)-based agents are stateful, autonomous entities capable of perceiving and interacting with their environments through vision and language. Multi-agent systems comprise specialized agents who collaborate to solve a (complex) task. A core security property is robustness, stating that the system should maintain its integrity under adversarial attacks. However, the design of existing multi-agent systems lacks the robustness consideration, as a successful exploit against one agent can spread and infect other agents to undermine the entire system's assurance. To address this, we propose a new defense approach, Cowpox, to provably enhance the robustness of multi-agent systems. It incorporates a distributed mechanism, which improves the recovery rate of agents by limiting the expected number of infections to other agents. The core idea is to generate and distribute a special cure sample that immunizes an agent against the attack before exposure and helps recover the already infected agents. We demonstrate the effectiveness of Cowpox empirically and provide theoretical robustness guarantees.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09230.pdf", "abstract_url": "https://arxiv.org/abs/2508.09230", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为Cowpox的新防御方法，旨在提高基于视觉语言模型（VLM）的多智能体系统的鲁棒性，通过分布式机制限制感染的预期数量，并生成特殊治愈样本以免疫和恢复受感染的智能体。", "motivation": "现有的多智能体系统设计缺乏对鲁棒性的考虑，一旦一个智能体被成功攻击，攻击可能会扩散并感染其他智能体，从而破坏整个系统的保障。", "method": "提出Cowpox防御方法，采用分布式机制，生成和分发特殊治愈样本，以在暴露前免疫智能体并帮助恢复已感染的智能体。", "result": "通过实证研究证明了Cowpox的有效性，并提供了理论上的鲁棒性保证。", "conclusion": "Cowpox方法能够显著提高多智能体系统的鲁棒性，有效防止攻击的扩散和系统的破坏。"}}
{"id": "2508.09444", "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation", "authors": ["Haoxiang Shi", "Xiang Deng", "Zaijing Li", "Gongwei Chen", "Yaowei Wang", "Liqiang Nie"], "abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires agents to follow natural language instructions through free-form 3D spaces. Existing VLN-CE approaches typically use a two-stage waypoint planning framework, where a high-level waypoint predictor generates the navigable waypoints, and then a navigation planner suggests the intermediate goals in the high-level action space. However, this two-stage decomposition framework suffers from: (1) global sub-optimization due to the proxy objective in each stage, and (2) a performance bottleneck caused by the strong reliance on the quality of the first-stage predicted waypoints. To address these limitations, we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE policy that unifies the traditional two stages, i.e. waypoint generation and planning, into a single diffusion policy. Notably, DifNav employs a conditional diffusion policy to directly model multi-modal action distributions over future actions in continuous navigation space, eliminating the need for a waypoint predictor while enabling the agent to capture multiple possible instruction-following behaviors. To address the issues of compounding error in imitation learning and enhance spatial reasoning in long-horizon navigation tasks, we employ DAgger for online policy training and expert trajectory augmentation, and use the aggregated data to further fine-tune the policy. This approach significantly improves the policy's robustness and its ability to recover from error states. Extensive experiments on benchmark datasets demonstrate that, even without a waypoint predictor, the proposed method substantially outperforms previous state-of-the-art two-stage waypoint-based models in terms of navigation performance. Our code is available at:", "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09444.pdf", "abstract_url": "https://arxiv.org/abs/2508.09444", "categories": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为DAgger Diffusion Navigation（DifNav）的端到端优化VLN-CE策略，通过将传统的两阶段（路径点生成和规划）统一为一个单一的扩散策略，解决了现有方法中的全局次优化和对第一阶段预测路径点质量的强依赖问题。DifNav利用条件扩散策略直接建模连续导航空间中未来动作的多模态动作分布，无需路径点预测器，同时使代理能够捕捉多种可能的指令跟随行为。通过使用DAgger进行在线策略训练和专家轨迹增强，并利用聚合数据进一步微调策略，显著提高了策略的鲁棒性和从错误状态恢复的能力。在基准数据集上的大量实验表明，即使没有路径点预测器，该方法在导航性能上也显著优于之前最先进的两阶段基于路径点的模型。", "motivation": "解决现有VLN-CE方法中由于两阶段分解框架导致的全局次优化和对第一阶段预测路径点质量的强依赖问题。", "method": "提出DAgger Diffusion Navigation（DifNav），一个端到端优化的VLN-CE策略，将传统的两阶段（路径点生成和规划）统一为一个单一的扩散策略，利用条件扩散策略直接建模连续导航空间中未来动作的多模态动作分布，并通过DAgger进行在线策略训练和专家轨迹增强。", "result": "在基准数据集上的实验表明，即使没有路径点预测器，DifNav在导航性能上显著优于之前最先进的两阶段基于路径点的模型。", "conclusion": "DifNav通过将传统的两阶段统一为一个单一的扩散策略，并利用DAgger增强策略的鲁棒性和恢复能力，显著提高了VLN-CE任务的导航性能。"}}
{"id": "2508.09919", "title": "T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis", "authors": ["Xiaojiao Xiao", "Jianfeng Zhao", "Qinmin Vivian Hu", "Guanghui Wang"], "abstract": "Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of liver cancer, significantly improving the classification of the lesion and patient outcomes. However, traditional MRI faces challenges including risks from contrast agent (CA) administration, time-consuming manual assessment, and limited annotated datasets. To address these limitations, we propose a Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a conditional token encoding (CTE) mechanism that unifies anatomical priors and temporal phase information into latent representations; and a dynamic time-aware attention mask (DTAM) that adaptively modulates inter-phase information flow using a Gaussian-decayed attention mechanism, ensuring smooth and physiologically plausible transitions across phases. Furthermore, a constraint for temporal classification consistency (TCC) aligns the lesion classification output with the evolution of the physiological signal, further enhancing diagnostic reliability. Extensive experiments on two independent liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification. This framework offers a clinically relevant and efficient alternative to traditional contrast-enhanced imaging, improving safety, diagnostic efficiency, and reliability for the assessment of liver lesion. The implementation of T-CACE is publicly available at:", "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "comments": "IEEE Journal of Biomedical and Health Informatics, 2025", "pdf_url": "https://arxiv.org/pdf/2508.09919.pdf", "abstract_url": "https://arxiv.org/abs/2508.09919", "categories": ["Image and Video Processing (eess.IV)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种时间条件自回归对比增强多任务框架（T-CACE），用于直接从非对比MRI（NCMRI）合成多期相对比增强MRI（CEMRI），旨在解决传统MRI在肝脏癌症诊断中的挑战。", "motivation": "传统MRI在肝脏癌症诊断中面临对比剂（CA）使用的风险、耗时的手动评估和有限的标注数据集等挑战。", "method": "T-CACE框架引入了三个核心创新：条件令牌编码（CTE）机制、动态时间感知注意力掩码（DTAM）和时间分类一致性（TCC）约束。", "result": "在两个独立的肝脏MRI数据集上的广泛实验表明，T-CACE在图像合成、分割和病变分类方面优于现有技术。", "conclusion": "T-CACE框架为传统对比增强成像提供了一种临床相关且高效的替代方案，提高了肝脏病变评估的安全性、诊断效率和可靠性。"}}
{"id": "2508.09624", "title": "Goal Discovery with Causal Capacity for Efficient Reinforcement Learning", "authors": ["Yan Yu", "Yaodong Yang", "Zhengbo Lu", "Chengdong Ma", "Wengang Zhou", "Houqiang Li"], "abstract": "Causal inference is crucial for humans to explore the world, which can be modeled to enable an agent to efficiently explore the environment in reinforcement learning. Existing research indicates that establishing the causality between action and state transition will enhance an agent to reason how a policy affects its future trajectory, thereby promoting directed exploration. However, it is challenging to measure the causality due to its intractability in the vast state-action space of complex scenarios. In this paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework for efficient environment exploration. Specifically, we first derive a measurement of causality in state space, \\emph{i.e.,} causal capacity, which represents the highest influence of an agent's behavior on future trajectories. After that, we present a Monte Carlo based method to identify critical points in discrete state space and further optimize this method for continuous high-dimensional environments. Those critical points are used to uncover where the agent makes important decisions in the environment, which are then regarded as our subgoals to guide the agent to make exploration more purposefully and efficiently. Empirical results from multi-objective tasks demonstrate that states with high causal capacity align with our expected subgoals, and our GDCC achieves significant success rate improvements compared to baselines.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09624.pdf", "abstract_url": "https://arxiv.org/abs/2508.09624", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新颖的目标发现与因果能力（GDCC）框架，用于强化学习中的高效环境探索。通过测量状态空间中的因果关系（即因果能力），并利用蒙特卡洛方法识别关键点作为子目标，GDCC框架显著提高了探索的效率和目的性。", "motivation": "在强化学习中，因果推理对于代理高效探索环境至关重要。然而，在复杂场景的广阔状态-动作空间中，测量因果关系具有挑战性。本文旨在解决这一问题，通过提出GDCC框架来促进有目的的探索。", "method": "首先推导出状态空间中因果关系的测量方法——因果能力，表示代理行为对未来轨迹的最高影响。然后，提出了一种基于蒙特卡洛的方法来识别离散状态空间中的关键点，并针对连续高维环境优化了该方法。这些关键点被视为子目标，以指导代理更有目的性和效率地探索。", "result": "多目标任务的经验结果表明，具有高因果能力的状态与预期的子目标一致，GDCC框架相比基线方法显著提高了成功率。", "conclusion": "GDCC框架通过测量因果能力和识别关键子目标，有效地促进了强化学习中的高效环境探索，为复杂场景下的探索策略提供了新的视角和方法。"}}
{"id": "2508.09791", "title": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations", "authors": ["Junxiao Han", "Yarong Wang", "Xiaodong Gu", "Cuiyun Gao", "Yao Wan", "Song Han", "David Lo", "Shuiguang Deng"], "abstract": "In this paper, we propose LibRec, a novel framework that integrates the capabilities of LLMs with retrieval-augmented generation(RAG) techniques to automate the recommendation of alternative libraries. The framework further employs in-context learning to extract migration intents from commit messages to enhance the accuracy of its recommendations. To evaluate the effectiveness of LibRec, we introduce LibEval, a benchmark designed to assess the performance in the library migration recommendation task. LibEval comprises 2,888 migration records associated with 2,368 libraries extracted from 2,324 Python repositories. Each migration record captures source-target library pairs, along with their corresponding migration intents and intent types. Based on LibEval, we evaluated the effectiveness of ten popular LLMs within our framework, conducted an ablation study to examine the contributions of key components within our framework, explored the impact of various prompt strategies on the framework's performance, assessed its effectiveness across various intent types, and performed detailed failure case analyses.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2508.09791.pdf", "abstract_url": "https://arxiv.org/abs/2508.09791", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了LibRec，一个结合大型语言模型(LLMs)和检索增强生成(RAG)技术的新框架，用于自动化推荐替代库。该框架利用上下文学习从提交消息中提取迁移意图，以提高推荐的准确性。为了评估LibRec的有效性，作者引入了LibEval基准，包含2,888条迁移记录，涉及2,368个库，来自2,324个Python仓库。通过LibEval，评估了十种流行LLMs在框架中的表现，进行了消融研究，探讨了不同提示策略的影响，并分析了失败案例。", "motivation": "解决在库迁移推荐任务中自动化推荐替代库的问题，提高推荐的准确性和效率。", "method": "结合大型语言模型(LLMs)和检索增强生成(RAG)技术，利用上下文学习从提交消息中提取迁移意图。", "result": "引入了LibEval基准，评估了十种流行LLMs在框架中的表现，进行了消融研究，探讨了不同提示策略的影响，并分析了失败案例。", "conclusion": "LibRec框架通过结合LLMs和RAG技术，有效地自动化了库迁移推荐任务，LibEval基准为评估此类任务提供了标准。"}}
{"id": "2508.09971", "title": "Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model", "authors": ["Zihan Wang", "Nina Mahmoudian"], "abstract": "Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. We formalize river following as a coverage control problem in which the reward function is submodular, yielding diminishing returns as more unique river segments are visited, thereby framing the task as a Submodular Markov Decision Process. First, we introduce Marginal Gain Advantage Estimation, which refines the reward advantage function by using a sliding window baseline computed from historical episodic returns, thus aligning the advantage estimation with the agent's evolving recognition of action value in non-Markovian settings. Second, we develop a Semantic Dynamics Model based on patchified water semantic masks that provides more interpretable and data-efficient short-term prediction of future observations compared to latent vision dynamics models. Third, we present the Constrained Actor Dynamics Estimator architecture, which integrates the actor, the cost estimator, and SDM for cost advantage estimation to form a model-based SafeRL framework capable of solving partially observable Constrained Submodular Markov Decision Processes. Simulation results demonstrate that MGAE achieves faster convergence and superior performance over traditional critic-based methods like Generalized Advantage Estimation. SDM provides more accurate short-term state predictions that enable the cost estimator to better predict potential violations. Overall, CADE effectively integrates safety regulation into model-based RL, with the Lagrangian approach achieving the soft balance of reward and safety during training, while the safety layer enhances performance during inference by hard action overlay.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": "Submitted to Robotics and Autonomous Systems (RAS) journal", "pdf_url": "https://arxiv.org/pdf/2508.09971.pdf", "abstract_url": "https://arxiv.org/abs/2508.09971", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于安全强化学习的视觉驱动无人机河流跟随方法，通过引入边际增益优势估计、语义动力学模型和约束演员动力学估计器架构，解决了在GPS信号不可靠的密集河流环境中自主导航的问题。", "motivation": "解决在GPS信号不可靠的密集河流环境中，无人机自主跟随河流进行救援、监视和环境监测等应用的需求。", "method": "提出了边际增益优势估计（MGAE）来优化奖励优势函数，开发了基于语义水掩膜的语义动力学模型（SDM）进行短期预测，并设计了约束演员动力学估计器（CADE）架构，结合模型基础的安全强化学习框架。", "result": "模拟结果显示，MGAE比传统的基于评论家的方法如广义优势估计（GAE）收敛更快且性能更优，SDM提供了更准确的短期状态预测，CADE有效地将安全规则整合到模型基础的强化学习中。", "conclusion": "通过拉格朗日方法在训练中实现了奖励与安全的软平衡，安全层通过硬动作覆盖在推理中增强了性能，为无人机在复杂环境中的自主导航提供了有效的解决方案。"}}
