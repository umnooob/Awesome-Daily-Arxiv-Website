{"id": "2507.22917", "title": "Reading Between the Timelines: RAG for Answering Diachronic Questions", "authors": ["Kwun Hang Lau", "Ruiyuan Zhang", "Weijie Shi", "Xiaofang Zhou", "Xiaojun Cheng"], "abstract": "While Retrieval-Augmented Generation (RAG) excels at injecting static, factual knowledge into Large Language Models (LLMs), it exhibits a critical deficit in handling longitudinal queries that require tracking entities and phenomena across time. This blind spot arises because conventional, semantically-driven retrieval methods are not equipped to gather evidence that is both topically relevant and temporally coherent for a specified duration. We address this challenge by proposing a new framework that fundamentally redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by disentangling a user's query into its core subject and its temporal window. It then employs a specialized retriever that calibrates semantic matching against temporal relevance, ensuring the collection of a contiguous evidence set that spans the entire queried period. To enable rigorous evaluation of this capability, we also introduce the Analytical Diachronic Question Answering Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus of real and synthetic financial news. Empirical results on ADQAB show that our approach yields substantial gains in answer accuracy, surpassing standard RAG implementations by 13% to 27%. This work provides a validated pathway toward RAG systems capable of performing the nuanced, evolutionary analysis required for complex, real-world questions. The dataset and code for this study are publicly available at", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.22917.pdf", "abstract_url": "https://arxiv.org/abs/2507.22917", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种新的RAG框架，旨在解决传统RAG在处理需要跨时间跟踪实体和现象的纵向查询时的不足，通过引入时间逻辑和专门设计的检索器，显著提高了答案的准确性。", "motivation": "传统基于语义的检索方法在处理需要跨时间跟踪的纵向查询时存在不足，无法确保收集到的证据既主题相关又时间连贯。", "method": "提出了一种新框架，通过将用户的查询分解为核心主题和时间窗口，并采用专门设计的检索器来校准语义匹配与时间相关性，确保收集到跨越整个查询时间段的连续证据集。", "result": "在ADQAB基准测试中，该方法在答案准确性上比标准RAG实现提高了13%至27%。", "conclusion": "这项工作为RAG系统提供了一条经过验证的路径，使其能够执行复杂、现实世界问题所需的细致、进化分析。"}}
{"id": "2507.23242", "title": "Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents", "authors": ["Sungguk Cha", "DongWook Kim", "Taeseung Hahn", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon"], "abstract": "Retrieval-Augmented Generation (RAG) systems rely heavily on effective query formulation to unlock external knowledge, yet optimizing queries for diverse, unstructured real-world documents remains a challenge. We introduce \\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query rewriting that eliminates the need for human-annotated datasets and extends applicability to both text-only and multi-modal databases. By synthesizing scenario-question pairs and leveraging Generalized Reward Policy Optimization (GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing retrieval performance across varied domains. Experiments on industrial in-house data demonstrate significant improvements, with $\\text{RL-QR}_{\\text{multi-modal}}$ achieving an 11\\% relative gain in NDCG@3 for multi-modal RAG and $\\text{RL-QR}_{\\text{lexical}}$ yielding a 9\\% gain for lexical retrievers. However, challenges persist with semantic and hybrid retrievers, where rewriters failed to improve performance, likely due to training misalignments. Our findings highlight RL-QR's potential to revolutionize query optimization for RAG systems, offering a scalable, annotation-free solution for real-world retrieval tasks, while identifying avenues for further refinement in semantic retrieval contexts.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23242.pdf", "abstract_url": "https://arxiv.org/abs/2507.23242", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了RL-QR，一个基于强化学习的查询重写框架，旨在优化检索增强生成（RAG）系统中的查询效率，无需人工标注数据集，适用于文本和多模态数据库。实验显示在工业内部数据上取得了显著改进，但在语义和混合检索器上仍有挑战。", "motivation": "优化检索增强生成（RAG）系统中针对多样化和非结构化真实世界文档的查询效率，是一个未解决的挑战。", "method": "提出了RL-QR框架，通过合成场景-问题对和利用广义奖励策略优化（GRPO），训练特定于检索器的查询重写器。", "result": "在多模态RAG中，RL-QR多模态版本在NDCG@3上实现了11%的相对增益；在词汇检索器中，RL-QR词汇版本实现了9%的增益。但在语义和混合检索器上未能提升性能。", "conclusion": "RL-QR有潜力革新RAG系统的查询优化，为真实世界检索任务提供可扩展、无需标注的解决方案，同时指出了在语义检索环境中进一步改进的方向。"}}
{"id": "2507.22923", "title": "How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting", "authors": ["Aman Gupta", "Yingying Zhuang", "Zhou Yu", "Ziji Zhang", "Anurag Beniwal"], "abstract": "Despite advances in the multilingual capabilities of Large Language Models (LLMs), their performance varies substantially across different languages and tasks. In multilingual retrieval-augmented generation (RAG)-based systems, knowledge bases (KB) are often shared from high-resource languages (such as English) to low-resource ones, resulting in retrieved information from the KB being in a different language than the rest of the context. In such scenarios, two common practices are pre-translation to create a mono-lingual prompt and cross-lingual prompting for direct inference. However, the impact of these choices remains unclear. In this paper, we systematically evaluate the impact of different prompt translation strategies for classification tasks with RAG-enhanced LLMs in multilingual systems. Experimental results show that an optimized prompting strategy can significantly improve knowledge sharing across languages, therefore improve the performance on the downstream classification task. The findings advocate for a broader utilization of multilingual resource sharing and cross-lingual prompt optimization for non-English languages, especially the low-resource ones.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "Accepted at Prompt Optimization KDD '25", "pdf_url": "https://arxiv.org/pdf/2507.22923.pdf", "abstract_url": "https://arxiv.org/abs/2507.22923", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文探讨了在跨语言大型语言模型（LLM）提示中翻译策略的影响，特别是在多语言检索增强生成（RAG）系统中。通过系统评估不同的提示翻译策略，研究发现优化的提示策略能显著改善跨语言知识共享，从而提升下游分类任务的性能。", "motivation": "尽管大型语言模型（LLMs）在多语言能力上有所进步，但在不同语言和任务中的表现仍有显著差异。特别是在多语言检索增强生成（RAG）系统中，知识库（KB）常从高资源语言（如英语）共享到低资源语言，导致检索到的信息与上下文语言不同。这种情况下，预翻译创建单语言提示和跨语言提示直接推理是两种常见做法，但这些选择的影响尚不明确。", "method": "本文系统评估了在多语言系统中，使用RAG增强的LLMs进行分类任务时，不同提示翻译策略的影响。", "result": "实验结果表明，优化的提示策略能显著改善跨语言知识共享，进而提升下游分类任务的性能。", "conclusion": "研究结果提倡更广泛地利用多语言资源共享和跨语言提示优化，特别是对于非英语语言，尤其是低资源语言。"}}
{"id": "2507.22925", "title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "abstract": "Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.22925.pdf", "abstract_url": "https://arxiv.org/abs/2507.22925", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种用于大型语言模型代理（LLM Agents）的分层记忆（H-MEM）架构，旨在通过多层次的语义抽象组织和更新记忆，以提高长期推理的效率和效果。", "motivation": "长期记忆是影响大型语言模型代理推理能力的关键因素之一。现有的记忆存储和检索方法在结构化记忆组织和高效检索方面存在不足。", "method": "提出了一种分层记忆架构（H-MEM），该架构基于语义抽象程度以多层次方式组织和更新记忆，并采用基于索引的路由机制进行高效检索。", "result": "在LoCoMo数据集的五个任务设置上评估了该方法，实验结果表明，H-MEM在长期对话场景中 consistently outperforms 五种基线方法。", "conclusion": "H-MEM架构通过有效的记忆组织和检索机制，显著提升了LLM Agents在长期对话中的决策能力和上下文连贯性。"}}
{"id": "2507.23276", "title": "How Far Are AI Scientists from Changing the World?", "authors": ["Qiujie Xie", "Yixuan Weng", "Minjun Zhu", "Fuchen Shen", "Shulin Huang", "Zhen Lin", "Jiahui Zhou", "Zilan Mao", "Zijie Yang", "Linyi Yang", "Jian Wu", "Yue Zhang"], "abstract": "The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now taking the lead in scientific research. Several influential works have already appeared in the field of AI Scientist systems, with AI-generated research papers having been accepted at the ICLR 2025 workshop, suggesting that a human-level AI Scientist capable of uncovering phenomena previously unknown to humans, may soon become a reality. In this survey, we focus on the central question: How far are AI scientists from changing the world and reshaping the scientific research paradigm? To answer this question, we provide a prospect-driven review that comprehensively analyzes the current achievements of AI Scientist systems, identifying key bottlenecks and the critical components required for the emergence of a scientific agent capable of producing ground-breaking discoveries that solve grand challenges. We hope this survey will contribute to a clearer understanding of limitations of current AI Scientist systems, showing where we are, what is missing, and what the ultimate goals for scientific AI should be.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23276.pdf", "abstract_url": "https://arxiv.org/abs/2507.23276", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了基于大型语言模型（LLMs）的AI科学家系统在科学研究中的进展，评估了它们改变世界和重塑科研范式的潜力，并指出了当前系统的局限性和未来发展的关键要素。", "motivation": "探索AI科学家系统在科学研究中的应用及其潜力，评估它们是否能够解决重大挑战并改变科研范式。", "method": "通过前景驱动的综述，全面分析了AI科学家系统的当前成就，识别了关键瓶颈和实现突破性发现所需的组成部分。", "result": "AI科学家系统已在科研中取得显著进展，但仍存在局限性，需要进一步发展以实现解决重大挑战的能力。", "conclusion": "本文旨在帮助更清晰地理解当前AI科学家系统的局限性，明确现状、缺失部分以及科学AI的终极目标。"}}
{"id": "2507.23330", "title": "AI Must not be Fully Autonomous", "authors": ["Tosin Adewumi", "Lama Alkhaled", "Florent Imbert", "Hui Han", "Nudrat Habib", "Karl Löwenmark"], "abstract": "Autonomous Artificial Intelligence (AI) has many benefits. It also has many risks. In this work, we identify the 3 levels of autonomous AI. We are of the position that AI must not be fully autonomous because of the many risks, especially as artificial superintelligence (ASI) is speculated to be just decades away. Fully autonomous AI, which can develop its own objectives, is at level 3 and without responsible human oversight. However, responsible human oversight is crucial for mitigating the risks. To ague for our position, we discuss theories of autonomy, AI and agents. Then, we offer 12 distinct arguments and 6 counterarguments with rebuttals to the counterarguments. We also present 15 pieces of recent evidence of AI misaligned values and other risks in the appendix.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "11 pages, 1 figure", "pdf_url": "https://arxiv.org/pdf/2507.23330.pdf", "abstract_url": "https://arxiv.org/abs/2507.23330", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文主张人工智能（AI）不应完全自主，因为存在诸多风险，特别是在人工超级智能（ASI）可能几十年内实现的背景下。作者提出了AI自主性的三个级别，并认为第三级别（完全自主，无需人类监督）尤其危险。通过讨论自主性、AI和代理理论，作者提出了12个支持论点、6个反对论点及其反驳，并在附录中提供了15个AI价值观错位及其他风险的近期证据。", "motivation": "探讨AI完全自主性带来的风险，特别是在ASI可能即将到来的背景下，强调负责任的人类监督的重要性。", "method": "通过理论讨论、提出支持与反对论点及反驳，以及提供近期证据来支持其立场。", "result": "提出了AI不应完全自主的12个支持论点，并成功反驳了6个反对论点，同时提供了15个AI风险的近期证据。", "conclusion": "AI的发展必须避免完全自主，特别是在ASI可能实现的情况下，负责任的人类监督是减轻风险的关键。"}}
{"id": "2507.23429", "title": "Chatting with your ERP: A Recipe", "authors": ["Jorge Ruiz Gómez", "Lidia Andrés Susinos", "Jorge Alamo Olivé", "Sonia Rey Osorno", "Manuel Luis Gonzalez Hernández"], "abstract": "This paper presents the design, implementation, and evaluation behind a Large Language Model (LLM) agent that chats with an industrial production-grade ERP system. The agent is capable of interpreting natural language queries and translating them into executable SQL statements, leveraging open-weight LLMs. A novel dual-agent architecture combining reasoning and critique stages was proposed to improve query generation reliability.", "subjects": "Artificial Intelligence (cs.AI); Databases (cs.DB); Emerging Technologies (cs.ET); Human-Computer Interaction (cs.HC); Multiagent Systems (cs.MA)", "comments": "11 pages, includes 3 tables summarizing schema and model performance. Submitted on July 31, 2025. Targets integration of LLM agents with ERP systems using open-weight models and Ollama deployment", "pdf_url": "https://arxiv.org/pdf/2507.23429.pdf", "abstract_url": "https://arxiv.org/abs/2507.23429", "categories": ["Artificial Intelligence (cs.AI)", "Databases (cs.DB)", "Emerging Technologies (cs.ET)", "Human-Computer Interaction (cs.HC)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种大型语言模型（LLM）代理的设计、实现和评估，该代理能够与工业级ERP系统进行对话。通过利用开放权重的LLM，该代理能够解释自然语言查询并将其转换为可执行的SQL语句。为了提高查询生成的可靠性，提出了一种结合推理和批判阶段的新型双代理架构。", "motivation": "解决工业级ERP系统中自然语言查询转换为可执行SQL语句的挑战，提高查询生成的准确性和可靠性。", "method": "采用开放权重的大型语言模型（LLM），并设计了一种新型的双代理架构，结合了推理和批判阶段。", "result": "开发出的代理能够有效解释自然语言查询并生成可执行的SQL语句，双代理架构显著提高了查询生成的可靠性。", "conclusion": "通过结合推理和批判阶段的双代理架构，可以显著提高大型语言模型在工业级ERP系统中处理自然语言查询的准确性和可靠性。"}}
{"id": "2507.23336", "title": "DSBC : Data Science task Benchmarking with Context engineering", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Giulio Martini", "Suman Debnath", "Hamza Farooq"], "abstract": "Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)", "comments": "32 pages", "pdf_url": "https://arxiv.org/pdf/2507.23336.pdf", "abstract_url": "https://arxiv.org/abs/2507.23336", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一个专门设计的基准测试，用于评估数据科学代理在现实世界用户交互中的效果和限制，测试了三种大型语言模型和三种方法，并探讨了模型对常见提示问题的敏感性。", "motivation": "大型语言模型（LLMs）的进步显著影响了数据科学工作流程，催生了专门的数据科学代理来自动化分析任务。然而，系统评估这些代理效果和限制的基准测试仍然稀缺。", "method": "通过观察商业应用的使用情况，设计了反映真实世界用户交互的基准测试，评估了三种LLMs（Claude-4.0-Sonnet、Gemini-2.5-Flash和OpenAI-o4-Mini）和三种方法（零射击与上下文工程、多步与上下文工程、以及使用SmolAgent）。", "result": "研究发现，评估的模型和方法之间存在明显的性能差异，突出了影响实际部署的关键因素。", "conclusion": "引入的基准数据集和评估框架旨在为未来研究更强大和有效的数据科学代理提供基础。"}}
{"id": "2507.23554", "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "abstract": "Large language model-based agents, empowered by in-context learning (ICL), have demonstrated strong capabilities in complex reasoning and tool-use tasks. However, existing works have shown that the effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. While prior work has explored example selection, including in some agentic or multi-step settings, existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps. Therefore, it is non-trivial to develop a principled, general-purpose method for selecting demonstrations that consistently benefit agent performance. In this paper, we address this challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Our approach decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization. We further propose a stepwise selection criterion with a formal guarantee of improved agent performance. Importantly, DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23554.pdf", "abstract_url": "https://arxiv.org/abs/2507.23554", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了DICE，一种基于理论基础的动态上下文示例选择框架，旨在提升大型语言模型代理在复杂推理和工具使用任务中的表现。通过因果视角分解演示知识，并提出具有正式性能保证的逐步选择标准，DICE作为一种通用、框架无关的解决方案，能够无缝集成到现有代理框架中。", "motivation": "现有研究表明，上下文学习（ICL）的效果高度依赖于演示示例的选择，而次优示例往往导致性能不稳定或下降。尽管先前工作探索了示例选择，但缺乏一个普遍、理论基础的准则来定义跨推理步骤的有效演示。", "method": "DICE通过因果视角将演示知识分解为可转移和不可转移的组件，提出了一种逐步选择标准，并展示了如何通过减少不可转移组件引入的虚假依赖来提升代理性能。", "result": "跨多个领域的广泛实验证明了DICE方法的有效性和通用性，强调了基于原则、上下文感知的演示选择对于构建强大且高效的大型语言模型代理的重要性。", "conclusion": "DICE作为一种无需额外训练成本的插件模块，能够显著提升现有代理框架的性能，为开发更稳健和高效的大型语言模型代理提供了新的方向。"}}
{"id": "2507.22927", "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation", "authors": ["Zhehao Tan", "Yihan Jiao", "Dan Yang", "Lei Liu", "Jie Feng", "Duolin Sun", "Yue Shen", "Jian Wang", "Peng Wei", "Jinjie Gu"], "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge, where the LLM's ability to generate responses based on the combination of a given query and retrieved documents is crucial. However, most benchmarks focus on overall RAG system performance, rarely assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects such as noise robustness, but lack a systematic and granular evaluation framework on document utilization. To this end, we introduce \\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark, emphasizing the following progressive dimensions: (1) multi-level filtering abilities, (2) combination abilities, and (3) reference reasoning. To provide a more nuanced understanding of LLMs' roles in RAG systems, we formulate an innovative placeholder-based approach to decouple the contributions of the LLM's parametric knowledge and the external knowledge. Experiments demonstrate the limitations of representative LLMs in the RAG system's generation capabilities, particularly in error resilience and context faithfulness. Our benchmark provides a reproducible framework for developing more reliable and efficient RAG systems. Our code is available in", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.22927.pdf", "abstract_url": "https://arxiv.org/abs/2507.22927", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了PRGB基准，一个用于评估检索增强生成（RAG）系统中大型语言模型（LLM）能力的多级细粒度基准。通过创新的占位符方法，该基准旨在解耦LLM的参数知识和外部知识的贡献，揭示了当前LLM在RAG系统中的局限性。", "motivation": "当前大多数基准测试关注的是RAG系统的整体性能，而很少评估LLM在特定能力上的表现，特别是在文档利用方面的系统性和细粒度评估框架的缺乏。", "method": "提出了一个多级细粒度的基准测试\textit{Placeholder-RAG-Benchmark}，强调三个渐进维度：多级过滤能力、组合能力和参考推理能力，并采用创新的占位符方法来解耦LLM的参数知识和外部知识的贡献。", "result": "实验揭示了代表性LLM在RAG系统生成能力上的局限性，特别是在错误恢复和上下文忠实度方面。", "conclusion": "该基准为开发更可靠和高效的RAG系统提供了一个可复现的框架，有助于更细致地理解LLM在RAG系统中的作用。"}}
{"id": "2507.22929", "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow", "authors": ["Xiaoyu Pan", "Yang Bai", "Ke Zou", "Yang Zhou", "Jun Zhou", "Huazhu Fu", "Yih-Chung Tham", "Yong Liu"], "abstract": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at", "subjects": "Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Multiagent Systems (cs.MA)", "comments": "9 figures, 5 tables. submit/6621751", "pdf_url": "https://arxiv.org/pdf/2507.22929.pdf", "abstract_url": "https://arxiv.org/abs/2507.22929", "categories": ["Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了EH-Benchmark，一个用于评估医学大型语言模型（MLLMs）在眼科诊断中幻觉现象的新基准，并提出了一种基于代理的三阶段框架来减轻这些幻觉。", "motivation": "医学大型语言模型（MLLMs）在眼科诊断中因知识有限、视觉定位和推理能力不足以及多模态眼科数据稀缺而产生的幻觉问题，影响了其准确性和可靠性。现有医学基准无法有效评估或解决这些问题。", "method": "提出了EH-Benchmark基准，将MLLMs的幻觉分为视觉理解和逻辑组合两大类，并进一步细分子类。采用基于代理的三阶段框架，包括知识级检索阶段、任务级案例研究阶段和结果级验证阶段。", "result": "实验结果表明，多代理框架显著减轻了两种类型的幻觉，提高了准确性、可解释性和可靠性。", "conclusion": "EH-Benchmark和提出的三阶段框架为解决MLLMs在眼科诊断中的幻觉问题提供了有效的评估和缓解方法，有助于提升模型的性能和临床应用价值。"}}
{"id": "2507.22931", "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "authors": ["Shuyu Guo", "Zhaochun Ren"], "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.22931.pdf", "abstract_url": "https://arxiv.org/abs/2507.22931", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种名为ACC-RAG的自适应上下文压缩框架，旨在通过动态调整压缩率来优化检索增强生成（RAG）的效率，同时不牺牲准确性。", "motivation": "检索增强生成（RAG）虽然通过外部知识增强了大型语言模型（LLMs），但由于检索到的上下文过长，导致了显著的推理成本。现有的固定压缩率方法要么对简单查询过度压缩，要么对复杂查询压缩不足。", "method": "ACC-RAG框架结合了分层压缩器（用于多粒度嵌入）和上下文选择器，根据输入复杂度动态调整压缩率，保留最小足够信息，类似于人类的略读。", "result": "在Wikipedia和五个QA数据集上的评估显示，ACC-RAG在保持或提高准确性的同时，比固定率方法表现更好，并且比标准RAG实现了超过4倍的推理速度提升。", "conclusion": "ACC-RAG通过自适应压缩率有效优化了RAG的推理效率，为处理不同复杂度的查询提供了一种高效且准确的方法。"}}
{"id": "2507.22932", "title": "FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification", "authors": ["Baptiste Lefort", "Eric Benhamou", "Beatrice Guez", "Jean-Jacques Ohana", "Ethan Setrouk", "Alban Etienne"], "abstract": "This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.", "subjects": "Computation and Language (cs.CL); General Finance (q-fin.GN)", "comments": "8 pages", "pdf_url": "https://arxiv.org/pdf/2507.22932.pdf", "abstract_url": "https://arxiv.org/abs/2507.22932", "categories": ["Computation and Language (cs.CL)", "General Finance (q-fin.GN)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种新颖的分层投资组合优化框架，结合轻量级大型语言模型(LLMs)和深度强化学习(DRL)，将金融新闻的情绪信号与传统市场指标相结合。", "motivation": "解决如何有效结合金融新闻的情绪分析和传统市场数据以优化投资组合的问题。", "method": "采用三层架构：基础RL代理处理混合数据，元代理聚合决策，超级代理基于市场数据和情绪分析合并决策。", "result": "在2018至2024年数据上评估，训练数据为2000-2017年，框架实现了26%的年化回报率和1.2的夏普比率，优于等权重和标普500基准。", "conclusion": "主要贡献包括可扩展的跨模态集成、增强稳定性的分层RL结构以及开源可重复性。"}}
{"id": "2507.22938", "title": "A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents", "authors": ["Sumit Soman", "H. G. Ranjani", "Sujoy Roychowdhury", "Venkata Dharma Surya Narayana Sastry", "Akshat Jain", "Pranav Gangrade", "Ayaaz Khan"], "abstract": "Question-Answering (QA) from technical documents often involves questions whose answers are present in figures, such as flowcharts or flow diagrams. Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such questions. We leverage graph representations of flowcharts obtained from Visual large Language Models (VLMs) and incorporate them in a text-based RAG system to show that this approach can enable image retrieval for QA in the telecom domain. We present the end-to-end approach from processing technical documents, classifying image types, building graph representations, and incorporating them with the text embedding pipeline for efficient retrieval. We benchmark the same on a QA dataset created based on proprietary telecom product information documents. Results show that the graph representations obtained using a fine-tuned VLM model have lower edit distance with respect to the ground truth, which illustrate the robustness of these representations for flowchart images. Further, the approach for QA using these representations gives good retrieval performance using text-based embedding models, including a telecom-domain adapted one. Our approach also alleviates the need for a VLM in inference, which is an important cost benefit for deployed QA systems.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "Accepted for publication at the KDD 2025 Workshop on Structured Knowledge for Large Language Models", "pdf_url": "https://arxiv.org/pdf/2507.22938.pdf", "abstract_url": "https://arxiv.org/abs/2507.22938", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种基于图的多模态问答方法，用于从电信文档中的流程图中提取答案，通过结合视觉大语言模型（VLM）获得的流程图图表示和基于文本的检索增强生成（RAG）系统，提高了问答系统的性能。", "motivation": "解决技术文档中基于流程图的问答问题，传统基于文本的RAG系统在此类问题上表现不佳。", "method": "利用VLM模型获取流程图的图表示，并将其与文本嵌入管道结合，构建了一个端到端的问答系统。", "result": "使用微调的VLM模型获得的图表示与真实值之间的编辑距离较低，表明这些表示对流程图图像具有鲁棒性。此外，该方法在使用基于文本的嵌入模型（包括电信领域适应的模型）时表现出良好的检索性能。", "conclusion": "该方法不仅提高了从流程图中提取答案的准确性，还减少了在推理过程中对VLM的依赖，为部署的问答系统带来了重要的成本效益。"}}
{"id": "2507.23565", "title": "Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI", "authors": ["Botao Zhu", "Xianbin Wang", "Dusit Niyato"], "abstract": "In collaborative systems, the effective completion of tasks hinges on task-specific trust evaluations of potential devices for distributed collaboration. However, the complexity of tasks, the spatiotemporal dynamism of distributed device resources, and the inevitable assessment overhead dramatically increase the complexity and resource consumption of the trust evaluation process. As a result, ill-timed or overly frequent trust evaluations can reduce utilization rate of constrained resources, negatively affecting collaborative task execution. To address this challenge, this paper proposes an autonomous trust orchestration method based on a new concept of semantic chain-of-trust. Our technique employs agentic AI and hypergraph to establish and maintain trust relationships among devices. By leveraging its strengths in autonomous perception, task decomposition, and semantic reasoning, we propose agentic AI to perceive device states and autonomously perform trust evaluations of collaborators based on historical performance data only during device idle periods, thereby enabling efficient utilization of distributed resources. In addition, agentic AI performs task-specific trust evaluations on collaborator resources by analyzing the alignment between resource capabilities and task requirements. Moreover, by maintaining a trust hypergraph embedded with trust semantics for each device, agentic AI enables hierarchical management of collaborators and identifies collaborators requiring trust evaluation based on trust semantics, thereby achieving a balance between overhead and trust accuracy. Furthermore, local trust hypergraphs from multiple devices can be chained together to support multi-hop collaboration, enabling efficient coordination in large-scale systems. Experimental results demonstrate that the proposed method achieves resource-efficient trust evaluation.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23565.pdf", "abstract_url": "https://arxiv.org/abs/2507.23565", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了一种基于语义信任链的自主信任编排方法，利用代理AI和超图技术，在设备空闲期间基于历史性能数据进行信任评估，以实现分布式资源的高效利用。", "motivation": "解决协作系统中任务特定信任评估的复杂性和资源消耗问题，以及由于信任评估不及时或过于频繁而导致的资源利用率下降问题。", "method": "采用代理AI和超图技术，通过自主感知、任务分解和语义推理，进行设备状态的感知和信任评估，同时维护嵌入信任语义的信任超图，实现协作者的分层管理和信任评估的平衡。", "result": "实验结果表明，所提出的方法实现了资源高效的信任评估。", "conclusion": "通过语义信任链和代理AI技术，本文提出的方法能够在大规模系统中实现高效的协作协调，平衡了开销和信任准确性。"}}
{"id": "2507.23633", "title": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying", "authors": ["Qian Zhao", "Zhuo Sun", "Bin Guo", "Zhiwen Yu"], "abstract": "Agent-assisted memory recall is one critical research problem in the field of human-computer interaction. In conventional methods, the agent can retrieve information from its equipped memory module to help the person recall incomplete or vague memories. The limited size of memory module hinders the acquisition of complete memories and impacts the memory recall performance in practice. Memory theories suggest that the person's relevant memory can be proactively activated through some effective cues. Inspired by this, we propose a novel strategy-guided agent-assisted memory recall method, allowing the agent to transform an original query into a cue-rich one via the judiciously designed strategy to help the person recall memories. To this end, there are two key challenges. (1) How to choose the appropriate recall strategy for diverse forgetting scenarios with distinct memory-recall characteristics? (2) How to obtain the high-quality responses leveraging recall strategies, given only abstract and sparsely annotated strategy patterns? To address the challenges, we propose a Recall Router framework. Specifically, we design a 5W Recall Map to classify memory queries into five typical scenarios and define fifteen recall strategy patterns across the corresponding scenarios. We then propose a hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to optimize the selection of strategy and the generation of strategy responses. We construct an instruction tuning dataset and fine-tune multiple open-source large language models (LLMs) to develop MemoCue, an agent that excels in providing memory-inspired responses. Experiments on three representative datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall inspiration. Further human evaluation highlights its advantages in memory-recall applications.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23633.pdf", "abstract_url": "https://arxiv.org/abs/2507.23633", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "MemoCue是一种基于LLM的代理，通过策略引导查询增强人类记忆回忆能力，采用Recall Router框架和蒙特卡洛树搜索算法优化策略选择和响应生成，实验显示其在记忆回忆灵感上优于现有方法。", "motivation": "解决传统代理辅助记忆回忆方法因记忆模块大小限制而影响回忆性能的问题，受记忆理论启发，通过有效线索主动激活相关记忆。", "method": "设计了5W Recall Map分类记忆查询场景，定义十五种回忆策略模式，结合分层回忆树和蒙特卡洛树搜索算法优化策略选择及响应生成，并微调开源大型语言模型开发MemoCue代理。", "result": "在三个代表性数据集上的实验表明，MemoCue在回忆灵感上比基于LLM的方法提高了17.74%，人类评估也凸显了其在记忆回忆应用中的优势。", "conclusion": "MemoCue通过策略引导的查询和优化的策略选择，显著提高了记忆回忆的效率和效果，为人类记忆辅助技术提供了新的研究方向。"}}
{"id": "2507.23701", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "abstract": "Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23701.pdf", "abstract_url": "https://arxiv.org/abs/2507.23701", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了TextQuests，一个基于Infocom互动小说游戏的基准测试，旨在评估AI代理在需要长时间自主推理和探索的环境中的能力。", "motivation": "现有的代理基准测试往往无法全面评估AI代理在需要长时间自主推理和探索的环境中的能力，特别是在排除外部工具使用的情况下。", "method": "通过引入基于Infocom互动小说游戏的TextQuests基准测试，评估LLM代理在需要长时间自主推理和探索的环境中的能力。", "result": "TextQuests基准测试能够有效评估AI代理在需要长时间自主推理和探索的环境中的能力，特别是在排除外部工具使用的情况下。", "conclusion": "TextQuests基准测试为开发能够在需要长时间自主推理和探索的环境中表现出色的AI代理提供了有效的评估工具。"}}
{"id": "2507.23773", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "authors": ["Mingkai Deng", "Jinyu Hou", "Yilin Shen", "Hongxia Jin", "Graham Neubig", "Zhiting Hu", "Eric Xing"], "abstract": "AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \\modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \\modelname improves the success of flight search from 0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent advantage of up to 124\\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \\modelname with pretrained LLMs, available as a research demo for public testing.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23773.pdf", "abstract_url": "https://arxiv.org/abs/2507.23773", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)", "Robotics (cs.RO)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "SimuRA是一种面向通用目标的智能体架构，通过基于大型语言模型（LLM）的世界模型进行模拟推理，旨在克服自回归LLM的局限性，实现更通用和强大的AI代理。", "motivation": "当前基于大型语言模型的AI代理采用一对一任务的方法，缺乏可扩展性和通用性，且受限于自回归LLM的基本限制。本文旨在通过模拟人类推理过程，提出一种更通用和强大的AI代理架构。", "method": "SimuRA引入了一个基于LLM的世界模型，用于通过模拟进行规划，利用自然语言的概念丰富潜在空间在各种环境中灵活规划。", "result": "在困难的网页浏览任务中，SimuRA将航班搜索的成功率从0%提高到32.2%，基于世界模型的规划相比自回归规划显示出高达124%的持续优势。", "conclusion": "SimuRA展示了基于LLM的世界模型模拟作为一种推理范式的优势，为训练一个可以在所有环境中超智能行动的单一通用代理模型提供了可能性。"}}
{"id": "2507.22897", "title": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems", "authors": ["Luyu Chen", "Quanyu Dai", "Zeyu Zhang", "Xueyang Feng", "Mingyu Zhang", "Pengcheng Tang", "Xu Chen", "Yue Zhu", "Zhenhua Dong"], "abstract": "Conversational recommender systems (CRS) enhance user experience through multi-turn interactions, yet evaluating CRS remains challenging. User simulators can provide comprehensive evaluations through interactions with CRS, but building realistic and diverse simulators is difficult. While recent work leverages large language models (LLMs) to simulate user interactions, they still fall short in emulating individual real users across diverse scenarios and lack explicit rating mechanisms for quantitative evaluation. To address these gaps, we propose RecUserSim, an LLM agent-based user simulator with enhanced simulation realism and diversity while providing explicit scores. RecUserSim features several key modules: a profile module for defining realistic and diverse user personas, a memory module for tracking interaction history and discovering unknown preferences, and a core action module inspired by Bounded Rationality theory that enables nuanced decision-making while generating more fine-grained actions and personalized responses. To further enhance output control, a refinement module is designed to fine-tune final responses. Experiments demonstrate that RecUserSim generates diverse, controllable outputs and produces realistic, high-quality dialogues, even with smaller base LLMs. The ratings generated by RecUserSim show high consistency across different base LLMs, highlighting its effectiveness for CRS evaluation.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "comments": "Accepted by TheWebConf'25 Industry Track", "pdf_url": "https://arxiv.org/pdf/2507.22897.pdf", "abstract_url": "https://arxiv.org/abs/2507.22897", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "RecUserSim是一个基于大型语言模型（LLM）代理的用户模拟器，旨在通过增强模拟的真实性和多样性，并提供明确的评分机制，来评估对话推荐系统（CRS）。", "motivation": "现有的对话推荐系统（CRS）评估方法面临模拟用户交互的真实性和多样性不足，以及缺乏明确的评分机制进行定量评估的问题。", "method": "RecUserSim采用LLM代理，包含几个关键模块：用于定义真实多样用户角色的配置文件模块，用于跟踪交互历史和发现未知偏好的记忆模块，以及受有限理性理论启发的核心行动模块，该模块支持细致的决策制定，生成更细粒度的动作和个性化响应。此外，还设计了一个细化模块来微调最终响应。", "result": "实验表明，RecUserSim能够生成多样且可控的输出，并产生真实、高质量的对话，即使使用较小的基础LLM。RecUserSim生成的评分在不同基础LLM之间显示出高度一致性，证明了其在CRS评估中的有效性。", "conclusion": "RecUserSim通过其模块化设计和LLM代理的应用，提供了一种有效的方法来模拟真实多样的用户交互，为对话推荐系统的评估提供了新的可能性。"}}
{"id": "2507.22904", "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches", "authors": ["Ehsan Latif", "Zirak Khan", "Xiaoming Zhai"], "abstract": "Scientific sketches (e.g., models) offer a powerful lens into students' conceptual understanding, yet AI-powered automated assessment of such free-form, visually diverse artifacts remains a critical challenge. Existing solutions often treat sketch evaluation as either an image classification task or monolithic vision-language models, which lack interpretability, pedagogical alignment, and adaptability across cognitive levels. To address these limitations, we present SketchMind, a cognitively grounded, multi-agent framework for evaluating and improving student-drawn scientific sketches. SketchMind comprises modular agents responsible for rubric parsing, sketch perception, cognitive alignment, and iterative feedback with sketch modification, enabling personalized and transparent evaluation. We evaluate SketchMind on a curated dataset of 3,575 student-generated sketches across six science assessment items with different highest order of Bloom's level that require students to draw models to explain phenomena. Compared to baseline GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG integration achieves 77.1% average accuracy (+21.4% average absolute gain). We also demonstrate that multi-agent orchestration with SRG enhances SketchMind performance, for example, GPT-4.1 gains an average 8.9% increase in sketch prediction accuracy, outperforming single-agent pipelines across all items. Human evaluators rated the feedback and co-created sketches generated by \\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5, significantly higher than those of baseline models (e.g., 2.3 for GPT-4o). Experts noted the system's potential to meaningfully support conceptual growth through guided revision. Our code and (pending approval) dataset will be released to support reproducibility and future research in AI-driven education.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "comments": "Submitted to NeurIPS2025", "pdf_url": "https://arxiv.org/pdf/2507.22904.pdf", "abstract_url": "https://arxiv.org/abs/2507.22904", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "SketchMind是一个基于多智能体认知框架的系统，旨在评估和改进学生绘制的科学草图，通过模块化代理实现个性化、透明的评估，并在科学教育中显示出显著的效果提升。", "motivation": "解决现有AI评估学生自由形式、视觉多样的科学草图时缺乏可解释性、教学对齐和跨认知水平适应性的问题。", "method": "采用多智能体框架，包括评分标准解析、草图感知、认知对齐和迭代反馈与草图修改等模块化代理。", "result": "与基线GPT-4o相比，SketchMind在科学评估项目上的平均准确率提高了21.4%，人类评估者对其反馈和共同创作的草图评分显著高于基线模型。", "conclusion": "SketchMind通过多智能体协调和认知对齐，有效支持学生的概念成长，具有在AI驱动教育中应用的潜力。"}}
{"id": "2507.22902", "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "abstract": "Background: Globally we face a projected shortage of 11 million healthcare practitioners by 2030, and administrative burden consumes 50% of clinical time. Artificial intelligence (AI) has the potential to help alleviate these problems. However, no end-to-end autonomous large language model (LLM)-based AI system has been rigorously evaluated in real-world clinical practice. In this study, we evaluated whether a multi-agent LLM-based AI framework can function autonomously as an AI doctor in a virtual urgent care setting. Methods: We retrospectively compared the performance of the multi-agent AI system Doctronic and board-certified clinicians across 500 consecutive urgent-care telehealth encounters. The primary end points: diagnostic concordance, treatment plan consistency, and safety metrics, were assessed by blinded LLM-based adjudication and expert human review. Results: The top diagnosis of Doctronic and clinician matched in 81% of cases, and the treatment plan aligned in 99.2% of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not supported by clinical findings). In an expert review of discordant cases, AI performance was superior in 36.1%, and human performance was superior in 9.3%; the diagnoses were equivalent in the remaining cases. Conclusions: In this first large-scale validation of an autonomous AI doctor, we demonstrated strong diagnostic and treatment plan concordance with human clinicians, with AI performance matching and in some cases exceeding that of practicing clinicians. These findings indicate that multi-agent AI systems achieve comparable clinical decision-making to human providers and offer a potential solution to healthcare workforce shortages.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.22902.pdf", "abstract_url": "https://arxiv.org/abs/2507.22902", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本研究首次大规模验证了自主AI医生在虚拟紧急护理环境中的表现，展示了与人类临床医生在诊断和治疗计划上的高度一致性，AI表现甚至在某些情况下超越人类医生。", "motivation": "全球预计到2030年将短缺1100万医疗从业者，且行政负担占用了临床时间的50%。人工智能（AI）有潜力帮助缓解这些问题，但此前尚未有端到端自主大型语言模型（LLM）为基础的AI系统在真实世界临床实践中被严格评估。", "method": "研究回顾性比较了多代理AI系统Doctronic和获得委员会认证的临床医生在500次连续紧急护理远程医疗遭遇中的表现。主要终点：诊断一致性、治疗计划一致性和安全指标，通过盲法LLM裁决和专家人工审查评估。", "result": "Doctronic和临床医生的最高诊断匹配率为81%，治疗计划一致率为99.2%。未发生临床幻觉（例如，诊断或治疗不受临床发现支持）。在专家审查不一致病例中，AI表现优于人类在36.1%的病例中，人类表现优于AI在9.3%的病例中；其余病例诊断等效。", "conclusion": "在这首次大规模自主AI医生验证中，我们展示了与人类临床医生在诊断和治疗计划上的强一致性，AI表现匹配并在某些情况下超越执业临床医生。这些发现表明，多代理AI系统实现了与人类提供者相当的临床决策，为解决医疗劳动力短缺提供了潜在解决方案。"}}
{"id": "2507.23095", "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity", "authors": ["Ishani Mondal", "Meera Bharadwaj", "Ayush Roy", "Aparna Garimella", "Jordan Lee Boyd-Graber"], "abstract": "We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "Under Submission", "pdf_url": "https://arxiv.org/pdf/2507.23095.pdf", "abstract_url": "https://arxiv.org/abs/2507.23095", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "SMART-Editor是一个多智能体框架，用于在结构化和非结构化领域中进行类似人类的设计编辑，保持全局一致性。", "motivation": "解决现有模型在进行局部编辑时无法保持全局一致性的问题。", "method": "采用两种策略：Reward-Refine（推理时奖励引导的细化方法）和RewardDPO（训练时使用奖励对齐布局对的偏好优化方法）。", "result": "SMART-Editor在结构化设置中比InstructPix2Pix和HIVE等强基线表现更优，RewardDPO实现了高达15%的提升，Reward-Refine在自然图像上显示出优势。", "conclusion": "自动和人工评估证实了奖励引导规划在产生语义一致和视觉对齐的编辑中的价值。"}}
{"id": "2507.23194", "title": "Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks", "authors": ["Jianghui Wang", "Vinay Joshi", "Saptarshi Majumder", "Xu Chao", "Bin Ding", "Ziqiong Liu", "Pratik Prabhanjan Brahma", "Dong Li", "Zicheng Liu", "Emad Barsoum"], "abstract": "The demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23194.pdf", "abstract_url": "https://arxiv.org/abs/2507.23194", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了GEAK框架，一个利用先进LLMs为AMD GPU生成高效Triton代码的系统，以及一个评估Triton基础GPU内核的测试套件。GEAK通过推理时计算缩放和Reflexion式反馈机制，在正确性和执行速度上显著优于基线方法。", "motivation": "随着深度学习工作负载的复杂性和多样性增加，自动化低级内核开发以满足性能和生产力需求变得至关重要。", "method": "GEAK框架利用前沿的LLMs和推理时计算缩放，通过适应自Reflexion式反馈机制的理由循环，生成针对AMD GPU的高性能Triton代码。", "result": "在两个评估基准上，GEAK在正确性上达到了63%，执行速度提升了2.59倍，显著优于直接提示前沿LLMs和基于Reflexion的生成管道的基线方法。", "conclusion": "GEAK类代理代码生成有望加速多样化硬件平台的采用，并 democratizing 访问专家级内核性能。"}}
{"id": "2507.23734", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "authors": ["Dongming Wu", "Yanping Fu", "Saike Huang", "Yingfei Liu", "Fan Jia", "Nian Liu", "Feng Dai", "Tiancai Wang", "Rao Muhammad Anwer", "Fahad Shahbaz Khan", "Jianbing Shen"], "abstract": "General robotic grasping systems require accurate object affordance perception in diverse open-world scenarios following human instructions. However, current studies suffer from the problem of lacking reasoning-based large-scale affordance prediction data, leading to considerable concern about open-world effectiveness. To address this limitation, we build a large-scale grasping-oriented affordance segmentation benchmark with human-like instructions, named RAGNet. It contains 273k images, 180 categories, and 26k reasoning instructions. The images cover diverse embodied data domains, such as wild, robot, ego-centric, and even simulation data. They are carefully annotated with an affordance map, while the difficulty of language instructions is largely increased by removing their category name and only providing functional descriptions. Furthermore, we propose a comprehensive affordance-based grasping framework, named AffordanceNet, which consists of a VLM pre-trained on our massive affordance data and a grasping network that conditions an affordance map to grasp the target. Extensive experiments on affordance segmentation benchmarks and real-robot manipulation tasks show that our model has a powerful open-world generalization ability. Our data and code is available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.23734.pdf", "abstract_url": "https://arxiv.org/abs/2507.23734", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Robotics (cs.RO)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了RAGNet，一个面向通用抓取的大规模基于推理的可用性分割基准，旨在解决当前研究中缺乏基于推理的大规模可用性预测数据的问题。", "motivation": "通用机器人抓取系统需要在多样化的开放世界场景中根据人类指令准确感知物体的可用性。然而，当前研究缺乏基于推理的大规模可用性预测数据，这引发了关于开放世界有效性的重大关切。", "method": "为了解决这一限制，我们构建了一个名为RAGNet的大规模抓取导向的可用性分割基准，包含273k图像、180个类别和26k推理指令。此外，我们提出了一个全面的基于可用性的抓取框架AffordanceNet，该框架包括一个在我们的大规模可用性数据上预训练的VLM和一个根据可用性地图抓取目标的抓取网络。", "result": "在可用性分割基准和真实机器人操作任务上的大量实验表明，我们的模型具有强大的开放世界泛化能力。", "conclusion": "我们的数据和代码已公开，为通用机器人抓取系统的研究和开发提供了宝贵的资源。"}}
{"id": "2507.23772", "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting", "authors": ["Di Li", "Jie Feng", "Jiahao Chen", "Weisheng Dong", "Guanbin Li", "Yuhui Zheng", "Mingtao Feng", "Guangming Shi"], "abstract": "3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23772.pdf", "abstract_url": "https://arxiv.org/abs/2507.23772", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了SeqAffordSplat，一个基于3D高斯泼溅（3DGS）的场景级顺序功能推理新任务，并提出了SeqSplatNet框架，通过大型语言模型和条件解码器生成3D功能掩码序列，以解决复杂场景中的长视野、多对象任务。", "motivation": "现有的基于3D高斯泼溅的功能推理方法局限于单对象、单步交互，无法满足复杂现实应用中长视野、多对象任务的需求。", "method": "提出了SeqSplatNet框架，利用大型语言模型自回归生成文本与分割标记，指导条件解码器生成对应的3D掩码序列，并引入了条件几何重建预训练策略和特征注入机制。", "result": "在包含1800+场景的大规模基准测试中，SeqSplatNet实现了最先进的性能，有效推动了功能推理从单步交互到复杂场景级顺序任务的进步。", "conclusion": "SeqSplatNet通过结合大型语言模型和条件解码器，以及引入的预训练策略和特征注入机制，为复杂3D场景中的顺序功能推理提供了有效的解决方案。"}}
{"id": "2507.23779", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "authors": ["Miaosen Zhang", "Ziqiang Xu", "Jialiang Zhu", "Qi Dai", "Kai Qiu", "Yifan Yang", "Chong Luo", "Tianyi Chen", "Justin Wagle", "Tim Franklin", "Baining Guo"], "abstract": "With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the \\textbf{Phi-Ground} model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under $10B$ parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on ScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: \\href{", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multimedia (cs.MM)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23779.pdf", "abstract_url": "https://arxiv.org/abs/2507.23779", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Multimedia (cs.MM)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了Phi-Ground技术报告，旨在提升GUI接地（GUI grounding）在计算机使用代理（CUAs）中的感知能力。通过实证研究，开发了Phi-Ground模型家族，在多个接地基准测试中取得了最先进的性能。", "motivation": "随着多模态推理模型的发展，计算机使用代理（CUAs）的实现成为可能。GUI接地是CUAs执行实际动作的核心组件，其准确性直接关系到系统的成败。当前端到端接地模型在挑战性基准测试中的准确率仍低于65%，远未达到部署标准。", "method": "本研究对接地模型的训练进行了实证研究，从数据收集到模型训练的细节进行了考察。最终开发了Phi-Ground模型家族，专注于提升模型在代理设置下的性能。", "result": "Phi-Ground模型家族在所有五个接地基准测试中，对于参数小于100亿的模型，均取得了最先进的性能。在端到端模型设置中，该模型在ScreenSpot-pro和UI-Vision上的得分分别为43.2和27.2，同样达到了SOTA结果。", "conclusion": "本文讨论的各种细节，以及我们的成功和失败，不仅阐明了接地模型的构建，也对其他感知任务有所裨益。Phi-Ground模型的成功为GUI接地领域的发展提供了新的方向。"}}
{"id": "2507.22952", "title": "Automated Label Placement on Maps via Large Language Models", "authors": ["Harry Shomer", "Jiejun Xu"], "abstract": "Label placement is a critical aspect of map design, serving as a form of spatial annotation that directly impacts clarity and interpretability. Despite its importance, label placement remains largely manual and difficult to scale, as existing automated systems struggle to integrate cartographic conventions, adapt to context, or interpret labeling instructions. In this work, we introduce a new paradigm for automatic label placement (ALP) that formulates the task as a data editing problem and leverages large language models (LLMs) for context-aware spatial annotation. To support this direction, we curate MAPLE, the first known benchmarking dataset for evaluating ALP on real-world maps, encompassing diverse landmark types and label placement annotations from open-source data. Our method retrieves labeling guidelines relevant to each landmark type leveraging retrieval-augmented generation (RAG), integrates them into prompts, and employs instruction-tuned LLMs to generate ideal label coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall performance and generalization across different types of landmarks. This includes both zero-shot and instruction-tuned performance. Our results demonstrate that LLMs, when guided by structured prompts and domain-specific retrieval, can learn to perform accurate spatial edits, aligning the generated outputs with expert cartographic standards. Overall, our work presents a scalable framework for AI-assisted map finishing and demonstrates the potential of foundation models in structured data editing tasks. The code and data can be found at", "subjects": "Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "comments": "Workshop on AI for Data Editing (AI4DE) at KDD 2025", "pdf_url": "https://arxiv.org/pdf/2507.22952.pdf", "abstract_url": "https://arxiv.org/abs/2507.22952", "categories": ["Human-Computer Interaction (cs.HC)", "Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种利用大型语言模型（LLMs）进行地图标签自动放置（ALP）的新方法，通过将任务制定为数据编辑问题并利用LLMs进行上下文感知的空间注释。", "motivation": "地图标签放置是地图设计中的一个关键方面，但现有的自动化系统难以整合制图惯例、适应上下文或解释标签指令，导致标签放置主要依赖手动操作且难以扩展。", "method": "作者提出了一个框架，该框架通过检索增强生成（RAG）获取与每种地标类型相关的标签指南，将其整合到提示中，并利用指令调优的LLMs生成理想的标签坐标。", "result": "在MAPLE基准数据集上的评估显示，LLMs在结构化提示和领域特定检索的引导下，能够学习执行准确的空间编辑，生成的输出与专家制图标准一致。", "conclusion": "这项工作为AI辅助地图完成提供了一个可扩展的框架，并展示了基础模型在结构化数据编辑任务中的潜力。"}}
{"id": "2507.23227", "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs", "authors": ["Sophie Kearney", "Shu Yang", "Zixuan Wen", "Bojian Hou", "Duy Duong-Tran", "Tianlong Chen", "Jason Moore", "Marylyn Ritchie", "Li Shen"], "abstract": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics.", "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23227.pdf", "abstract_url": "https://arxiv.org/abs/2507.23227", "categories": ["Computation and Language (cs.CL)", "Machine Learning (cs.LG)", "Quantitative Methods (q-bio.QM)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为TAP-GPT的新框架，利用大型语言模型（LLMs）进行阿尔茨海默病（AD）的早期诊断，特别是在小样本量的结构化生物标志物数据上。通过调整TableGPT2模型，TAP-GPT在AD诊断任务上表现优于其他通用LLMs和专门为预测任务开发的表格基础模型（TFM）。", "motivation": "阿尔茨海默病（AD）是一种复杂的神经退行性疾病，其早期和准确诊断需要分析多种异质性生物标志物。这些生物标志物通常以表格形式表示，但传统方法在处理小样本量和多模态数据时面临挑战。", "method": "研究团队提出了TAP-GPT框架，该框架通过构建少量样本的表格提示和使用上下文学习示例，对TableGPT2进行微调，以适应临床二元分类任务（AD或认知正常）。采用了参数高效的qLoRA适应技术来优化模型性能。", "result": "TAP-GPT框架利用TableGPT2的强大表格理解能力和LLMs的先验知识，在AD诊断任务上表现优于更先进的通用LLMs和专门为预测任务开发的TFM。", "conclusion": "这是首次将LLMs应用于基于表格生物标志物数据的预测任务，为未来在生物医学信息学中开发LLM驱动的多智能体框架铺平了道路。"}}
{"id": "2507.23588", "title": "DiffLoRA: Differential Low-Rank Adapters for Large Language Models", "authors": ["Alexandre Misrahi", "Nadezhda Chirkova", "Maxime Louis", "Vassilina Nikoulina"], "abstract": "Differential Transformer has recently been proposed to improve performance in Transformer models by canceling out noise through a denoiser attention mechanism. In this work, we introduce DiffLoRA, a parameter-efficient adaptation of the differential attention mechanism, with low-rank adapters on both positive and negative attention terms. This approach retains the efficiency of LoRA while aiming to benefit from the performance gains of differential attention. We evaluate DiffLoRA across a broad range of NLP tasks, including general benchmarks, many-shot in-context learning, RAG, and long-context tests. We observe that, although DiffLoRA falls short of other parameter-efficient fine-tuning methods in most evaluation tasks, it shows interesting results in certain domains (+11 pts on LoRA for HumanEval). We analyze the attention patterns post-finetuning to identify the reasons for this behavior.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23588.pdf", "abstract_url": "https://arxiv.org/abs/2507.23588", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "DiffLoRA是一种参数高效的差分注意力机制适配器，通过在正负注意力项上使用低秩适配器，旨在结合LoRA的效率和差分注意力的性能提升。", "motivation": "解决Transformer模型中通过差分注意力机制提高性能的问题，同时保持参数效率。", "method": "在正负注意力项上引入低秩适配器（DiffLoRA），保留LoRA的效率同时利用差分注意力的优势。", "result": "在大多数评估任务中，DiffLoRA表现不如其他参数高效的微调方法，但在某些领域（如HumanEval）显示出有趣的结果（比LoRA高11分）。", "conclusion": "DiffLoRA在特定领域展现出潜力，通过分析微调后的注意力模式，可以理解其行为的原因。"}}
{"id": "2507.23334", "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "abstract": "Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)", "comments": "8 pages, 2 figures", "pdf_url": "https://arxiv.org/pdf/2507.23334.pdf", "abstract_url": "https://arxiv.org/abs/2507.23334", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "MusT-RAG是一个基于检索增强生成（RAG）的框架，旨在通过音乐专用向量数据库MusWikiDB和上下文信息的使用，将通用大型语言模型（LLMs）适配于仅文本的音乐问答（MQA）任务，显著提升了模型在音乐领域的适应能力。", "motivation": "大型语言模型（LLMs）在音乐相关应用中效果有限，因为其训练数据中音乐特定知识的比例较小。", "method": "提出MusT-RAG框架，包括音乐专用向量数据库MusWikiDB用于检索阶段，并在推理和微调过程中利用上下文信息，以优化RAG在音乐领域的应用。", "result": "MusT-RAG在增强LLMs音乐领域适应能力方面显著优于传统微调方法，在域内和域外MQA基准测试中均显示出一致的改进。MusWikiDB比通用Wikipedia语料库更有效，提供了更高的性能和计算效率。", "conclusion": "MusT-RAG通过结合音乐专用数据库和上下文信息的使用，有效地将通用LLMs转化为音乐特定模型，为音乐领域的文本问答任务提供了强有力的解决方案。"}}
{"id": "2507.22898", "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "abstract": "We developed a voice-driven artificial intelligence (AI) system that guides anyone - from paramedics to family members - through expert-level stroke evaluations using natural conversation, while also enabling smartphone video capture of key examination components for documentation and potential expert review. This addresses a critical gap in emergency care: current stroke recognition by first responders is inconsistent and often inaccurate, with sensitivity for stroke detection as low as 58%, causing life-threatening delays in treatment. Three non-medical volunteers used our AI system to assess ten simulated stroke patients, including cases with likely large vessel occlusion (LVO) strokes and stroke-like conditions, while we measured diagnostic accuracy, completion times, user confidence, and expert physician review of the AI-generated reports. The AI system correctly identified 84% of individual stroke signs and detected 75% of likely LVOs, completing evaluations in just over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use (mean 4.67/5). The system successfully identified 86% of actual strokes but also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert physician reviewed the AI reports with videos, they identified the correct diagnosis in 100% of cases, but felt confident enough to make preliminary treatment decisions in only 40% of cases due to observed AI errors including incorrect scoring and false information. While the current system's limitations necessitate human oversight, ongoing rapid advancements in speech-to-speech AI models suggest that future versions are poised to enable highly accurate assessments. Achieving human-level voice interaction could transform emergency medical care, putting expert-informed assessment capabilities in everyone's hands.", "subjects": "Human-Computer Interaction (cs.HC); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.22898.pdf", "abstract_url": "https://arxiv.org/abs/2507.22898", "categories": ["Human-Computer Interaction (cs.HC)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "开发了一个语音驱动的人工智能系统，用于通过自然对话引导非专业人员进行专家级中风评估，同时通过智能手机视频记录关键检查内容，以提高院前中风识别的准确性和效率。", "motivation": "解决急救护理中中风识别不一致和不准确的问题，当前第一响应者的中风检测敏感性低至58%，导致治疗延误。", "method": "开发了一个语音AI系统，由非医疗志愿者使用该系统评估模拟中风患者，测量诊断准确性、完成时间、用户信心及专家医生对AI生成报告的审查。", "result": "AI系统正确识别了84%的中风迹象和75%的可能大血管闭塞中风，评估时间约6分钟，用户报告高信心和易用性，但存在误报非中风病例的情况。专家医生审查后能100%正确诊断，但因AI错误仅对40%病例有信心做出初步治疗决定。", "conclusion": "尽管当前系统需要人类监督，但语音到语音AI模型的快速进步预示着未来版本能实现高精度评估，达到人类水平的语音交互可能彻底改变急救医疗护理。"}}
{"id": "2507.23348", "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "abstract": "Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.", "subjects": "Software Engineering (cs.SE); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.23348.pdf", "abstract_url": "https://arxiv.org/abs/2507.23348", "categories": ["Software Engineering (cs.SE)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "SWE-Debate提出了一种竞争性多代理辩论框架，用于软件问题解决，通过多样化推理路径和更巩固的问题定位，提高了问题解决的效率和准确性。", "motivation": "现有的基于代理的问题解决方法主要依赖于代理的独立探索，这往往导致陷入局部解决方案，无法识别跨代码库不同部分的问题模式。", "method": "SWE-Debate首先通过遍历代码依赖图创建多个故障传播痕迹作为定位提案，然后组织三轮辩论，由体现不同推理视角的专门代理参与，最终通过MCTS基础的代码修改代理生成补丁。", "result": "在SWE-bench基准测试中，SWE-Debate在开源代理框架中取得了新的最先进成果，并大幅优于基线。", "conclusion": "SWE-Debate通过竞争性多代理辩论框架，有效地解决了软件问题解决中的局部解决方案和跨代码库问题模式识别的问题，为软件工程任务提供了更高效的解决方案。"}}
{"id": "2507.22947", "title": "ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios", "authors": ["Shou'ang Wei", "Xinyun Wang", "Shuzhen Bi", "Jian Chen", "Ruijia Li", "Bo Jiang", "Xin Lin", "Min Zhang", "Yu Song", "BingDong Li", "Aimin Zhou", "Hao Hao"], "abstract": "The emergence of Large Language Models (LLMs) presents transformative opportunities for education, generating numerous novel application scenarios. However, significant challenges remain: evaluation metrics vary substantially across different educational scenarios, while many emerging scenarios lack appropriate assessment metrics. Current benchmarks predominantly measure general intelligence rather than pedagogical capabilities. To address this gap, we introduce ELMES, an open-source automated evaluation framework specifically designed for assessing LLMs in educational settings. ELMES features a modular architecture that enables researchers to create dynamic, multi-agent dialogues through simple configuration files, facilitating flexible scenario design without requiring extensive programming expertise. The framework incorporates a hybrid evaluation engine that objectively quantifies traditionally subjective pedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic benchmarking of state-of-the-art LLMs across four critical educational scenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching, Interdisciplinary Lesson Plan Generation, and Contextualized Question Generation, employing fine-grained metrics developed in collaboration with education specialists. Our results demonstrate distinct capability distributions among models, revealing context-specific strengths and limitations. ELMES provides educators and researchers with an accessible evaluation framework that significantly reduces adaptation barriers for diverse educational applications while advancing the practical implementation of LLMs in pedagogy. The framework is publicly available at \\emph{", "subjects": "Computers and Society (cs.CY); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.22947.pdf", "abstract_url": "https://arxiv.org/abs/2507.22947", "categories": ["Computers and Society (cs.CY)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "ELMES是一个开源的自动化评估框架，专门设计用于在教育环境中评估大型语言模型（LLMs）。它通过模块化架构和多代理对话，简化了教育场景的设计，并采用LLM-as-a-Judge方法客观量化教学指标。", "motivation": "大型语言模型（LLMs）在教育领域的应用潜力巨大，但现有的评估指标大多针对一般智能而非教学能力，且不同教育场景的评估标准不一，缺乏统一的评估框架。", "method": "ELMES框架采用模块化架构，允许研究者通过简单的配置文件创建动态的多代理对话，无需深入的编程知识。框架内嵌混合评估引擎，使用LLM-as-a-Judge方法客观量化教学指标。", "result": "在四个关键教育场景（知识点解释、引导式问题解决教学、跨学科课程计划生成和情境化问题生成）中，ELMES系统地评估了最先进的LLMs，揭示了模型在不同情境下的能力分布和局限性。", "conclusion": "ELMES为教育工作者和研究者提供了一个易于使用的评估框架，显著降低了LLMs在教育应用中适应的障碍，推动了LLMs在教学实践中的实际应用。"}}
{"id": "2507.23088", "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance", "authors": ["Lalithkumar Seenivasan", "Jiru Xu", "Roger D. Soberanis Mukul", "Hao Ding", "Grayson Byrd", "Yu-Chun Ku", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "abstract": "Emerging surgical data science and robotics solutions, especially those designed to provide assistance in situ, require natural human-machine interfaces to fully unlock their potential in providing adaptive and intuitive aid. Contemporary AI-driven solutions remain inherently rigid, offering limited flexibility and restricting natural human-machine interaction in dynamic surgical environments. These solutions rely heavily on extensive task-specific pre-training, fixed object categories, and explicit manual-prompting. This work introduces a novel Perception Agent that leverages speech-integrated prompt-engineered large language models (LLMs), segment anything model (SAM), and any-point tracking foundation models to enable a more natural human-machine interaction in real-time intraoperative surgical assistance. Incorporating a memory repository and two novel mechanisms for segmenting unseen elements, Perception Agent offers the flexibility to segment both known and unseen elements in the surgical scene through intuitive interaction. Incorporating the ability to memorize novel elements for use in future surgeries, this work takes a marked step towards human-machine symbiosis in surgical procedures. Through quantitative analysis on a public dataset, we show that the performance of our agent is on par with considerably more labor-intensive manual-prompting strategies. Qualitatively, we show the flexibility of our agent in segmenting novel elements (instruments, phantom grafts, and gauze) in a custom-curated dataset. By offering natural human-machine interaction and overcoming rigidity, our Perception Agent potentially brings AI-based real-time assistance in dynamic surgical environments closer to reality.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23088.pdf", "abstract_url": "https://arxiv.org/abs/2507.23088", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种新型的感知代理，利用语音集成的提示工程大型语言模型（LLMs）、分割任何模型（SAM）和任意点跟踪基础模型，以实现更自然的人机交互，用于实时术中手术辅助。", "motivation": "解决当前AI驱动的手术辅助解决方案在动态手术环境中固有的刚性，提供有限的灵活性和限制自然的人机交互的问题。", "method": "采用语音集成的提示工程大型语言模型、分割任何模型和任意点跟踪基础模型，结合记忆存储库和两种新机制，用于分割手术场景中的已知和未见元素。", "result": "定量分析显示，该代理的性能与更为劳动密集的手动提示策略相当；定性分析展示了代理在分割自定义数据集中未见元素（器械、幻影移植和纱布）的灵活性。", "conclusion": "通过提供自然的人机交互并克服刚性，感知代理可能使基于AI的动态手术环境中的实时辅助更接近现实。"}}
{"id": "2507.23674", "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses", "authors": ["Muhammad Taha Cheema", "Abeer Aamir", "Khawaja Gul Muhammad", "Naveed Anwar Bhatti", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "abstract": "Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.", "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)", "comments": "13 pages, 9 figures", "pdf_url": "https://arxiv.org/pdf/2507.23674.pdf", "abstract_url": "https://arxiv.org/abs/2507.23674", "categories": ["Machine Learning (cs.LG)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "TweakLLM是一种新颖的路由架构，旨在通过轻量级LLM动态调整缓存响应以适应传入提示，从而提高缓存效率而不损害用户体验。", "motivation": "解决在使用大型语言模型（LLMs）处理大量查询时，通过缓存优化成本和延迟的同时，保持对用户查询的相关性和个性化交互的挑战。", "method": "采用轻量级LLM动态调整缓存响应，通过用户研究、满意度投票和多代理LLM辩论进行综合评估。", "result": "TweakLLM在保持与前沿模型相当的响应质量的同时，显著提高了缓存效率。", "conclusion": "TweakLLM为高容量LLM部署提供了一种可扩展、资源高效的缓存解决方案，且不损害用户体验。"}}
{"id": "2507.23361", "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "abstract": "Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.", "subjects": "Software Engineering (cs.SE); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.23361.pdf", "abstract_url": "https://arxiv.org/abs/2507.23361", "categories": ["Software Engineering (cs.SE)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了SWE-Exp，一种通过从先前的代理轨迹中提取简洁且可操作的经验来增强软件问题解决能力的方法，实现了跨问题的持续学习。", "motivation": "当前的大型语言模型（LLM）代理在软件问题解决中表现出色，但它们作为无记忆的探索者，无法保留或重用之前修复经验的知识，导致冗余的失败轨迹探索和错过将成功的解决方法应用于类似问题的机会。", "method": "SWE-Exp引入了一个多方面的经验库，捕获成功和失败的修复尝试，从高层次的问题理解到具体的代码变更，提取可重用的问题解决知识。", "result": "实验表明，SWE-Exp在开源代理框架下的SWE-bench-Verified上实现了最先进的解决率（41.6% Pass@1）。", "conclusion": "SWE-Exp建立了一个新的范式，自动化软件工程代理系统地积累和利用修复专业知识，从根本上从试错探索转变为战略性的、经验驱动的问题解决。"}}
{"id": "2507.23217", "title": "Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation", "authors": ["Hyeon Seong Jeong", "Sangwoo Jo", "Byeong Hyun Yoon", "Yoonseok Heo", "Haedong Jeong", "Taehoon Kim"], "abstract": "Understanding complex multimodal documents remains challenging due to their structural inconsistencies and limited training data availability. We introduce \\textit{DocsRay}, a training-free document understanding system that integrates pseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented Generation (RAG). Our approach leverages multimodal Large Language Models' (LLMs) native capabilities to seamlessly process documents containing diverse elements such as text, images, charts, and tables without requiring specialized models or additional training. DocsRay's framework synergistically combines three key techniques: (1) a semantic structuring module using prompt-based LLM interactions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal analysis that converts diverse document elements into unified, text-centric representations using the inherent capabilities of multimodal LLMs, and (3) an efficient two-stage hierarchical retrieval system that reduces retrieval complexity from $O(N)$ to $O(S + k_1 \\cdot N_s)$. Evaluated on documents averaging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency from 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the MMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%, substantially surpassing previous state-of-the-art results.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23217.pdf", "abstract_url": "https://arxiv.org/abs/2507.23217", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了DocsRay，一种无需训练的文档理解系统，通过伪目录生成和分层检索增强生成技术，利用多模态大型语言模型处理复杂多模态文档。", "motivation": "解决复杂多模态文档因结构不一致和训练数据有限而难以理解的问题。", "method": "结合伪目录生成、零样本多模态分析和高效的两阶段分层检索系统，利用多模态大型语言模型的原生能力。", "result": "DocsRay在平均49.4页和20,971个文本标记的文档上，将查询延迟从3.89秒减少到2.12秒，效率提升45%。在MMLongBench-Doc基准测试中，DocsRay-Pro达到64.7%的准确率。", "conclusion": "DocsRay通过其创新框架显著提高了文档理解的效率和准确性，无需额外训练，为处理复杂多模态文档提供了有效解决方案。"}}
{"id": "2507.23261", "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System", "authors": ["Hui Yi Leong", "Yuqing Wu"], "abstract": "Current multi-agent systems (MAS) frameworks often rely on manually designed and static collaboration graph structures, limiting adaptability and performance. To address these limitations, we propose DynaSwarm, a dynamic framework that enhances LLM-based MAS through two key innovations: (1) an actor-critic reinforcement learning (A2C) mechanism to optimize graph structures with improved stability over prior RL methods, and (2) a dynamic graph selector that adaptively chooses the optimal graph structure for each input sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the need for rigid, one-fits-all graph architectures, instead leveraging sample-specific idiosyncrasies to dynamically route queries through specialized agent networks. (c) We propose to fine-tune the demonstration retriever to fully exploit the power of in-context learning (ICL). Extensive experiments on question answering, mathematical reasoning, and coding tasks demonstrate that DynaSwarm consistently outperforms state-of-the-art single-agent and MAS baselines across multiple LLM backbones. Our findings highlight the importance of sample-aware structural flexibility in LLM MAS designs.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23261.pdf", "abstract_url": "https://arxiv.org/abs/2507.23261", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "DynaSwarm是一个动态框架，通过A2C强化学习机制和动态图选择器优化LLM-based多智能体系统的图结构，提高适应性和性能。", "motivation": "解决当前多智能体系统框架依赖手动设计和静态协作图结构，限制适应性和性能的问题。", "method": "采用A2C强化学习机制优化图结构，并通过动态图选择器为每个输入样本自适应选择最优图结构。", "result": "在问答、数学推理和编码任务上，DynaSwarm consistently outperforms state-of-the-art单智能体和多智能体基线。", "conclusion": "样本感知的结构灵活性在LLM多智能体系统设计中至关重要，DynaSwarm通过动态图结构选择提高了系统的适应性和性能。"}}
{"id": "2507.23370", "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "abstract": "Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models (LLMs), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of LLM-based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, pruning, and selection. We conduct extensive experiments using three leading LLMs on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": "Pengfei Gao and Zhao Tian contributed equally to this technical report", "pdf_url": "https://arxiv.org/pdf/2507.23370.pdf", "abstract_url": "https://arxiv.org/abs/2507.23370", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "Trae Agent是一种基于大型语言模型（LLM）的代理，用于软件工程中的问题解决，通过测试时缩放技术提高性能。", "motivation": "解决软件工程中问题解决的挑战，特别是在利用大型语言模型进行实际软件工程任务时，现有基于提示的方法在探索大型集成空间和缺乏仓库级理解能力方面的限制。", "method": "提出Trae Agent，一种基于代理的集成推理方法，通过生成、修剪和选择模块化代理来应对大型集成空间和仓库级理解的挑战。", "result": "在广泛采用的SWE-bench基准测试中，Trae Agent在Pass@1指标上平均比所有基线提高了10.22%，并在SWE-bench Verified排行榜上以75.20%的Pass@1得分位居第一。", "conclusion": "Trae Agent作为一种开源项目发布，支持研究社区，展示了在软件工程问题解决中的卓越性能和潜力。"}}
{"id": "2507.23694", "title": "A survey of multi-agent geosimulation methodologies: from ABM to LLM", "authors": ["Virginia Padilla", "Jacinto Dávila"], "abstract": "We provide a comprehensive examination of agent-based approaches that codify the principles and linkages underlying multi-agent systems, simulations, and information systems. Based on two decades of study, this paper confirms a framework intended as a formal specification for geosimulation platforms. Our findings show that large language models (LLMs) can be effectively incorporated as agent components if they follow a structured architecture specific to fundamental agent activities such as perception, memory, planning, and action. This integration is precisely consistent with the architecture that we formalize, providing a solid platform for next-generation geosimulation systems.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)", "comments": "20 pages, 1 table", "pdf_url": "https://arxiv.org/pdf/2507.23694.pdf", "abstract_url": "https://arxiv.org/abs/2507.23694", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文综述了从基于代理的模型（ABM）到大型语言模型（LLM）的多代理地理模拟方法，提出了一个二十年来研究确认的地理模拟平台的形式化规范框架。", "motivation": "解决多代理系统、模拟和信息系统之间原则和联系的形式化问题，以及如何将大型语言模型有效整合为代理组件。", "method": "基于二十年的研究，提出了一个形式化规范框架，用于地理模拟平台，并探讨了将大型语言模型作为代理组件的结构化架构。", "result": "研究发现，如果大型语言模型遵循特定的结构化架构，可以有效地作为代理组件整合到地理模拟系统中，这与我们形式化的架构完全一致。", "conclusion": "本文提出的框架为下一代地理模拟系统提供了一个坚实的平台，大型语言模型的整合为地理模拟带来了新的可能性。"}}
{"id": "2507.23698", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "authors": ["Shaofei Cai", "Zhancun Mu", "Haiwen Xia", "Bowei Zhang", "Anji Liu", "Yitao Liang"], "abstract": "While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by $4\\times$ and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23698.pdf", "abstract_url": "https://arxiv.org/abs/2507.23698", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了如何通过强化学习（RL）在Minecraft中微调的视觉运动代理实现零样本泛化到未见过的世界，以增强在3D世界中的通用空间推理和交互能力。", "motivation": "强化学习在语言建模方面取得了显著成功，但在视觉运动代理中的应用尚未完全实现。主要挑战是RL模型容易过拟合特定任务或环境，阻碍了在不同设置中获得通用行为。", "method": "通过分析并建立跨视图目标规范作为视觉运动策略的统一多任务目标空间，以及提出在高度可定制的Minecraft环境中进行自动化任务合成以支持大规模多任务RL训练，并构建高效的分布式RL框架。", "result": "实验结果显示，RL将交互成功率提高了4倍，并实现了在包括真实世界设置在内的多样化环境中的零样本空间推理泛化。", "conclusion": "我们的发现强调了在3D模拟环境中，尤其是那些适合大规模任务生成的环境中进行RL训练的巨大潜力，可以显著推进视觉运动代理的空间推理能力。"}}
{"id": "2507.23735", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Yvan R. Petillot"], "abstract": "Achieving robust cognitive autonomy in robots navigating complex, unpredictable environments remains a fundamental challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architecture leveraging distributed Large Language Model AI agents integrated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning-driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA's promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.23735.pdf", "abstract_url": "https://arxiv.org/abs/2507.23735", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了水下机器人自组织自主性（UROSA）架构，利用分布式大型语言模型AI代理在ROS 2框架内实现自主水下车辆的高级认知能力。", "motivation": "解决机器人在复杂、不可预测环境中实现稳健认知自主性的基本挑战。", "method": "采用分布式大型语言模型AI代理，分散认知到专门负责多模态感知、自适应推理、动态任务规划和实时决策的AI代理。", "result": "UROSA在模拟和实际水下任务中显示出优于传统基于规则的架构，在处理意外场景、环境不确定性和新任务目标方面具有显著优势。", "conclusion": "UROSA不仅推进了水下自主性，还建立了一个可扩展、安全且多功能的认知机器人框架，能够推广到多样化的实际应用中。"}}
