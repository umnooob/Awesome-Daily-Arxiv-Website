{"id": "2506.12232", "title": "Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles", "authors": ["Mohammed Elhenawy", "Shadi Jaradat", "Taqwa I. Alhadidi", "Huthaifa I. Ashqar", "Ahmed Jaber", "Andry Rakotonirainy", "Mohammad Abu Tami"], "abstract": "Scene understanding is critical for various downstream tasks in autonomous driving, including facilitating driver-agent communication and enhancing human-centered explainability of autonomous vehicle (AV) decisions. This paper evaluates the capability of four multimodal large language models (MLLMs), including relatively small models, to understand scenes in a zero-shot, in-context learning setting. Additionally, we explore whether combining these models using an ensemble approach with majority voting can enhance scene understanding performance. Our experiments demonstrate that GPT-4o, the largest model, outperforms the others in scene understanding. However, the performance gap between GPT-4o and the smaller models is relatively modest, suggesting that advanced techniques such as improved in-context learning, retrieval-augmented generation (RAG), or fine-tuning could further optimize the smaller models' performance. We also observe mixed results with the ensemble approach: while some scene attributes show improvement in performance metrics such as F1-score, others experience a decline. These findings highlight the need for more sophisticated ensemble techniques to achieve consistent gains across all scene attributes. This study underscores the potential of leveraging MLLMs for scene understanding and provides insights into optimizing their performance for autonomous driving applications.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12232.pdf", "abstract_url": "https://arxiv.org/abs/2506.12232", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent", "@RAG"]}
{"id": "2506.12066", "title": "Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading", "authors": ["Gérôme Meyer", "Philip Breuer"], "abstract": "Digital technologies are increasingly used in education to reduce the workload of teachers and students. However, creating open-ended study or examination questions and grading their answers is still a tedious task. This thesis presents the foundation for a system that generates questions grounded in class materials and automatically grades student answers. It introduces a sophisticated method for chunking documents with a visual layout, specifically targeting PDF documents. This method enhances the accuracy of downstream tasks, including Retrieval Augmented Generation (RAG). Our thesis demonstrates that high-quality questions and reference answers can be generated from study material. Further, it introduces a new benchmark for automated grading of short answers to facilitate comparison of automated grading systems. An evaluation of various grading systems is conducted and indicates that Large Language Models (LLMs) can generalise to the task of automated grading of short answers from their pre-training tasks. As with other tasks, increasing the parameter size of the LLMs leads to greater performance. Currently, available systems still need human oversight, especially in examination scenarios.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12066.pdf", "abstract_url": "https://arxiv.org/abs/2506.12066", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12103", "title": "The Amazon Nova Family of Models: Technical Report and Model Card", "authors": ["Amazon AGI", "Aaron Langford", "Aayush Shah", "Abhanshu Gupta", "Abhimanyu Bhatter", "Abhinav Goyal", "Abhinav Mathur", "Abhinav Mohanty", "Abhishek Kumar", "Abhishek Sethi", "Abi Komma", "Abner Pena", "Achin Jain", "Adam Kunysz", "Adam Opyrchal", "Adarsh Singh", "Aditya Rawal", "Adok Achar Budihal Prasad", "Adrià de Gispert", "Agnika Kumar", "Aishwarya Aryamane", "Ajay Nair", "Akilan M", "Akshaya Iyengar", "Akshaya Vishnu Kudlu Shanbhogue", "Alan He", "Alessandra Cervone", "Alex Loeb", "Alex Zhang", "Alexander Fu", "Alexander Lisnichenko", "Alexander Zhipa", "Alexandros Potamianos", "Ali Kebarighotbi", "Aliakbar Daronkolaei", "Alok Parmesh", "Amanjot Kaur Samra", "Ameen Khan", "Amer Rez", "Amir Saffari", "Amit Agarwalla", "Amit Jhindal", "Amith Mamidala", "Ammar Asmro", "Amulya Ballakur", "Anand Mishra", "Anand Sridharan", "Anastasiia Dubinina", "Andre Lenz", "Andreas Doerr", "Andrew Keating", "Andrew Leaver", "Andrew Smith", "Andrew Wirth", "Andy Davey", "Andy Rosenbaum", "Andy Sohn", "Angela Chan", "Aniket Chakrabarti", "Anil Ramakrishna", "Anirban Roy", "Anita Iyer", "Anjali Narayan-Chen", "Ankith Yennu", "Anna Dabrowska", "Anna Gawlowska", "Anna Rumshisky", "Anna Turek", "Anoop Deoras", "Anton Bezruchkin", "Anup Prasad", "Anupam Dewan", "Anwith Kiran", "Apoorv Gupta", "Aram Galstyan", "Aravind Manoharan", "Arijit Biswas", "Arindam Mandal", "Arpit Gupta", "Arsamkhan Pathan", "Arun Nagarajan", "Arushan Rajasekaram", "Arvind Sundararajan", "Ashwin Ganesan", "Ashwin Swaminathan", "Athanasios Mouchtaris", "Audrey Champeau", "Avik Ray", "Ayush Jaiswal", "Ayush Sharma", "Bailey Keefer", "Balamurugan Muthiah", "Beatriz Leon-Millan", "Ben Koopman", "Ben Li", "Benjamin Biggs", "Benjamin Ott", "Bhanu Vinzamuri", "Bharath Venkatesh", "Bhavana Ganesh"], "abstract": "We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.", "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)", "comments": "48 pages, 10 figures", "pdf_url": "https://arxiv.org/pdf/2506.12103.pdf", "abstract_url": "https://arxiv.org/abs/2506.12103", "categories": ["Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.12149", "title": "Maximally-Informative Retrieval for State Space Model Generation", "authors": ["Evan Becker", "Benjamin Bowman", "Matthew Trager", "Tian Yu Liu", "Luca Zancato", "Wei Xia", "Stefano Soatto"], "abstract": "Given a query and dataset, the optimal way of answering the query is to make use all the information available. Modern LLMs exhibit impressive ability to memorize training data, but data not deemed important during training is forgotten, and information outside that training set cannot be made use of. Processing an entire dataset at inference time is infeasible due to the bounded nature of model resources (e.g. context size in transformers or states in state space models), meaning we must resort to external memory. This constraint naturally leads to the following problem: How can we decide based on the present query and model, what among a virtually unbounded set of known data matters for inference? To minimize model uncertainty for a particular query at test-time, we introduce Retrieval In-Context Optimization (RICO), a retrieval method that uses gradients from the LLM itself to learn the optimal mixture of documents for answer generation. Unlike traditional retrieval-augmented generation (RAG), which relies on external heuristics for document retrieval, our approach leverages direct feedback from the model. Theoretically, we show that standard top-$k$ retrieval with model gradients can approximate our optimization procedure, and provide connections to the leave-one-out loss. We demonstrate empirically that by minimizing an unsupervised loss objective in the form of question perplexity, we can achieve comparable retriever metric performance to BM25 with \\emph{no finetuning}. Furthermore, when evaluated on quality of the final prediction, our method often outperforms fine-tuned dense retrievers such as E5.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12149.pdf", "abstract_url": "https://arxiv.org/abs/2506.12149", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12152", "title": "Because we have LLMs, we Can and Should Pursue Agentic Interpretability", "authors": ["Been Kim", "John Hewitt", "Neel Nanda", "Noah Fiedel", "Oyvind Tafjord"], "abstract": "The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12152.pdf", "abstract_url": "https://arxiv.org/abs/2506.12152", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.12200", "title": "PRO-V: An Efficient Program Generation Multi-Agent System for Automatic RTL Verification", "authors": ["Yujie Zhao", "Zhijing Wu", "Hejia Zhang", "Zhongming Yu", "Wentao Ni", "Chia-Tung Ho", "Haoxing Ren", "Jishen Zhao"], "abstract": "LLM-assisted hardware verification is gaining substantial attention due to its potential to significantly reduce the cost and effort of crafting effective testbenches. It also serves as a critical enabler for LLM-aided end-to-end hardware language design. However, existing current LLMs often struggle with Register Transfer Level (RTL) code generation, resulting in testbenches that exhibit functional errors in Hardware Description Languages (HDL) logic. Motivated by the strong performance of LLMs in Python code generation under inference-time sampling strategies, and their promising capabilities as judge agents, we propose PRO-V a fully program generation multi-agent system for robust RTL verification. Pro-V incorporates an efficient best-of-n iterative sampling strategy to enhance the correctness of generated testbenches. Moreover, it introduces an LLM-as-a-judge aid validation framework featuring an automated prompt generation pipeline. By converting rule-based static analysis from the compiler into natural language through in-context learning, this pipeline enables LLMs to assist the compiler in determining whether verification failures stem from errors in the RTL design or the testbench. PRO-V attains a verification accuracy of 87.17% on golden RTL implementations and 76.28% on RTL mutants. Our code is open-sourced at", "subjects": "Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12200.pdf", "abstract_url": "https://arxiv.org/abs/2506.12200", "categories": ["Artificial Intelligence (cs.AI)", "Hardware Architecture (cs.AR)"], "matching_keywords": ["agent"]}
{"id": "2506.12241", "title": "Privacy Reasoning in Ambiguous Contexts", "authors": ["Ren Yi", "Octavian Suciu", "Adria Gascon", "Sarah Meiklejohn", "Eugene Bagdasarian", "Marco Gruteser"], "abstract": "We study the ability of language models to reason about appropriate information disclosure - a central aspect of the evolving field of agentic privacy. Whereas previous works have focused on evaluating a model's ability to align with human decisions, we examine the role of ambiguity and missing context on model performance when making information-sharing decisions. We identify context ambiguity as a crucial barrier for high performance in privacy assessments. By designing Camber, a framework for context disambiguation, we show that model-generated decision rationales can reveal ambiguities and that systematically disambiguating context based on these rationales leads to significant accuracy improvements (up to 13.3\\% in precision and up to 22.3\\% in recall) as well as reductions in prompt sensitivity. Overall, our results indicate that approaches for context disambiguation are a promising way forward to enhance agentic privacy reasoning.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12241.pdf", "abstract_url": "https://arxiv.org/abs/2506.12241", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.12245", "title": "Reversing the Paradigm: Building AI-First Systems with Human Guidance", "authors": ["Cosimo Spera", "Garima Agrawal"], "abstract": "The relationship between humans and artificial intelligence is no longer science fiction -- it's a growing reality reshaping how we live and work. AI has moved beyond research labs into everyday life, powering customer service chats, personalizing travel, aiding doctors in diagnosis, and supporting educators. What makes this moment particularly compelling is AI's increasing collaborative nature. Rather than replacing humans, AI augments our capabilities -- automating routine tasks, enhancing decisions with data, and enabling creativity in fields like design, music, and writing. The future of work is shifting toward AI agents handling tasks autonomously, with humans as supervisors, strategists, and ethical stewards. This flips the traditional model: instead of humans using AI as a tool, intelligent agents will operate independently within constraints, managing everything from scheduling and customer service to complex workflows. Humans will guide and fine-tune these agents to ensure alignment with goals, values, and context.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12245.pdf", "abstract_url": "https://arxiv.org/abs/2506.12245", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12270", "title": "Cloud Infrastructure Management in the Age of AI Agents", "authors": ["Zhenning Yang", "Archit Bhatnagar", "Yiming Qiu", "Tongyuan Miao", "Patrick Tser Jern Kon", "Yunming Xiao", "Yibo Huang", "Martin Casado", "Ang Chen"], "abstract": "Cloud infrastructure is the cornerstone of the modern IT industry. However, managing this infrastructure effectively requires considerable manual effort from the DevOps engineering team. We make a case for developing AI agents powered by large language models (LLMs) to automate cloud infrastructure management tasks. In a preliminary study, we investigate the potential for AI agents to use different cloud/user interfaces such as software development kits (SDK), command line interfaces (CLI), Infrastructure-as-Code (IaC) platforms, and web portals. We report takeaways on their effectiveness on different management tasks, and identify research challenges and potential solutions.", "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Systems and Control (eess.SY)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12270.pdf", "abstract_url": "https://arxiv.org/abs/2506.12270", "categories": ["Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)", "Machine Learning (cs.LG)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent"]}
{"id": "2506.12317", "title": "The Budget AI Researcher and the Power of RAG Chains", "authors": ["Franklin Lee", "Tengfei Ma"], "abstract": "Navigating the vast and rapidly growing body of scientific literature is a formidable challenge for aspiring researchers. Current approaches to supporting research idea generation often rely on generic large language models (LLMs). While LLMs are effective at aiding comprehension and summarization, they often fall short in guiding users toward practical research ideas due to their limitations. In this study, we present a novel structural framework for research ideation. Our framework, The Budget AI Researcher, uses retrieval-augmented generation (RAG) chains, vector databases, and topic-guided pairing to recombine concepts from hundreds of machine learning papers. The system ingests papers from nine major AI conferences, which collectively span the vast subfields of machine learning, and organizes them into a hierarchical topic tree. It uses the tree to identify distant topic pairs, generate novel research abstracts, and refine them through iterative self-evaluation against relevant literature and peer reviews, generating and refining abstracts that are both grounded in real-world research and demonstrably interesting. Experiments using LLM-based metrics indicate that our method significantly improves the concreteness of generated research ideas relative to standard prompting approaches. Human evaluations further demonstrate a substantial enhancement in the perceived interestingness of the outputs. By bridging the gap between academic data and creative generation, the Budget AI Researcher offers a practical, free tool for accelerating scientific discovery and lowering the barrier for aspiring researchers. Beyond research ideation, this approach inspires solutions to the broader challenge of generating personalized, context-aware outputs grounded in evolving real-world knowledge.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "Intended for AAAI's AI4Research Workshop", "pdf_url": "https://arxiv.org/pdf/2506.12317.pdf", "abstract_url": "https://arxiv.org/abs/2506.12317", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12266", "title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs", "authors": ["Avinash Baidya", "Kamalika Das", "Xiang Gao"], "abstract": "Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)", "comments": "ACL 2025; 18 pages, 8 figures", "pdf_url": "https://arxiv.org/pdf/2506.12266.pdf", "abstract_url": "https://arxiv.org/abs/2506.12266", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.12366", "title": "Ghost Policies: A New Paradigm for Understanding and Learning from Failure in Deep Reinforcement Learning", "authors": ["Xabier Olaz"], "abstract": "Deep Reinforcement Learning (DRL) agents often exhibit intricate failure modes that are difficult to understand, debug, and learn from. This opacity hinders their reliable deployment in real-world applications. To address this critical gap, we introduce ``Ghost Policies,'' a concept materialized through Arvolution, a novel Augmented Reality (AR) framework. Arvolution renders an agent's historical failed policy trajectories as semi-transparent ``ghosts'' that coexist spatially and temporally with the active agent, enabling an intuitive visualization of policy divergence. Arvolution uniquely integrates: (1) AR visualization of ghost policies, (2) a behavioural taxonomy of DRL maladaptation, (3) a protocol for systematic human disruption to scientifically study failure, and (4) a dual-learning loop where both humans and agents learn from these visualized failures. We propose a paradigm shift, transforming DRL agent failures from opaque, costly errors into invaluable, actionable learning resources, laying the groundwork for a new research field: ``Failure Visualization Learning.''", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12366.pdf", "abstract_url": "https://arxiv.org/abs/2506.12366", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12421", "title": "Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM", "authors": ["Dongjie Yang", "Chengqiang Lu", "Qimeng Wang", "Xinbei Ma", "Yan Gao", "Yao Hu", "Hai Zhao"], "abstract": "Travel planning is a complex task requiring the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints and preferences in the context, leading to suboptimal itineraries. We formulate this as an $L^3$ planning problem, emphasizing long context, long instruction, and long output. To tackle this, we introduce Multiple Aspects of Planning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve complex planning problems. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planning models, enabling strong inference-time scalability for better performance. In addition, current benchmarks overlook travel's dynamic nature, where past events impact subsequent journeys, failing to reflect real-world feasibility. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world travel simulation. This work advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through agent-based simulation.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12421.pdf", "abstract_url": "https://arxiv.org/abs/2506.12421", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.12453", "title": "Topology-Assisted Spatio-Temporal Pattern Disentangling for Scalable MARL in Large-scale Autonomous Traffic Control", "authors": ["Rongpeng Li", "Jianhang Zhu", "Jiahao Huang", "Zhifeng Zhao", "Honggang Zhang"], "abstract": "Intelligent Transportation Systems (ITSs) have emerged as a promising solution towards ameliorating urban traffic congestion, with Traffic Signal Control (TSC) identified as a critical component. Although Multi-Agent Reinforcement Learning (MARL) algorithms have shown potential in optimizing TSC through real-time decision-making, their scalability and effectiveness often suffer from large-scale and complex environments. Typically, these limitations primarily stem from a fundamental mismatch between the exponential growth of the state space driven by the environmental heterogeneities and the limited modeling capacity of current solutions. To address these issues, this paper introduces a novel MARL framework that integrates Dynamic Graph Neural Networks (DGNNs) and Topological Data Analysis (TDA), aiming to enhance the expressiveness of environmental representations and improve agent coordination. Furthermore, inspired by the Mixture of Experts (MoE) architecture in Large Language Models (LLMs), a topology-assisted spatial pattern disentangling (TSD)-enhanced MoE is proposed, which leverages topological signatures to decouple graph features for specialized processing, thus improving the model's ability to characterize dynamic and heterogeneous local observations. The TSD module is also integrated into the policy and value networks of the Multi-agent Proximal Policy Optimization (MAPPO) algorithm, further improving decision-making efficiency and robustness. Extensive experiments conducted on real-world traffic scenarios, together with comprehensive theoretical analysis, validate the superior performance of the proposed framework, highlighting the model's scalability and effectiveness in addressing the complexities of large-scale TSC tasks.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12453.pdf", "abstract_url": "https://arxiv.org/abs/2506.12453", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12482", "title": "Tiered Agentic Oversight: A Hierarchical Multi-Agent System for AI Safety in Healthcare", "authors": ["Yubin Kim", "Hyewon Jeong", "Chanwoo Park", "Eugene Park", "Haipeng Zhang", "Xin Liu", "Hyeonhoon Lee", "Daniel McDuff", "Marzyeh Ghassemi", "Cynthia Breazeal", "Samir Tulebaev", "Hae Won Park"], "abstract": "Current large language models (LLMs), despite their power, can introduce safety risks in clinical settings due to limitations such as poor error detection and single point of failure. To address this, we propose Tiered Agentic Oversight (TAO), a hierarchical multi-agent framework that enhances AI safety through layered, automated supervision. Inspired by clinical hierarchies (e.g., nurse, physician, specialist), TAO conducts agent routing based on task complexity and agent roles. Leveraging automated inter- and intra-tier collaboration and role-playing, TAO creates a robust safety framework. Ablation studies reveal that TAO's superior performance is driven by its adaptive tiered architecture, which improves safety by over 3.2% compared to static single-tier configurations; the critical role of its lower tiers, particularly tier 1, whose removal most significantly impacts safety; and the strategic assignment of more advanced LLM to these initial tiers, which boosts performance by over 2% compared to less optimal allocations while achieving near-peak safety efficiently. These mechanisms enable TAO to outperform single-agent and multi-agent frameworks in 4 out of 5 healthcare safety benchmarks, showing up to an 8.2% improvement over the next-best methods in these evaluations. Finally, we validate TAO via an auxiliary clinician-in-the-loop study where integrating expert feedback improved TAO's accuracy in medical triage from 40% to 60%.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12482.pdf", "abstract_url": "https://arxiv.org/abs/2506.12482", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.12483", "title": "MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination", "authors": ["Ao Jia", "Haiming Wu", "Guohui Yao", "Dawei Song", "Songkun Ji", "Yazhou Zhang"], "abstract": "Large language models (LLMs) are prone to three types of hallucination: Input-Conflicting, Context-Conflicting and Fact-Conflicting hallucinations. The purpose of this study is to mitigate the different types of hallucination by exploiting the interdependence between them. For this purpose, we propose a Multi-Information Adapter for Large Language Models (MALM). This framework employs a tailored multi-graph learning approach designed to elucidate the interconnections between original inputs, contextual information, and external factual knowledge, thereby alleviating the three categories of hallucination within a cohesive framework. Experiments were carried out on four benchmarking datasets: HaluEval, TruthfulQA, Natural Questions, and TriviaQA. We evaluated the proposed framework in two aspects: (1) adaptability to different base LLMs on HaluEval and TruthfulQA, to confirm if MALM is effective when applied on 7 typical LLMs. MALM showed significant improvements over LLaMA-2; (2) generalizability to retrieval-augmented generation (RAG) by combining MALM with three representative retrievers (BM25, Spider and DPR) separately. Furthermore, automated and human evaluations were conducted to substantiate the correctness of experimental results, where GPT-4 and 3 human volunteers judged which response was better between LLaMA-2 and MALM. The results showed that both GPT-4 and human preferred MALM in 79.4% and 65.6% of cases respectively. The results validate that incorporating the complex interactions between the three types of hallucination through a multilayered graph attention network into the LLM generation process is effective to mitigate the them. The adapter design of the proposed approach is also proven flexible and robust across different base LLMs.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12483.pdf", "abstract_url": "https://arxiv.org/abs/2506.12483", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12508", "title": "AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving", "authors": ["Wentao Zhang", "Ce Cui", "Yilei Zhao", "Yang Liu", "Bo An"], "abstract": "Recent advances in agent systems based on large language models (LLMs) have demonstrated strong capabilities in solving complex tasks. However, most current methods lack mechanisms for coordinating specialized agents and have limited ability to generalize to new or diverse domains. We introduce \\projectname, a hierarchical multi-agent framework for general-purpose task solving that integrates high-level planning with modular agent collaboration. Inspired by the way a conductor orchestrates a symphony and guided by the principles of \\textit{extensibility}, \\textit{multimodality}, \\textit{modularity}, and \\textit{coordination}, \\projectname features a central planning agent that decomposes complex objectives and delegates sub-tasks to a team of specialized agents. Each sub-agent is equipped with general programming and analytical tools, as well as abilities to tackle a wide range of real-world specific tasks, including data analysis, file operations, web navigation, and interactive reasoning in dynamic multimodal environments. \\projectname supports flexible orchestration through explicit sub-goal formulation, inter-agent communication, and adaptive role allocation. We evaluate the framework on three widely used benchmark datasets covering various real-world tasks, searching web pages, reasoning over heterogeneous modalities, etc. Experimental results demonstrate that \\projectname consistently outperforms flat-agent and monolithic baselines in task success rate and adaptability. These findings highlight the effectiveness of hierarchical organization and role specialization in building scalable and general-purpose LLM-based agent systems.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12508.pdf", "abstract_url": "https://arxiv.org/abs/2506.12508", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12494", "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation", "authors": ["Zhuocheng Zhang", "Yang Feng", "Min Zhang"], "abstract": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \\textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \\href{", "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": "Accepted by ACL 2025 Demo", "pdf_url": "https://arxiv.org/pdf/2506.12494.pdf", "abstract_url": "https://arxiv.org/abs/2506.12494", "categories": ["Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12664", "title": "Behavioral Generative Agents for Energy Operations", "authors": ["Cong Chen", "Omer Karaduman", "Xu Kuang"], "abstract": "Accurately modeling consumer behavior in energy operations remains challenging due to inherent uncertainties, behavioral complexities, and limited empirical data. This paper introduces a novel approach leveraging generative agents--artificial agents powered by large language models--to realistically simulate customer decision-making in dynamic energy operations. We demonstrate that these agents behave more optimally and rationally in simpler market scenarios, while their performance becomes more variable and suboptimal as task complexity rises. Furthermore, the agents exhibit heterogeneous customer preferences, consistently maintaining distinct, persona-driven reasoning patterns. Our findings highlight the potential value of integrating generative agents into energy management simulations to improve the design and effectiveness of energy policies and incentive programs.", "subjects": "Artificial Intelligence (cs.AI); Systems and Control (eess.SY)", "comments": "33 pages, 14 figures", "pdf_url": "https://arxiv.org/pdf/2506.12664.pdf", "abstract_url": "https://arxiv.org/abs/2506.12664", "categories": ["Artificial Intelligence (cs.AI)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent"]}
{"id": "2506.12666", "title": "LIFELONG SOTOPIA: Evaluating Social Intelligence of Language Agents Over Lifelong Social Interactions", "authors": ["Hitesh Goel", "Hao Zhu"], "abstract": "Humans engage in lifelong social interactions through interacting with different people under different scenarios for different social goals. This requires social intelligence to gather information through a long time span and use it to navigate various social contexts effectively. Whether AI systems are also capable of this is understudied in the existing research. In this paper, we present a novel benchmark, LIFELONG-SOTOPIA, to perform a comprehensive evaluation of language agents by simulating multi-episode interactions. In each episode, the language agents role-play characters to achieve their respective social goals in randomly sampled social tasks. With LIFELONG-SOTOPIA, we find that goal achievement and believability of all of the language models that we test decline through the whole interaction. Although using an advanced memory method improves the agents' performance, the best agents still achieve a significantly lower goal completion rate than humans on scenarios requiring an explicit understanding of interaction history. These findings show that we can use LIFELONG-SOTOPIA to evaluate the social intelligence of language agents over lifelong social interactions.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12666.pdf", "abstract_url": "https://arxiv.org/abs/2506.12666", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12689", "title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation", "authors": ["Xiaofeng Shi", "Qian Kou", "Yuduo Li", "Ning Tang", "Jinxin Xie", "Longbin Yu", "Songjing Wang", "Hua Zhou"], "abstract": "The rapid growth of scientific literature demands robust tools for automated survey-generation. However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations. To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm. SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement. We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls. Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores. Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency. Overall, SciSage offers a promising foundation for research-assistive writing tools.", "subjects": "Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12689.pdf", "abstract_url": "https://arxiv.org/abs/2506.12689", "categories": ["Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["agent"]}
{"id": "2506.12801", "title": "Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents", "authors": ["LeCheng Zhang", "Yuanshi Wang", "Haotian Shen", "Xujie Wang"], "abstract": "The Da Vinci Code, a game of logical deduction and imperfect information, presents unique challenges for artificial intelligence, demanding nuanced reasoning beyond simple pattern recognition. This paper investigates the efficacy of various AI paradigms in mastering this game. We develop and evaluate three distinct agent architectures: a Transformer-based baseline model with limited historical context, several Large Language Model (LLM) agents (including Gemini, DeepSeek, and GPT variants) guided by structured prompts, and an agent based on Proximal Policy Optimization (PPO) employing a Transformer encoder for comprehensive game history processing. Performance is benchmarked against the baseline, with the PPO-based agent demonstrating superior win rates ($58.5\\% \\pm 1.0\\%$), significantly outperforming the LLM counterparts. Our analysis highlights the strengths of deep reinforcement learning in policy refinement for complex deductive tasks, particularly in learning implicit strategies from self-play. We also examine the capabilities and inherent limitations of current LLMs in maintaining strict logical consistency and strategic depth over extended gameplay, despite sophisticated prompting. This study contributes to the broader understanding of AI in recreational games involving hidden information and multi-step logical reasoning, offering insights into effective agent design and the comparative advantages of different AI approaches.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12801.pdf", "abstract_url": "https://arxiv.org/abs/2506.12801", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12841", "title": "WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench", "authors": ["Xinyuan Xia", "Yuanyi Song", "Haomin Ma", "Jinyu Cai"], "abstract": "With the rapid development of LLM-based agents, increasing attention has been given to their social interaction and strategic reasoning capabilities. However, existing Werewolf-based benchmarking platforms suffer from overly simplified game settings, incomplete evaluation metrics, and poor scalability. To address these limitations, we propose WereWolf-Plus, a multi-model, multi-dimensional, and multi-method benchmarking platform for evaluating multi-agent strategic reasoning in the Werewolf game. The platform offers strong extensibility, supporting customizable configurations for roles such as Seer, Witch, Hunter, Guard, and Sheriff, along with flexible model assignment and reasoning enhancement strategies for different roles. In addition, we introduce a comprehensive set of quantitative evaluation metrics for all special roles, werewolves, and the sheriff, and enrich the assessment dimensions for agent reasoning ability, cooperation capacity, and social influence. WereWolf-Plus provides a more flexible and reliable environment for advancing research on inference and strategic interaction within multi-agent communities. Our code is open sourced at", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12841.pdf", "abstract_url": "https://arxiv.org/abs/2506.12841", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.13063", "title": "PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue", "authors": ["George Shaikovski", "Eugene Vorontsov", "Adam Casson", "Julian Viret", "Eric Zimmermann", "Neil Tenenholtz", "Yi Kan Wang", "Jan H. Bernhard", "Ran A. Godrich", "Juan A. Retamero", "Razik Yousfi", "Nicolo Fusi", "Thomas J. Fuchs", "Kristen Severson", "Siqi Liu"], "abstract": "Recent pathology foundation models can provide rich tile-level representations but fall short of delivering general-purpose clinical utility without further extensive model development. These models lack whole-slide image (WSI) understanding and are not trained with large-scale diagnostic data, limiting their performance on diverse downstream tasks. We introduce PRISM2, a multi-modal slide-level foundation model trained via clinical dialogue to enable scalable, generalizable pathology AI. PRISM2 is trained on nearly 700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic reports in a two-stage process. In Stage 1, a vision-language model is trained using contrastive and captioning objectives to align whole slide embeddings with textual clinical diagnosis. In Stage 2, the language model is unfrozen to enable diagnostic conversation and extract more clinically meaningful representations from hidden states. PRISM2 achieves strong performance on diagnostic and biomarker prediction tasks, outperforming prior slide-level models including PRISM and TITAN. It also introduces a zero-shot yes/no classification approach that surpasses CLIP-style methods without prompt tuning or class enumeration. By aligning visual features with clinical reasoning, PRISM2 improves generalization on both data-rich and low-sample tasks, offering a scalable path forward for building general pathology AI agents capable of assisting diagnostic and prognostic decisions.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13063.pdf", "abstract_url": "https://arxiv.org/abs/2506.13063", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.12571", "title": "DoTA-RAG: Dynamic of Thought Aggregation RAG", "authors": ["Saksorn Ruangtanusak", "Natthapath Rungseesiripak", "Peerawat Rojratchadakorn", "Monthol Charattrakool", "Natapong Nitarach"], "abstract": "In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": "SIGIR LiveRAG 2025 (oral presentation)", "pdf_url": "https://arxiv.org/pdf/2506.12571.pdf", "abstract_url": "https://arxiv.org/abs/2506.12571", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12607", "title": "Towards Building General Purpose Embedding Models for Industry 4.0 Agents", "authors": ["Christodoulos Constantinides", "Shuxin Lin", "Dhaval Patel"], "abstract": "In this work we focus on improving language models' understanding for asset maintenance to guide the engineer's decisions and minimize asset downtime. Given a set of tasks expressed in natural language for Industry 4.0 domain, each associated with queries related to a specific asset, we want to recommend relevant items and generalize to queries of similar assets. A task may involve identifying relevant sensors given a query about an asset's failure mode.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12607.pdf", "abstract_url": "https://arxiv.org/abs/2506.12607", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.12657", "title": "Synthetic Socratic Debates: Examining Persona Effects on Moral Decision and Persuasion Dynamics", "authors": ["Jiarui Liu", "Yueqi Song", "Yunze Xiao", "Mingqian Zheng", "Lindia Tjuatja", "Jana Schaich Borg", "Mona Diab", "Maarten Sap"], "abstract": "As large language models (LLMs) are increasingly used in morally sensitive domains, it is crucial to understand how persona traits affect their moral reasoning and persuasive behavior. We present the first large-scale study of multi-dimensional persona effects in AI-AI debates over real-world moral dilemmas. Using a 6-dimensional persona space (age, gender, country, class, ideology, and personality), we simulate structured debates between AI agents over 131 relationship-based cases. Our results show that personas affect initial moral stances and debate outcomes, with political ideology and personality traits exerting the strongest influence. Persuasive success varies across traits, with liberal and open personalities reaching higher consensus and win rates. While logit-based confidence grows during debates, emotional and credibility-based appeals diminish, indicating more tempered argumentation over time. These trends mirror findings from psychology and cultural studies, reinforcing the need for persona-aware evaluation frameworks for AI moral reasoning.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12657.pdf", "abstract_url": "https://arxiv.org/abs/2506.12657", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.12894", "title": "Homeostatic Coupling for Prosocial Behavior", "authors": ["Naoto Yoshida", "Kingson Man"], "abstract": "When regarding the suffering of others, we often experience personal distress and feel compelled to help\\footnote{Preprint. Under review.}. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \\emph{observe} their partner's internal state ({\\bf cognitive empathy}) or the agent's internal state can be \\emph{directly coupled} to that of their partner ({\\bf affective empathy}). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Additionally, we show that empathy can be learned: agents can ``decode\" their partner's external emotive states to infer the partner's internal homeostatic states. Assuming some level of physiological similarity, agents reference their own emotion-generation functions to invert the mapping from outward display to internal state. Overall, we demonstrate the emergence of prosocial behavior when homeostatic agents learn to ``read\" the emotions of others and then to empathize, or feel as they feel.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "Preprint. Unver review", "pdf_url": "https://arxiv.org/pdf/2506.12894.pdf", "abstract_url": "https://arxiv.org/abs/2506.12894", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2506.12927", "title": "Sectoral Coupling in Linguistic State Space", "authors": ["Sebastian Dumbrava"], "abstract": "This work presents a formal framework for quantifying the internal dependencies between functional subsystems within artificial agents whose belief states are composed of structured linguistic fragments. Building on the Semantic Manifold framework, which organizes belief content into functional sectors and stratifies them across hierarchical levels of abstraction, we introduce a system of sectoral coupling constants that characterize how one cognitive sector influences another within a fixed level of abstraction. The complete set of these constants forms an agent-specific coupling profile that governs internal information flow, shaping the agent's overall processing tendencies and cognitive style. We provide a detailed taxonomy of these intra-level coupling roles, covering domains such as perceptual integration, memory access and formation, planning, meta-cognition, execution control, and affective modulation. We also explore how these coupling profiles generate feedback loops, systemic dynamics, and emergent signatures of cognitive behavior. Methodologies for inferring these profiles from behavioral or internal agent data are outlined, along with a discussion of how these couplings evolve across abstraction levels. This framework contributes a mechanistic and interpretable approach to modeling complex cognition, with applications in AI system design, alignment diagnostics, and the analysis of emergent agent behavior.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": "56 pages, 12 figures", "pdf_url": "https://arxiv.org/pdf/2506.12927.pdf", "abstract_url": "https://arxiv.org/abs/2506.12927", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.12928", "title": "Scaling Test-time Compute for LLM Agents", "authors": ["King Zhu", "Hanhao Li", "Siwei Wu", "Tianshun Xing", "Dehua Ma", "Xiangru Tang", "Minghao Liu", "Jian Yang", "Jiaheng Liu", "Yuchen Eleanor Jiang", "Changwang Zhang", "Chenghua Lin", "Jun Wang", "Ge Zhang", "Wangchunshu Zhou"], "abstract": "Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12928.pdf", "abstract_url": "https://arxiv.org/abs/2506.12928", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12981", "title": "Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing", "authors": ["Safayat Bin Hakim", "Muhammad Adil", "Alvaro Velasquez", "Houbing Herbert Song"], "abstract": "Retrieval-Augmented Generation (RAG) systems address factual inconsistencies in Large Language Models by grounding generation in external knowledge, yet they face a fundamental efficiency problem: simple queries consume computational resources equivalent to complex multi-hop reasoning tasks. We present SymRAG, a neuro-symbolic framework that introduces adaptive query routing based on real-time complexity and system load assessments. SymRAG dynamically selects symbolic, neural, or hybrid processing paths to align resource use with query demands. Evaluated on 2,000 queries from HotpotQA and DROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0% exact match accuracy with significantly lower CPU utilization (3.6--6.2%) and processing time (0.985--3.165s). Disabling adaptive logic results in 169--1151% increase in processing time, highlighting the framework's impact. These results underscore the potential of adaptive neuro-symbolic routing for scalable, sustainable AI systems.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12981.pdf", "abstract_url": "https://arxiv.org/abs/2506.12981", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2506.13037", "title": "MAGIC: Multi-Agent Argumentation and Grammar Integrated Critiquer", "authors": ["Joaquin Jordan", "Xavier Yin", "Melissa Fabros", "Gireeja Ranade", "Narges Norouzi"], "abstract": "Automated Essay Scoring (AES) and Automatic Essay Feedback (AEF) systems aim to reduce the workload of human raters in educational assessment. However, most existing systems prioritize numeric scoring accuracy over the quality of feedback. This paper presents Multi-Agent Argumentation and Grammar Integrated Critiquer (MAGIC), a framework that uses multiple specialized agents to evaluate distinct writing aspects to both predict holistic scores and produce detailed, rubric-aligned feedback. To support evaluation, we curated a novel dataset of past GRE practice test essays with expert-evaluated scores and feedback. MAGIC outperforms baseline models in both essay scoring , as measured by Quadratic Weighted Kappa (QWK). We find that despite the improvement in QWK, there are opportunities for future work in aligning LLM-generated feedback to human preferences.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13037.pdf", "abstract_url": "https://arxiv.org/abs/2506.13037", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.13260", "title": "COME: Adding Scene-Centric Forecasting Control to Occupancy World Model", "authors": ["Yining Shi", "Kun Jiang", "Qiang Meng", "Ke Wang", "Jiabao Wang", "Wenchao Sun", "Tuopu Wen", "Mengmeng Yang", "Diange Yang"], "abstract": "World models are critical for autonomous driving to simulate environmental dynamics and generate synthetic data. Existing methods struggle to disentangle ego-vehicle motion (perspective shifts) from scene evolvement (agent interactions), leading to suboptimal predictions. Instead, we propose to separate environmental changes from ego-motion by leveraging the scene-centric coordinate systems. In this paper, we introduce COME: a framework that integrates scene-centric forecasting Control into the Occupancy world ModEl. Specifically, COME first generates ego-irrelevant, spatially consistent future features through a scene-centric prediction branch, which are then converted into scene condition using a tailored ControlNet. These condition features are subsequently injected into the occupancy world model, enabling more accurate and controllable future occupancy predictions. Experimental results on the nuScenes-Occ3D dataset show that COME achieves consistent and significant improvements over state-of-the-art (SOTA) methods across diverse configurations, including different input sources (ground-truth, camera-based, fusion-based occupancy) and prediction horizons (3s and 8s). For example, under the same settings, COME achieves 26.3% better mIoU metric than DOME and 23.7% better mIoU metric than UniScene. These results highlight the efficacy of disentangled representation learning in enhancing spatio-temporal prediction fidelity for world models. Code and videos will be available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13260.pdf", "abstract_url": "https://arxiv.org/abs/2506.13260", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2506.13113", "title": "Dynamic Reinsurance Treaty Bidding via Multi-Agent Reinforcement Learning", "authors": ["Stella C. Dong", "James R. Finlay"], "abstract": "This paper develops a novel multi-agent reinforcement learning (MARL) framework for reinsurance treaty bidding, addressing long-standing inefficiencies in traditional broker-mediated placement processes. We pose the core research question: Can autonomous, learning-based bidding systems improve risk transfer efficiency and outperform conventional pricing approaches in reinsurance markets?", "subjects": "Artificial Intelligence (cs.AI); General Economics (econ.GN)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13113.pdf", "abstract_url": "https://arxiv.org/abs/2506.13113", "categories": ["Artificial Intelligence (cs.AI)", "General Economics (econ.GN)"], "matching_keywords": ["agent"]}
{"id": "2506.13131", "title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery", "authors": ["Alexander Novikov", "Ngân Vũ", "Marvin Eisenberger", "Emilien Dupont", "Po-Sen Huang", "Adam Zsolt Wagner", "Sergey Shirobokov", "Borislav Kozlovskii", "Francisco J. R. Ruiz", "Abbas Mehrabian", "M. Pawan Kumar", "Abigail See", "Swarat Chaudhuri", "George Holland", "Alex Davies", "Sebastian Nowozin", "Pushmeet Kohli", "Matej Balog"], "abstract": "In this white paper, we present AlphaEvolve, an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important computational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct algorithms that surpass state-of-the-art solutions on a spectrum of problems in mathematics and computer science, significantly expanding the scope of prior automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve developed a search algorithm that found a procedure to multiply two $4 \\times 4$ complex-valued matrices using $48$ scalar multiplications; offering the first improvement, after 56 years, over Strassen's algorithm in this setting. We believe AlphaEvolve and coding agents like it can have a significant impact in improving solutions of problems across many areas of science and computation.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13131.pdf", "abstract_url": "https://arxiv.org/abs/2506.13131", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Neural and Evolutionary Computing (cs.NE)"], "matching_keywords": ["agent"]}
{"id": "2506.13245", "title": "A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs", "authors": ["Guoxi Zhang", "Jiawei Chen", "Tianzhuo Yang", "Jiaming Ji", "Yaodong Yang", "Juntao Dai"], "abstract": "The increasing prevalence of large language models (LLMs) is influencing global value systems. However, these models frequently exhibit a pronounced WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due to lack of attention to minority values. This monocultural perspective may reinforce dominant values and marginalize diverse cultural viewpoints, posing challenges for the development of equitable and inclusive AI systems. In this work, we introduce a systematic framework designed to boost fair and robust cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium and employ a game-theoretic negotiation method based on Policy-Space Response Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To evaluate this approach, we construct regional cultural agents using data transformed from the World Values Survey (WVS). Beyond the conventional model-level evaluation method, We further propose two quantitative metrics, Perplexity-based Acceptence and Values Self-Consistency, to assess consensus outcomes. Experimental results indicate that our approach generates consensus of higher quality while ensuring more balanced compromise compared to baselines. Overall, it mitigates WEIRD bias by guiding agents toward convergence through fair and gradual negotiation steps.", "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13245.pdf", "abstract_url": "https://arxiv.org/abs/2506.13245", "categories": ["Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)", "Computer Science and Game Theory (cs.GT)"], "matching_keywords": ["agent"]}
{"id": "2506.13324", "title": "Towards Pervasive Distributed Agentic Generative AI -- A State of The Art", "authors": ["Gianni Molinari", "Fabio Ciravegna"], "abstract": "The rapid advancement of intelligent agents and Large Language Models (LLMs) is reshaping the pervasive computing field. Their ability to perceive, reason, and act through natural language understanding enables autonomous problem-solving in complex pervasive environments, including the management of heterogeneous sensors, devices, and data. This survey outlines the architectural components of LLM agents (profiling, memory, planning, and action) and examines their deployment and evaluation across various scenarios. Than it reviews computational and infrastructural advancements (cloud to edge) in pervasive computing and how AI is moving in this field. It highlights state-of-the-art agent deployment strategies and applications, including local and distributed execution on resource-constrained devices. This survey identifies key challenges of these agents in pervasive computing such as architectural, energetic and privacy limitations. It finally proposes what we called \"Agent as a Tool\", a conceptual framework for pervasive agentic AI, emphasizing context awareness, modularity, security, efficiency and effectiveness.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13324.pdf", "abstract_url": "https://arxiv.org/abs/2506.13324", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.13065", "title": "MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?", "authors": ["Xixian Yong", "Jianxun Lian", "Xiaoyuan Yi", "Xiao Zhou", "Xing Xie"], "abstract": "Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about \"love & belonging\" motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs. The dataset, benchmark, and code are available at", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13065.pdf", "abstract_url": "https://arxiv.org/abs/2506.13065", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.13070", "title": "CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right", "authors": ["Jaebok Lee", "Yonghyun Ryu", "Seongmin Park", "Yoonjung Choi"], "abstract": "In this paper, we describe our approach for the SemEval 2025 Task 2 on Entity-Aware Machine Translation (EA-MT). Our system aims to improve the accuracy of translating named entities by combining two key approaches: Retrieval Augmented Generation (RAG) and iterative self-refinement techniques using Large Language Models (LLMs). A distinctive feature of our system is its self-evaluation mechanism, where the LLM assesses its own translations based on two key criteria: the accuracy of entity translations and overall translation quality. We demonstrate how these methods work together and effectively improve entity handling while maintaining high-quality translations.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "The 19th International Workshop on Semantic Evaluation", "pdf_url": "https://arxiv.org/pdf/2506.13070.pdf", "abstract_url": "https://arxiv.org/abs/2506.13070", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2506.13109", "title": "Leveraging In-Context Learning for Language Model Agents", "authors": ["Shivanshu Gupta", "Sameer Singh", "Ashish Sabharwal", "Tushar Khot", "Ben Bogin"], "abstract": "In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "16 pages, 12 figures", "pdf_url": "https://arxiv.org/pdf/2506.13109.pdf", "abstract_url": "https://arxiv.org/abs/2506.13109", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.13590", "title": "Agent Capability Negotiation and Binding Protocol (ACNBP)", "authors": ["Ken Huang", "Akram Sheriff", "Vineeth Sai Narajala", "Idan Habler"], "abstract": "As multi-agent systems evolve to encompass increasingly diverse and specialized agents, the challenge of enabling effective collaboration between heterogeneous agents has become paramount, with traditional agent communication protocols often assuming homogeneous environments or predefined interaction patterns that limit their applicability in dynamic, open-world scenarios. This paper presents the Agent Capability Negotiation and Binding Protocol (ACNBP), a novel framework designed to facilitate secure, efficient, and verifiable interactions between agents in heterogeneous multi-agent systems through integration with an Agent Name Service (ANS) infrastructure that provides comprehensive discovery, negotiation, and binding mechanisms. The protocol introduces a structured 10-step process encompassing capability discovery, candidate pre-screening and selection, secure negotiation phases, and binding commitment with built-in security measures including digital signatures, capability attestation, and comprehensive threat mitigation strategies, while a key innovation of ACNBP is its protocolExtension mechanism that enables backward-compatible protocol evolution and supports diverse agent architectures while maintaining security and interoperability. We demonstrate ACNBP's effectiveness through a comprehensive security analysis using the MAESTRO threat modeling framework, practical implementation considerations, and a detailed example showcasing the protocol's application in a document translation scenario, with the protocol addressing critical challenges in agent autonomy, capability verification, secure communication, and scalable agent ecosystem management.", "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Multiagent Systems (cs.MA)", "comments": "14 pages, 5 figures", "pdf_url": "https://arxiv.org/pdf/2506.13590.pdf", "abstract_url": "https://arxiv.org/abs/2506.13590", "categories": ["Artificial Intelligence (cs.AI)", "Cryptography and Security (cs.CR)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2506.13741", "title": "PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning", "authors": ["Brahim Driss", "Alex Davey", "Riad Akrour"], "abstract": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13741.pdf", "abstract_url": "https://arxiv.org/abs/2506.13741", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.13589", "title": "Omni-AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented for Efficient Long Video Understanding", "authors": ["Zhucun Xue", "Jiangning Zhang", "Xurong Xie", "Yuxuan Cai", "Yong Liu", "Xiangtai Li", "Dacheng Tao"], "abstract": "Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos use static retrieval strategies, leading to inefficiencies for simple queries and information loss for complex tasks. To address this, we propose AdaVideoRAG, a novel framework that dynamically adapts retrieval granularity based on query complexity using a lightweight intent classifier. Our framework employs an Omni-Knowledge Indexing module to build hierarchical databases from text (captions, ASR, OCR), visual features, and semantic graphs, enabling optimal resource allocation across tasks. We also introduce the HiVU benchmark for comprehensive evaluation. Experiments demonstrate improved efficiency and accuracy for long-video understanding, with seamless integration into existing MLLMs. AdaVideoRAG establishes a new paradigm for adaptive retrieval in video analysis. Codes will be open-sourced at", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13589.pdf", "abstract_url": "https://arxiv.org/abs/2506.13589", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["@RAG"]}
{"id": "2506.13654", "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning", "authors": ["Shulin Tian", "Ruiqi Wang", "Hongming Guo", "Penghao Wu", "Yuhao Dong", "Xiuying Wang", "Jingkang Yang", "Hao Zhang", "Hongyuan Zhu", "Ziwei Liu"], "abstract": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2506.13654.pdf", "abstract_url": "https://arxiv.org/abs/2506.13654", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12078", "title": "Modeling Earth-Scale Human-Like Societies with One Billion Agents", "authors": ["Haoxiang Guan", "Jiyan He", "Liyang Fan", "Zhenzhen Ren", "Shaobin He", "Xin Yu", "Yuan Chen", "Shuxin Zheng", "Tie-Yan Liu", "Zhen Liu"], "abstract": "Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY); Social and Information Networks (cs.SI)", "comments": "Work in progress", "pdf_url": "https://arxiv.org/pdf/2506.12078.pdf", "abstract_url": "https://arxiv.org/abs/2506.12078", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Computers and Society (cs.CY)", "Social and Information Networks (cs.SI)"], "matching_keywords": ["agent"]}
{"id": "2506.13474", "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning", "authors": ["David Bani-Harouni", "Chantal Pellegrini", "Ege Özsoy", "Matthias Keicher", "Nassir Navab"], "abstract": "Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited \"out-of-the-box\" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13474.pdf", "abstract_url": "https://arxiv.org/abs/2506.13474", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.13599", "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation", "authors": ["Yuwei Du", "Jie Feng", "Jian Yuan", "Yong Li"], "abstract": "Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered \\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation (\\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \\textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \\textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \\textbf{CAMS} generates more realistic and plausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13599.pdf", "abstract_url": "https://arxiv.org/abs/2506.13599", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.12094", "title": "Military AI Cyber Agents (MAICAs) Constitute a Global Threat to Critical Infrastructure", "authors": ["Timothy Dubber", "Seth Lazar"], "abstract": "This paper argues that autonomous AI cyber-weapons - Military-AI Cyber Agents (MAICAs) - create a credible pathway to catastrophic risk. It sets out the technical feasibility of MAICAs, explains why geopolitics and the nature of cyberspace make MAICAs a catastrophic risk, and proposes political, defensive-AI and analogue-resilience measures to blunt the threat.", "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12094.pdf", "abstract_url": "https://arxiv.org/abs/2506.12094", "categories": ["Computers and Society (cs.CY)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12098", "title": "\"I Hadn't Thought About That\": Creators of Human-like AI Weigh in on Ethics And Neurodivergence", "authors": ["Naba Rizvi", "Taggert Smith", "Tanvi Vidyala", "Mya Bolds", "Harper Strickland", "Andrew Begel", "Rua Williams", "Imani Munyaka"], "abstract": "Human-like AI agents such as robots and chatbots are becoming increasingly popular, but they present a variety of ethical concerns. The first concern is in how we define humanness, and how our definition impacts communities historically dehumanized by scientific research. Autistic people in particular have been dehumanized by being compared to robots, making it even more important to ensure this marginalization is not reproduced by AI that may promote neuronormative social behaviors. Second, the ubiquitous use of these agents raises concerns surrounding model biases and accessibility. In our work, we investigate the experiences of the people who build and design these technologies to gain insights into their understanding and acceptance of neurodivergence, and the challenges in making their work more accessible to users with diverse needs. Even though neurodivergent individuals are often marginalized for their unique communication styles, nearly all participants overlooked the conclusions their end-users and other AI system makers may draw about communication norms from the implementation and interpretation of humanness applied in participants' work. This highlights a major gap in their broader ethical considerations, compounded by some participants' neuronormative assumptions about the behaviors and traits that distinguish \"humans\" from \"bots\" and the replication of these assumptions in their work. We examine the impact this may have on autism inclusion in society and provide recommendations for additional systemic changes towards more ethical research directions.", "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)", "comments": "published at FAccT 2025, 15 pages, 2 tables, 4 figures", "pdf_url": "https://arxiv.org/pdf/2506.12098.pdf", "abstract_url": "https://arxiv.org/abs/2506.12098", "categories": ["Computers and Society (cs.CY)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12100", "title": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis", "authors": ["Reza Fayyazi", "Michael Zuzak", "Shanchieh Jay Yang"], "abstract": "Security vulnerabilities are rapidly increasing in frequency and complexity, creating a shifting threat landscape that challenges cybersecurity defenses. Large Language Models (LLMs) have been widely adopted for cybersecurity threat analysis. When querying LLMs, dealing with new, unseen vulnerabilities is particularly challenging as it lies outside LLMs' pre-trained distribution. Retrieval-Augmented Generation (RAG) pipelines mitigate the problem by injecting up-to-date authoritative sources into the model context, thus reducing hallucinations and increasing the accuracy in responses. Meanwhile, the deployment of LLMs in security-sensitive environments introduces challenges around trust and safety. This raises a critical open question: How to quantify or attribute the generated response to the retrieved context versus the model's pre-trained knowledge? This work proposes LLM Embedding-based Attribution (LEA) -- a novel, explainable metric to paint a clear picture on the 'percentage of influence' the pre-trained knowledge vs. retrieved content has for each generated response. We apply LEA to assess responses to 100 critical CVEs from the past decade, verifying its effectiveness to quantify the insightfulness for vulnerability analysis. Our development of LEA reveals a progression of independency in hidden states of LLMs: heavy reliance on context in early layers, which enables the derivation of LEA; increased independency in later layers, which sheds light on why scale is essential for LLM's effectiveness. This work provides security analysts a means to audit LLM-assisted workflows, laying the groundwork for transparent, high-assurance deployments of RAG-enhanced LLMs in cybersecurity operations.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12100.pdf", "abstract_url": "https://arxiv.org/abs/2506.12100", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12104", "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents", "authors": ["Hao Li", "Xiaogeng Liu", "Hung-Chun Chiu", "Dianqi Li", "Ning Zhang", "Chaowei Xiao"], "abstract": "Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent's behavior, potentially resulting in economic loss, privacy leakage, or system compromise. System-level defenses have recently shown promise by enforcing static or predefined policies, but they still face two key challenges: the ability to dynamically update security rules and the need for memory stream isolation. To address these challenges, we propose DRIFT, a Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which enforces both control- and data-level constraints. A Secure Planner first constructs a minimal function trajectory and a JSON-schema-style parameter checklist for each function node based on the user query. A Dynamic Validator then monitors deviations from the original plan, assessing whether changes comply with privilege limitations and the user's intent. Finally, an Injection Isolator detects and masks any instructions that may conflict with the user query from the memory stream to mitigate long-term risks. We empirically validate the effectiveness of DRIFT on the AgentDojo benchmark, demonstrating its strong security performance while maintaining high utility across diverse models -- showcasing both its robustness and adaptability.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": "18 pages, 12 figures", "pdf_url": "https://arxiv.org/pdf/2506.12104.pdf", "abstract_url": "https://arxiv.org/abs/2506.12104", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.12110", "title": "EconGym: A Scalable AI Testbed with Diverse Economic Tasks", "authors": ["Qirui Mi", "Qipeng Yang", "Zijun Fan", "Wentian Fan", "Heyang Ma", "Chengdong Ma", "Siyu Xia", "Bo An", "Jun Wang", "Haifeng Zhang"], "abstract": "Artificial intelligence (AI) has become a powerful tool for economic research, enabling large-scale simulation and policy optimization. However, applying AI effectively requires simulation platforms for scalable training and evaluation-yet existing environments remain limited to simplified, narrowly scoped tasks, falling short of capturing complex economic challenges such as demographic shifts, multi-government coordination, and large-scale agent interactions. To address this gap, we introduce EconGym, a scalable and modular testbed that connects diverse economic tasks with AI algorithms. Grounded in rigorous economic modeling, EconGym implements 11 heterogeneous role types (e.g., households, firms, banks, governments), their interaction mechanisms, and agent models with well-defined observations, actions, and rewards. Users can flexibly compose economic roles with diverse agent algorithms to simulate rich multi-agent trajectories across 25+ economic tasks for AI-driven policy learning and analysis. Experiments show that EconGym supports diverse and cross-domain tasks-such as coordinating fiscal, pension, and monetary policies-and enables benchmarking across AI, economic methods, and hybrids. Results indicate that richer task composition and algorithm diversity expand the policy space, while AI agents guided by classical economic methods perform best in complex settings. EconGym also scales to 10k agents with high realism and efficiency.", "subjects": "General Economics (econ.GN); Artificial Intelligence (cs.AI)", "comments": "28 pages, 7 figures, 17 tables", "pdf_url": "https://arxiv.org/pdf/2506.12110.pdf", "abstract_url": "https://arxiv.org/abs/2506.12110", "categories": ["General Economics (econ.GN)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.13743", "title": "LTRR: Learning To Rank Retrievers for LLMs", "authors": ["To Eun Kim", "Fernando Diaz"], "abstract": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed retriever, despite growing evidence that no single retriever performs optimally across all query types. In this paper, we explore a query routing approach that dynamically selects from a pool of retrievers based on the query, using both train-free heuristics and learned routing models. We frame routing as a learning-to-rank (LTR) problem and introduce LTRR, a framework that learns to rank retrievers by their expected utility gain to downstream LLM performance. Our experiments, conducted on synthetic QA data with controlled query type variations, show that routing-based RAG systems can outperform the best single-retriever-based systems. Performance gains are especially pronounced in models trained with the Answer Correctness (AC) metric and with pairwise learning approaches, especially with XGBoost. We also observe improvements in generalization to out-of-distribution queries. As part of the SIGIR 2025 LiveRAG challenge, our submitted system demonstrated the practical viability of our approach, achieving competitive performance in both answer correctness and faithfulness. These findings highlight the importance of both training methodology and metric selection in query routing for RAG systems.", "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": "SIGIR 2025 LiveRAG Spotlight", "pdf_url": "https://arxiv.org/pdf/2506.13743.pdf", "abstract_url": "https://arxiv.org/abs/2506.13743", "categories": ["Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12234", "title": "Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation", "authors": ["Tetiana Gladkykh", "Kyrylo Kirykov"], "abstract": "Text-to-SQL systems enable users to query databases using natural language, democratizing access to data analytics. However, they face challenges in understanding ambiguous phrasing, domain-specific vocabulary, and complex schema relationships. This paper introduces Datrics Text2SQL, a Retrieval-Augmented Generation (RAG)-based framework designed to generate accurate SQL queries by leveraging structured documentation, example-based learning, and domain-specific rules. The system builds a rich Knowledge Base from database documentation and question-query examples, which are stored as vector embeddings and retrieved through semantic similarity. It then uses this context to generate syntactically correct and semantically aligned SQL code. The paper details the architecture, training methodology, and retrieval logic, highlighting how the system bridges the gap between user intent and database structure without requiring SQL expertise.", "subjects": "Databases (cs.DB); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": "28 pages, 6 figures, initial whitepaper version 1.0, submitted March 2025", "pdf_url": "https://arxiv.org/pdf/2506.12234.pdf", "abstract_url": "https://arxiv.org/abs/2506.12234", "categories": ["Databases (cs.DB)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2506.12202", "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions", "authors": ["Stephen Mell", "Botong Zhang", "David Mell", "Shuo Li", "Ramya Ramalingam", "Nathan Yu", "Steve Zdancewic", "Osbert Bastani"], "abstract": "Modern large language models (LLMs) are often deployed as agents, calling external tools adaptively to solve tasks. Rather than directly calling tools, it can be more effective for LLMs to write code to perform the tool calls, enabling them to automatically generate complex control flow such as conditionals and loops. Such code actions are typically provided as Python code, since LLMs are quite proficient at it; however, Python may not be the ideal language due to limited built-in support for performance, security, and reliability. We propose a novel programming language for code actions, called Quasar, which has several benefits: (1) automated parallelization to improve performance, (2) uncertainty quantification to improve reliability and mitigate hallucinations, and (3) security features enabling the user to validate actions. LLMs can write code in a subset of Python, which is automatically transpiled to Quasar. We evaluate our approach on the ViperGPT visual question answering agent, applied to the GQA dataset, demonstrating that LLMs with Quasar actions instead of Python actions retain strong performance, while reducing execution time when possible by 42%, improving security by reducing user approval interactions when possible by 52%, and improving reliability by applying conformal prediction to achieve a desired target coverage level.", "subjects": "Programming Languages (cs.PL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12202.pdf", "abstract_url": "https://arxiv.org/abs/2506.12202", "categories": ["Programming Languages (cs.PL)", "Artificial Intelligence (cs.AI)", "Cryptography and Security (cs.CR)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.12331", "title": "IndoorWorld: Integrating Physical Task Solving and Social Simulation in A Heterogeneous Multi-Agent Environment", "authors": ["Dekun Wu", "Frederik Brudy", "Bang Liu", "Yi Wang"], "abstract": "Virtual environments are essential to AI agent research. Existing environments for LLM agent research typically focus on either physical task solving or social simulation, with the former oversimplifying agent individuality and social dynamics, and the latter lacking physical grounding of social behaviors. We introduce IndoorWorld, a heterogeneous multi-agent environment that tightly integrates physical and social dynamics. By introducing novel challenges for LLM-driven agents in orchestrating social dynamics to influence physical environments and anchoring social interactions within world states, IndoorWorld opens up possibilities of LLM-based building occupant simulation for architectural design. We demonstrate the potential with a series of experiments within an office setting to examine the impact of multi-agent collaboration, resource competition, and spatial layout on agent behavior.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12331.pdf", "abstract_url": "https://arxiv.org/abs/2506.12331", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12339", "title": "SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation", "authors": ["Ruiyan Zhu", "Xi Cheng", "Ke Liu", "Brian Zhu", "Daniel Jin", "Neeraj Parihar", "Zhoutian Xu", "Oliver Gao"], "abstract": "We present SheetMind, a modular multi-agent framework powered by large language models (LLMs) for spreadsheet automation via natural language instructions. The system comprises three specialized agents: a Manager Agent that decomposes complex user instructions into subtasks; an Action Agent that translates these into structured commands using a Backus Naur Form (BNF) grammar; and a Reflection Agent that validates alignment between generated actions and the user's original intent. Integrated into Google Sheets via a Workspace extension, SheetMind supports real-time interaction without requiring scripting or formula knowledge. Experiments on benchmark datasets demonstrate an 80 percent success rate on single step tasks and approximately 70 percent on multi step instructions, outperforming ablated and baseline variants. Our results highlight the effectiveness of multi agent decomposition and grammar based execution for bridging natural language and spreadsheet functionalities.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "comments": "Ruiyan Zhu and Xi Cheng contributed equally to this work", "pdf_url": "https://arxiv.org/pdf/2506.12339.pdf", "abstract_url": "https://arxiv.org/abs/2506.12339", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12469", "title": "Levels of Autonomy for AI Agents", "authors": ["K. J. Kevin Feng", "David W. McDonald", "Amy X. Zhang"], "abstract": "Autonomy is a double-edged sword for AI agents, simultaneously unlocking transformative possibilities and serious risks. How can agent developers calibrate the appropriate levels of autonomy at which their agents should operate? We argue that an agent's level of autonomy can be treated as a deliberate design decision, separate from its capability and operational environment. In this work, we define five levels of escalating agent autonomy, characterized by the roles a user can take when interacting with an agent: operator, collaborator, consultant, approver, and observer. Within each level, we describe the ways by which a user can exert control over the agent and open questions for how to design the nature of user-agent interaction. We then highlight a potential application of our framework towards AI autonomy certificates to govern agent behavior in single- and multi-agent systems. We conclude by proposing early ideas for evaluating agents' autonomy. Our work aims to contribute meaningful, practical steps towards responsibly deployed and useful AI agents in the real world.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "comments": "Forthcoming paper in the Knight First Amendment Institute's \"AI and Democratic Freedoms\" essay series", "pdf_url": "https://arxiv.org/pdf/2506.12469.pdf", "abstract_url": "https://arxiv.org/abs/2506.12469", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12474", "title": "Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture", "authors": ["Wenyun Li", "Wenjie Huang", "Zejian Deng", "Chen Sun"], "abstract": "Accurate driving behavior modeling is fundamental to safe and efficient trajectory prediction, yet remains challenging in complex traffic scenarios. This paper presents a novel Inverse Reinforcement Learning (IRL) framework that captures human-like decision-making by inferring diverse reward functions, enabling robust cross-scenario adaptability. The learned reward function is utilized to maximize the likelihood of output by the encoder-decoder architecture that combines Mamba blocks for efficient long-sequence dependency modeling with graph attention networks to encode spatial interactions among traffic agents. Comprehensive evaluations on urban intersections and roundabouts demonstrate that the proposed method not only outperforms various popular approaches in prediction accuracy but also achieves 2 times higher generalization performance to unseen scenarios compared to other IRL-based method.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12474.pdf", "abstract_url": "https://arxiv.org/abs/2506.12474", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12600", "title": "Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow", "authors": ["Jie Pan", "Tianyi Wang", "Christian Claudel", "Jing Shi"], "abstract": "Intelligent transportation systems require connected and automated vehicles (CAVs) to conduct safe and efficient cooperation with human-driven vehicles (HVs) in complex real-world traffic environments. However, the inherent unpredictability of human behaviour, especially at bottlenecks such as highway on-ramp merging areas, often disrupts traffic flow and compromises system performance. To address the challenge of cooperative on-ramp merging in heterogeneous traffic environments, this study proposes a trust-based multi-agent reinforcement learning (Trust-MARL) framework. At the macro level, Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust to improve bottleneck throughput and mitigate traffic shockwave through emergent group-level coordination. At the micro level, a dynamic trust mechanism is designed to enable CAVs to adjust their cooperative strategies in response to real-time behaviors and historical interactions with both HVs and other CAVs. Furthermore, a trust-triggered game-theoretic decision-making module is integrated to guide each CAV in adapting its cooperation factor and executing context-aware lane-changing decisions under safety, comfort, and efficiency constraints. An extensive set of ablation studies and comparative experiments validates the effectiveness of the proposed Trust-MARL approach, demonstrating significant improvements in safety, efficiency, comfort, and adaptability across varying CAV penetration rates and traffic densities.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Computer Science and Game Theory (cs.GT); Robotics (cs.RO)", "comments": "34 pages, 7 figures, 4 tables", "pdf_url": "https://arxiv.org/pdf/2506.12600.pdf", "abstract_url": "https://arxiv.org/abs/2506.12600", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Emerging Technologies (cs.ET)", "Computer Science and Game Theory (cs.GT)", "Robotics (cs.RO)"], "matching_keywords": ["agent"]}
{"id": "2506.12735", "title": "Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling", "authors": ["Zhilin Lin", "Shiliang Sun"], "abstract": "Reinforcement learning (RL) is playing an increasingly important role in fields such as robotic control and autonomous driving. However, the gap between simulation and the real environment remains a major obstacle to the practical deployment of RL. Agents trained in simulators often struggle to maintain performance when transferred to real-world physical environments. In this paper, we propose a latent space based approach to analyze the impact of simulation on real-world policy improvement in model-based settings. As a natural extension of model-based methods, our approach enables an intuitive observation of the challenges faced by model-based methods in sim-to-real transfer. Experiments conducted in the MuJoCo environment evaluate the performance of our method in both measuring and mitigating the sim-to-real gap. The experiments also highlight the various challenges that remain in overcoming the sim-to-real gap, especially for model-based methods.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12735.pdf", "abstract_url": "https://arxiv.org/abs/2506.12735", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12795", "title": "Resilient-native and Intelligent NextG Systems", "authors": ["Mehdi Bennis"], "abstract": "Just like power, water and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient to unforeseen events, able to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Despite its critical importance, resilience remains an elusive concept, with its mathematical foundations still underdeveloped. Unlike robustness and reliability, resilience is premised on the fact that disruptions will inevitably happen. Resilience, in terms of elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents (or networks) that can flexibly expand their states, hypotheses and course of actions, by transforming through real-time adaptation and reconfiguration. This constant situational awareness and vigilance of adapting world models and counterfactually reasoning about potential system failures and the corresponding best responses, is a core aspect of resilience. This article seeks to first define resilience and disambiguate it from reliability and robustness, before delving into the mathematics of resilience. Finally, the article concludes by presenting nuanced metrics and discussing trade-offs tailored to the unique characteristics of network resilience.", "subjects": "Emerging Technologies (cs.ET); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.12795.pdf", "abstract_url": "https://arxiv.org/abs/2506.12795", "categories": ["Emerging Technologies (cs.ET)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12879", "title": "Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation", "authors": ["Frederic Gmeiner", "Kaitao Luo", "Ye Wang", "Kenneth Holstein", "Nikolas Martelaro"], "abstract": "Despite the potential of generative AI (GenAI) design tools to enhance design processes, professionals often struggle to integrate AI into their workflows. Fundamental cognitive challenges include the need to specify all design criteria as distinct parameters upfront (intent formulation) and designers' reduced cognitive involvement in the design process due to cognitive offloading, which can lead to insufficient problem exploration, underspecification, and limited ability to evaluate outcomes. Motivated by these challenges, we envision novel metacognitive support agents that assist designers in working more reflectively with GenAI. To explore this vision, we conducted exploratory prototyping through a Wizard of Oz elicitation study with 20 mechanical designers probing multiple metacognitive support strategies. We found that agent-supported users created more feasible designs than non-supported users, with differing impacts between support strategies. Based on these findings, we discuss opportunities and tradeoffs of metacognitive support agents and considerations for future AI-based design tools.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "comments": "26 pages, to be published in the proceedings of the Designing Interactive Systems Conference (DIS'25)", "pdf_url": "https://arxiv.org/pdf/2506.12879.pdf", "abstract_url": "https://arxiv.org/abs/2506.12879", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.12949", "title": "eLog analysis for accelerators: status and future outlook", "authors": ["Antonin Sulc", "Thorsten Hellert", "Aaron Reed", "Adam Carpenter", "Alex Bien", "Chris Tennant", "Claudio Bisegni", "Daniel Lersch", "Daniel Ratner", "David Lawrence", "Diana McSpadden", "Hayden Hoschouer", "Jason St. John", "Thomas Britton"], "abstract": "This work demonstrates electronic logbook (eLog) systems leveraging modern AI-driven information retrieval capabilities at the accelerator facilities of Fermilab, Jefferson Lab, Lawrence Berkeley National Laboratory (LBNL), SLAC National Accelerator Laboratory. We evaluate contemporary tools and methodologies for information retrieval with Retrieval Augmented Generation (RAGs), focusing on operational insights and integration with existing accelerator control systems.", "subjects": "High Energy Physics - Experiment (hep-ex); Artificial Intelligence (cs.AI)", "comments": "4 pages, 2 figures, 16th International Particle Accelerator Conference (IPAC'25)", "pdf_url": "https://arxiv.org/pdf/2506.12949.pdf", "abstract_url": "https://arxiv.org/abs/2506.12949", "categories": ["High Energy Physics - Experiment (hep-ex)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.13171", "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches", "authors": ["Lukasz Mazur", "Nenad Petrovic", "James Pontes Miranda", "Ansgar Radermacher", "Robert Rasche", "Alois Knoll"], "abstract": "Large language models (LLMs) offer new opportunities for interacting with complex software artifacts, such as software models, through natural language. They present especially promising benefits for large software models that are difficult to grasp in their entirety, making traditional interaction and analysis approaches challenging. This paper investigates two approaches for leveraging LLMs to answer questions over software models: direct prompting, where the whole software model is provided in the context, and an agentic approach combining LLM-based agents with general-purpose file access tools. We evaluate these approaches using an Ecore metamodel designed for timing analysis and software optimization in automotive and embedded domains. Our findings show that while the agentic approach achieves accuracy comparable to direct prompting, it is significantly more efficient in terms of token usage. This efficiency makes the agentic approach particularly suitable for the automotive industry, where the large size of software models makes direct prompting infeasible, establishing LLM agents as not just a practical alternative but the only viable solution. Notably, the evaluation was conducted using small LLMs, which are more feasible to be executed locally - an essential advantage for meeting strict requirements around privacy, intellectual property protection, and regulatory compliance. Future work will investigate software models in diverse formats, explore more complex agent architectures, and extend agentic workflows to support not only querying but also modification of software models.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13171.pdf", "abstract_url": "https://arxiv.org/abs/2506.13171", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.13205", "title": "Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments", "authors": ["Xuan Wang", "Siyuan Liang", "Zhe Liu", "Yi Yu", "Yuliang Lu", "Xiaochun Cao", "Ee-Chien Chang"], "abstract": "With the growing integration of vision-language models (VLMs), mobile agents are now widely used for tasks like UI automation and camera-based user assistance. These agents are often fine-tuned on limited user-generated datasets, leaving them vulnerable to covert threats during the training process. In this work we present GHOST, the first clean-label backdoor attack specifically designed for mobile agents built upon VLMs. Our method manipulates only the visual inputs of a portion of the training samples - without altering their corresponding labels or instructions - thereby injecting malicious behaviors into the model. Once fine-tuned with this tampered data, the agent will exhibit attacker-controlled responses when a specific visual trigger is introduced at inference time. The core of our approach lies in aligning the gradients of poisoned samples with those of a chosen target instance, embedding backdoor-relevant features into the poisoned training data. To maintain stealth and enhance robustness, we develop three realistic visual triggers: static visual patches, dynamic motion cues, and subtle low-opacity overlays. We evaluate our method across six real-world Android apps and three VLM architectures adapted for mobile use. Results show that our attack achieves high attack success rates (up to 94.67 percent) while maintaining high clean-task performance (FSR up to 95.85 percent). Additionally, ablation studies shed light on how various design choices affect the efficacy and concealment of the attack. Overall, this work is the first to expose critical security flaws in VLM-based mobile agents, highlighting their susceptibility to clean-label backdoor attacks and the urgent need for effective defense mechanisms in their training pipelines. Code and examples are available at:", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": "12 pages", "pdf_url": "https://arxiv.org/pdf/2506.13205.pdf", "abstract_url": "https://arxiv.org/abs/2506.13205", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.13246", "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains", "authors": ["Craig Steven Wright"], "abstract": "This paper presents a formalised architecture for synthetic agents designed to retain immutable memory, verifiable reasoning, and constrained epistemic growth. Traditional AI systems rely on mutable, opaque statistical models prone to epistemic drift and historical revisionism. In contrast, we introduce the concept of the Merkle Automaton, a cryptographically anchored, deterministic computational framework that integrates formal automata theory with blockchain-based commitments. Each agent transition, memory fragment, and reasoning step is committed within a Merkle structure rooted on-chain, rendering it non-repudiable and auditably permanent. To ensure selective access and confidentiality, we derive symmetric encryption keys from ECDH exchanges contextualised by hierarchical privilege lattices. This enforces cryptographic access control over append-only DAG-structured knowledge graphs. Reasoning is constrained by formal logic systems and verified through deterministic traversal of policy-encoded structures. Updates are non-destructive and historied, preserving epistemic lineage without catastrophic forgetting. Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion attestations. Collectively, this architecture reframes memory not as a cache but as a ledger - one whose contents are enforced by protocol, bound by cryptography, and constrained by formal logic. The result is not an intelligent agent that mimics thought, but an epistemic entity whose outputs are provably derived, temporally anchored, and impervious to post hoc revision. This design lays foundational groundwork for legal, economic, and high-assurance computational systems that require provable memory, unforgeable provenance, and structural truth.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)", "comments": "47 pages, includes formal automata specifications, cryptographic constructions, and epistemic architecture schema", "pdf_url": "https://arxiv.org/pdf/2506.13246.pdf", "abstract_url": "https://arxiv.org/abs/2506.13246", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)", "Distributed, Parallel, and Cluster Computing (cs.DC)"], "matching_keywords": ["agent"]}
{"id": "2506.13453", "title": "Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics", "authors": ["YR Darr", "MA Niazi"], "abstract": "The self-organization of robots for the formation of structures and shapes is a stimulating application of the swarm robotic system. It involves a large number of autonomous robots of heterogeneous behavior, coordination among them, and their interaction with the dynamic environment. This process of complex structure formation is considered a complex system, which needs to be modeled by using any modeling approach. Although the formal specification approach along with other formal methods has been used to model the behavior of robots in a swarm. However, to the best of our knowledge, the formal specification approach has not been used to model the self-organization process in swarm robotic systems for shape formation. In this paper, we use a formal specification approach to model the shape formation task of swarm robots. We use Z (Zed) language of formal specification, which is a state-based language, to model the states of the entities of the systems. We demonstrate the effectiveness of Z for the self-organized shape formation. The presented formal specification model gives the outlines for designing and implementing the swarm robotic system for the formation of complex shapes and structures. It also provides the foundation for modeling the complex shape formation process for swarm robotics using a multi-agent system in a simulation-based environment. Keywords: Swarm robotics, Self-organization, Formal specification, Complex systems", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13453.pdf", "abstract_url": "https://arxiv.org/abs/2506.13453", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.13469", "title": "A Two-stage Optimization Method for Wide-range Single-electron Quantum Magnetic Sensing", "authors": ["Shiqian Guo", "Jianqing Liu", "Thinh Le", "Huaiyu Dai"], "abstract": "Quantum magnetic sensing based on spin systems has emerged as a new paradigm for detecting ultra-weak magnetic fields with unprecedented sensitivity, revitalizing applications in navigation, geo-localization, biology, and beyond. At the heart of quantum magnetic sensing, from the protocol perspective, lies the design of optimal sensing parameters to manifest and then estimate the underlying signals of interest (SoI). Existing studies on this front mainly rely on adaptive algorithms based on black-box AI models or formula-driven principled searches. However, when the SoI spans a wide range and the quantum sensor has physical constraints, these methods may fail to converge efficiently or optimally, resulting in prolonged interrogation times and reduced sensing accuracy. In this work, we report the design of a new protocol using a two-stage optimization method. In the 1st Stage, a Bayesian neural network with a fixed set of sensing parameters is used to narrow the range of SoI. In the 2nd Stage, a federated reinforcement learning agent is designed to fine-tune the sensing parameters within a reduced search space. The proposed protocol is developed and evaluated in a challenging context of single-shot readout of an NV-center electron spin under a constrained total sensing time budget; and yet it achieves significant improvements in both accuracy and resource efficiency for wide-range D.C. magnetic field estimation compared to the state of the art.", "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13469.pdf", "abstract_url": "https://arxiv.org/abs/2506.13469", "categories": ["Quantum Physics (quant-ph)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.13566", "title": "A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints", "authors": ["Jonathan Hoss", "Felix Schelling", "Noah Klarmann"], "abstract": "The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing makespan under deterministic constraints. Real-world production environments introduce additional complexities that cause traditional scheduling approaches to be less effective. Reinforcement learning (RL) holds potential in addressing these challenges, as it allows agents to learn adaptive scheduling strategies. However, there is a lack of a comprehensive, general-purpose frameworks for effectively training and evaluating RL agents under real-world constraints. To address this gap, we propose a modular framework that extends classical JSSP formulations by incorporating key \\mbox{real-world} constraints inherent to the shopfloor, including transport logistics, buffer management, machine breakdowns, setup times, and stochastic processing conditions, while also supporting multi-objective optimization. The framework is a customizable solution that offers flexibility in defining problem instances and configuring simulation parameters, enabling adaptation to diverse production scenarios. A standardized interface ensures compatibility with various RL approaches, providing a robust environment for training RL agents and facilitating the standardized comparison of different scheduling methods under dynamic and uncertain conditions. We release JobShopLab as an open-source tool for both research and industrial applications, accessible at:", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "This paper has been accepted for presentation at the IEEE 21st International Conference on Automation Science and Engineering (CASE 2025)", "pdf_url": "https://arxiv.org/pdf/2506.13566.pdf", "abstract_url": "https://arxiv.org/abs/2506.13566", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.13583", "title": "Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes", "authors": ["Bernhard Hilpert", "Muhan Hou", "Kim Baraka", "Joost Broekens"], "abstract": "Reinforcement Learning (RL) agents often exhibit learning behaviors that are not intuitively interpretable by human observers, which can result in suboptimal feedback in collaborative teaching settings. Yet, how humans perceive and interpret RL agent's learning behavior is largely unknown. In a bottom-up approach with two experiments, this work provides a data-driven understanding of the factors of human observers' understanding of the agent's learning process. A novel, observation-based paradigm to directly assess human inferences about agent learning was developed. In an exploratory interview study (\\textit{N}=9), we identify four core themes in human interpretations: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second confirmatory study (\\textit{N}=34) applied an expanded version of the paradigm across two tasks (navigation/manipulation) and two RL algorithms (tabular/function approximation). Analyses of 816 responses confirmed the reliability of the paradigm and refined the thematic framework, revealing how these themes evolve over time and interrelate. Our findings provide a human-centered understanding of how people make sense of agent learning, offering actionable insights for designing interpretable RL systems and improving transparency in Human-Robot Interaction.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13583.pdf", "abstract_url": "https://arxiv.org/abs/2506.13583", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)"], "matching_keywords": ["agent"]}
{"id": "2506.13666", "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems", "authors": ["Junfeng Fang", "Zijun Yao", "Ruipeng Wang", "Haokai Ma", "Xiang Wang", "Tat-Seng Chua"], "abstract": "The development of large language models (LLMs) has entered in a experience-driven era, flagged by the emergence of environment feedback-driven learning via reinforcement learning and tool-using agents. This encourages the emergenece of model context protocol (MCP), which defines the standard on how should a LLM interact with external services, such as \\api and data. However, as MCP becomes the de facto standard for LLM agent systems, it also introduces new safety risks. In particular, MCP introduces third-party services, which are not controlled by the LLM developers, into the agent systems. These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. In this position paper, we advocate the research community in LLM safety to pay close attention to the new safety risks issues introduced by MCP, and develop new techniques to build safe MCP-powered agent systems. To establish our position, we argue with three key parts. (1) We first construct \\framework, a controlled framework to examine safety issues in MCP-powered agent systems. (2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial. (3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered agent systems. In particular, we would call for researchers to persue the following research directions: red teaming, MCP safe LLM development, MCP safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP safe ecosystem construction. We hope this position paper can raise the awareness of the research community in MCP safety and encourage more researchers to join this important research direction. Our code is available at", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.13666.pdf", "abstract_url": "https://arxiv.org/abs/2506.13666", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
