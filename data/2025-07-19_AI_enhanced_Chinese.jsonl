{"id": "2507.12732", "title": "Strategy Adaptation in Large Language Model Werewolf Agents", "authors": ["Fuya Nakamori", "Yin Jou Huang", "Fei Cheng"], "abstract": "This study proposes a method to improve the performance of Werewolf agents by switching between predefined strategies based on the attitudes of other players and the context of conversations. While prior works of Werewolf agents using prompt engineering have employed methods where effective strategies are implicitly defined, they cannot adapt to changing situations. In this research, we propose a method that explicitly selects an appropriate strategy based on the game context and the estimated roles of other players. We compare the strategy adaptation Werewolf agents with baseline agents using implicit or fixed strategies and verify the effectiveness of our proposed method.", "subjects": "Computation and Language (cs.CL)", "comments": "7 pages, 2 figures", "pdf_url": "https://arxiv.org/pdf/2507.12732.pdf", "abstract_url": "https://arxiv.org/abs/2507.12732", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本研究提出了一种通过根据其他玩家的态度和对话上下文切换预定义策略来提高狼人杀代理性能的方法。", "motivation": "解决现有狼人杀代理在使用提示工程时，无法适应变化情境的问题。", "method": "提出了一种基于游戏上下文和其他玩家估计角色明确选择适当策略的方法。", "result": "与使用隐式或固定策略的基线代理相比，验证了所提方法的有效性。", "conclusion": "明确策略适应的狼人杀代理在性能上优于隐式或固定策略的代理，为类似情境下的代理设计提供了新思路。"}}
{"id": "2507.12484", "title": "AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education", "authors": ["Jarosław A. Chudziak", "Adam Kostka"], "abstract": "The growing ubiquity of artificial intelligence (AI), in particular large language models (LLMs), has profoundly altered the way in which learners gain knowledge and interact with learning material, with many claiming that AI positively influences their learning achievements. Despite this advancement, current AI tutoring systems face limitations associated with their reactive nature, often providing direct answers without encouraging deep reflection or incorporating structured pedagogical tools and strategies. This limitation is most apparent in the field of mathematics, in which AI tutoring systems remain underdeveloped. This research addresses the question: How can AI tutoring systems move beyond providing reactive assistance to enable structured, individualized, and tool-assisted learning experiences? We introduce a novel multi-agent AI tutoring platform that combines adaptive and personalized feedback, structured course generation, and textbook knowledge retrieval to enable modular, tool-assisted learning processes. This system allows students to learn new topics while identifying and targeting their weaknesses, revise for exams effectively, and practice on an unlimited number of personalized exercises. This article contributes to the field of artificial intelligence in education by introducing a novel platform that brings together pedagogical agents and AI-driven components, augmenting the field with modular and effective systems for teaching mathematics.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "8 pages, 5 figures", "pdf_url": "https://arxiv.org/pdf/2507.12484.pdf", "abstract_url": "https://arxiv.org/abs/2507.12484", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种新型的多代理AI辅导平台，旨在通过自适应和个性化反馈、结构化课程生成和教科书知识检索，促进模块化、工具辅助的学习过程，特别是在数学教育领域。", "motivation": "当前AI辅导系统存在反应性限制，缺乏深度反思和结构化教学工具的整合，尤其是在数学领域。本研究旨在探索如何使AI辅导系统超越反应性辅助，实现结构化、个性化和工具辅助的学习体验。", "method": "研究引入了一个结合自适应和个性化反馈、结构化课程生成及教科书知识检索的多代理AI辅导平台。", "result": "开发了一个允许学生学习新主题、识别并针对弱点、有效复习考试以及练习无限个性化练习的系统。", "conclusion": "本文通过引入一个结合教学代理和AI驱动组件的新平台，为人工智能在教育领域的应用贡献了模块化和有效的数学教学系统。"}}
{"id": "2507.12795", "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning", "authors": ["Penglei Sun", "Yaoxian Song", "Xiangru Zhu", "Xiang Liu", "Qiang Wang", "Yue Liu", "Changqun Xia", "Tiefeng Li", "Yang Yang", "Xiaowen Chu"], "abstract": "Scene understanding enables intelligent agents to interpret and comprehend their environment. While existing large vision-language models (LVLMs) for scene understanding have primarily focused on indoor household tasks, they face two significant limitations when applied to outdoor large-scale scene understanding. First, outdoor scenarios typically encompass larger-scale environments observed through various sensors from multiple viewpoints (e.g., bird view and terrestrial view), while existing indoor LVLMs mainly analyze single visual modalities within building-scale contexts from humanoid viewpoints. Second, existing LVLMs suffer from missing multidomain perception outdoor data and struggle to effectively integrate 2D and 3D visual information. To address the aforementioned limitations, we build the first multidomain perception outdoor scene understanding dataset, named \\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale scenarios with multi\\textbf{\\underline{V}}iew and multi\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k images and $4, 811$M point clouds with $567$k question-answering pairs from vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To effectively fuse the multimodal data in the absence of one modality, we introduce incomplete multimodal learning to model outdoor scene understanding and design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is realized by constructing a joint probabilistic distribution space rather than implementing directly explicit fusion operations (e.g., concatenation). Experimental results on three typical outdoor scene understanding tasks show City-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in question-answering tasks averagely. Our method demonstrates pragmatic and generalization performance across multiple outdoor scenes.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.12795.pdf", "abstract_url": "https://arxiv.org/abs/2507.12795", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "City-VLM是一个针对户外大规模场景理解的多模态不完全学习模型，通过构建SVM-City数据集和引入不完全多模态学习，有效融合2D和3D视觉信息，提升了户外场景理解的能力。", "motivation": "解决现有大型视觉语言模型在户外大规模场景理解中的两个主要限制：一是户外场景通常涉及更大规模的环境和多视角观察，而现有模型主要针对室内单一视觉模态；二是缺乏多域感知户外数据，难以有效整合2D和3D视觉信息。", "method": "构建了首个多域感知户外场景理解数据集SVM-City，并设计了City-VLM模型，通过构建联合概率分布空间实现多模态融合，而非直接显式融合操作。", "result": "在三个典型的户外场景理解任务中，City-VLM在问答任务上的平均性能超过了现有大型视觉语言模型18.14%。", "conclusion": "City-VLM在多个户外场景中展示了实用性和泛化性能，为户外大规模场景理解提供了有效的解决方案。"}}
{"id": "2507.12494", "title": "MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents", "authors": ["Dustin Holley", "Jovin D'sa", "Hossein Nourkhiz Mahjoub", "Gibran Ali"], "abstract": "Enhancing simulation environments to replicate real-world driver behavior, i.e., more humanlike sim agents, is essential for developing autonomous vehicle technology. In the context of highway merging, previous works have studied the operational-level yielding dynamics of lag vehicles in response to a merging car at highway on-ramps. Other works focusing on tactical decision modeling generally consider limited action sets or utilize payoff functions with large parameter sets and limited payoff bounds. In this work, we aim to improve the simulation of the highway merge scenario by targeting a game theoretic model for tactical decision-making with improved payoff functions and lag actions. We couple this with an underlying dynamics model to have a unified decision and dynamics model that can capture merging interactions and simulate more realistic interactions in an explainable and interpretable fashion. The proposed model demonstrated good reproducibility of complex interactions when validated on a real-world dataset. The model was finally integrated into a high fidelity simulation environment and confirmed to have adequate computation time efficiency for use in large-scale simulations to support autonomous vehicle development.", "subjects": "Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA); Robotics (cs.RO)", "comments": "8 pages", "pdf_url": "https://arxiv.org/pdf/2507.12494.pdf", "abstract_url": "https://arxiv.org/abs/2507.12494", "categories": ["Artificial Intelligence (cs.AI)", "Computer Science and Game Theory (cs.GT)", "Multiagent Systems (cs.MA)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "MR-LDM模型是一种用于模拟高速公路合并场景中人类驾驶员战术决策的博弈论模型，旨在通过改进的收益函数和后车动作来增强模拟环境的真实性和可解释性。", "motivation": "为了提高自动驾驶技术开发中模拟环境的真实性，特别是在高速公路合并场景中，需要更准确地模拟人类驾驶员的行为。", "method": "提出了一个结合博弈论战术决策模型和底层动力学模型的统一框架，用于模拟合并交互，并通过真实世界数据集验证了模型的有效性。", "result": "模型在真实世界数据集上验证了其能够复现复杂的交互行为，并且在集成到高保真模拟环境后，显示出足够的计算时间效率，适用于大规模模拟。", "conclusion": "MR-LDM模型不仅提高了模拟高速公路合并场景中人类驾驶员行为的真实性和可解释性，而且计算效率高，适合支持自动驾驶技术的大规模模拟开发。"}}
{"id": "2507.12666", "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models", "authors": ["Alex Zook", "Josef Spjut", "Jonathan Tremblay"], "abstract": "Game design hinges on understanding how static rules and content translate into dynamic player behavior - something modern generative systems that inspect only a game's code or assets struggle to capture. We present an automated design iteration framework that closes this gap by pairing a reinforcement learning (RL) agent, which playtests the game, with a large multimodal model (LMM), which revises the game based on what the agent does. In each loop the RL player completes several episodes, producing (i) numerical play metrics and/or (ii) a compact image strip summarising recent video frames. The LMM designer receives a gameplay goal and the current game configuration, analyses the play traces, and edits the configuration to steer future behaviour toward the goal. We demonstrate results that LMMs can reason over behavioral traces supplied by RL agents to iteratively refine game mechanics, pointing toward practical, scalable tools for AI-assisted game design.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.12666.pdf", "abstract_url": "https://arxiv.org/abs/2507.12666", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种自动化游戏设计迭代框架，结合强化学习（RL）代理和大型多模态模型（LMM），通过RL代理的测试和LMM的修订来优化游戏设计。", "motivation": "现代生成系统仅通过检查游戏代码或资源难以捕捉静态规则和内容如何转化为动态玩家行为，这限制了游戏设计的效果。", "method": "框架中RL代理进行游戏测试，产生数值游戏指标或图像条摘要，LMM根据这些数据和当前游戏配置，分析游戏轨迹并编辑配置以引导未来行为达到目标。", "result": "研究表明，LMM能够通过RL代理提供的行为轨迹进行推理，迭代优化游戏机制，为AI辅助游戏设计提供了实用、可扩展的工具。", "conclusion": "该框架通过结合RL和LMM，有效地填补了游戏设计与玩家行为之间的鸿沟，为游戏设计提供了新的自动化迭代方法。"}}
{"id": "2507.12599", "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs", "authors": ["Léo Saulières"], "abstract": "The success of recent Artificial Intelligence (AI) models has been accompanied by the opacity of their internal mechanisms, due notably to the use of deep neural networks. In order to understand these internal mechanisms and explain the output of these AI models, a set of methods have been proposed, grouped under the domain of eXplainable AI (XAI). This paper focuses on a sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims to explain the actions of an agent that has learned by reinforcement learning. We propose an intuitive taxonomy based on two questions \"What\" and \"How\". The first question focuses on the target that the method explains, while the second relates to the way the explanation is provided. We use this taxonomy to provide a state-of-the-art review of over 250 papers. In addition, we present a set of domains close to XRL, which we believe should get attention from the community. Finally, we identify some needs for the field of XRL.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "69 pages, 19 figures", "pdf_url": "https://arxiv.org/pdf/2507.12599.pdf", "abstract_url": "https://arxiv.org/abs/2507.12599", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种直观的分类法，基于“什么”和“如何”两个问题，对可解释强化学习（XRL）领域进行了综述，涵盖了250多篇论文，并指出了该领域的一些需求和邻近领域。", "motivation": "随着人工智能（AI）模型，特别是深度神经网络的成功，其内部机制的不透明性也随之增加。为了理解这些内部机制并解释AI模型的输出，提出了可解释AI（XAI）领域的一系列方法。本文专注于XAI的一个子领域，即可解释强化学习（XRL），旨在解释通过强化学习学习的智能体的行为。", "method": "本文提出了一种基于“什么”和“如何”两个问题的直观分类法。“什么”问题关注方法解释的目标，“如何”问题则涉及提供解释的方式。利用这一分类法，对250多篇论文进行了综述。", "result": "通过分类法，本文提供了XRL领域的最新综述，并指出了一些需要社区关注的邻近领域。", "conclusion": "本文不仅综述了XRL领域的现状，还识别了该领域的一些需求，为未来的研究提供了方向。"}}
{"id": "2507.12801", "title": "Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning", "authors": ["Sosui Moribe", "Taketoshi Ushiama"], "abstract": "In recent years, peer learning has gained attention as a method that promotes spontaneous thinking among learners, and its effectiveness has been confirmed by numerous studies. This study aims to develop an AI Agent as a learning companion that enables peer learning anytime and anywhere. However, peer learning between humans has various limitations, and it is not always effective. Effective peer learning requires companions at the same proficiency levels. In this study, we assume that a learner's peers with the same proficiency level as the learner make the same mistakes as the learner does and focus on English composition as a specific example to validate this approach.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": ")", "pdf_url": "https://arxiv.org/pdf/2507.12801.pdf", "abstract_url": "https://arxiv.org/abs/2507.12801", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本研究旨在开发一种作为学习伴侣的AI代理，以实现在线同伴学习，特别关注英语作文中的错误模仿，以验证同伴学习的效果。", "motivation": "同伴学习虽有效，但存在限制，如同熟练度的同伴不易找到。本研究通过AI代理模仿学习者的错误，以克服这些限制。", "method": "开发一个AI代理作为学习伴侣，模仿学习者在英语作文中的错误，以模拟同熟练度同伴的学习环境。", "result": "通过模仿学习者的错误，AI代理能够有效地模拟同熟练度同伴的学习环境，验证了同伴学习的有效性。", "conclusion": "AI代理作为学习伴侣可以克服人类同伴学习的限制，为学习者提供有效的同伴学习体验，特别是在英语作文学习中。"}}
{"id": "2507.12806", "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models", "authors": ["Zhiwei Liu", "Jielin Qiu", "Shiyu Wang", "Jianguo Zhang", "Zuxin Liu", "Roshan Ram", "Haolin Chen", "Weiran Yao", "Huan Wang", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong"], "abstract": "The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \\oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.12806.pdf", "abstract_url": "https://arxiv.org/abs/2507.12806", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了MCPEval，一个基于模型上下文协议（MCP）的开源框架，旨在自动化和标准化大型语言模型（LLM）智能代理的深度评估。", "motivation": "随着基于大型语言模型（LLMs）的智能代理的迅速崛起，现有的评估方法依赖于静态基准和劳动密集型的数据收集，限制了实际评估的效率和范围。", "method": "MCPEval框架通过自动化端到端任务生成和深度评估，标准化了评估指标，并与原生代理工具无缝集成，消除了构建评估管道的手动努力。", "result": "在五个现实世界领域的实证结果表明，MCPEval在揭示领域特定性能方面具有有效性。", "conclusion": "MCPEval为LLM代理的评估提供了一个强大、可扩展的框架，公开释放以促进更广泛的采用和研究。"}}
{"id": "2507.12821", "title": "Assessing adaptive world models in machines with novel games", "authors": ["Lance Ying", "Katherine M. Collins", "Prafull Sharma", "Cedric Colas", "Kaiya Ivy Zhao", "Adrian Weller", "Zenna Tavares", "Phillip Isola", "Samuel J. Gershman", "Jacob D. Andreas", "Thomas L. Griffiths", "Francois Chollet", "Kelsey R. Allen", "Joshua B. Tenenbaum"], "abstract": "Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on a massive corpora of data, instead of the efficiency and efficacy of models in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this kind of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of the human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "17 pages, 4 figures", "pdf_url": "https://arxiv.org/pdf/2507.12821.pdf", "abstract_url": "https://arxiv.org/abs/2507.12821", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了人类智能在快速适应和解决新环境问题方面的能力，提出这种能力与高效构建和精炼内部环境表征（即世界模型）密切相关。作者呼吁在人工智能（AI）领域建立一个新的评估框架，以评估适应性世界模型，特别是在新颖游戏中的表现。", "motivation": "当前对人工智能中世界模型的理解和评估过于狭窄，主要集中在从大量数据中学习的静态表征，而不是通过在新环境中的互动和探索来学习这些表征的效率和效果。", "method": "作者提出了一个新的基准测试范式，基于一系列精心设计的游戏，这些游戏在底层游戏结构中具有真正的、深层次的和持续更新的新颖性，称为“新颖游戏”。", "result": "提出了构建这些游戏的关键要求和适当的度量标准，以明确挑战和评估代理快速诱导世界模型的能力。", "conclusion": "希望这一新的评估框架能激发未来对AI世界模型的评估努力，并为开发具有人类类似快速适应和强大泛化能力的AI系统提供关键步骤，这是人工通用智能的关键组成部分。"}}
{"id": "2507.12862", "title": "Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command", "authors": ["Hussein Abbass", "Taylan Akay", "Harrison Tolley"], "abstract": "In the age of AI, human commanders need to use the computational powers available in today's environment to simulate a very large number of scenarios. Within each scenario, situations occur where different decision design options could have ethical consequences. Making these decisions reliant on human judgement is both counter-productive to the aim of exploring very large number of scenarios in a timely manner and infeasible when considering the workload needed to involve humans in each of these choices. In this paper, we move human judgement outside the simulation decision cycle. Basically, the human will design the ethical metric space, leaving it to the simulated environment to explore the space. When the simulation completes its testing cycles, the testing environment will come back to the human commander with a few options to select from. The human commander will then exercise human-judgement to select the most appropriate course of action, which will then get executed accordingly. We assume that the problem of designing metrics that are sufficiently granular to assess the ethical implications of decisions is solved. Subsequently, the fundamental problem we look at in this paper is how to weight ethical decisions during the running of these simulations; that is, how to dynamically weight the ethical attributes when agents are faced with decision options with ethical implications during generative simulations. The multi-criteria decision making literature has started to look at nearby problems, where the concept of entropy has been used to determine the weights during aggregation. We draw from that literature different approaches to automatically calculate the weights for ethical attributes during simulation-based testing and evaluation.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.12862.pdf", "abstract_url": "https://arxiv.org/abs/2507.12862", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "在AI时代，本文提出了一种方法，将人类判断从模拟决策循环中移出，设计伦理度量空间，让模拟环境探索该空间，最终由人类指挥官选择最合适的行动方案。", "motivation": "解决在大量模拟场景中依赖人类判断进行伦理决策的低效和不可行问题。", "method": "设计伦理度量空间，利用模拟环境探索该空间，并采用多准则决策文献中的熵概念动态加权伦理属性。", "result": "提出了一种自动计算模拟测试和评估中伦理属性权重的方法。", "conclusion": "通过将人类判断移出模拟决策循环，可以高效探索大量场景，同时保留人类在最终决策中的作用。"}}
{"id": "2507.13190", "title": "GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems", "authors": ["Jisoo Lee", "Raeyoung Chang", "Dongwook Kwon", "Harmanpreet Singh", "Nikhil Verma"], "abstract": "Multi-agent systems built on language models have shown strong performance on collaborative reasoning tasks. However, existing evaluations focus only on the correctness of the final output, overlooking how inefficient communication and poor coordination contribute to redundant reasoning and higher computational costs. We introduce GEMMAS, a graph-based evaluation framework that analyzes the internal collaboration process by modeling agent interactions as a directed acyclic graph. To capture collaboration quality, we propose two process-level metrics: Information Diversity Score (IDS) to measure semantic variation in inter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant reasoning paths. We evaluate GEMMAS across five benchmarks and highlight results on GSM8K, where systems with only a 2.1% difference in accuracy differ by 12.8% in IDS and 80% in UPR, revealing substantial variation in internal collaboration. These findings demonstrate that outcome-only metrics are insufficient for evaluating multi-agent performance and highlight the importance of process-level diagnostics in designing more interpretable and resource-efficient collaborative AI systems.", "subjects": "Computation and Language (cs.CL)", "comments": "4 figures, 1 algorithm, 2 tables, 6 pages, under review at EMNLP Industry track 2025", "pdf_url": "https://arxiv.org/pdf/2507.13190.pdf", "abstract_url": "https://arxiv.org/abs/2507.13190", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "GEMMAS是一个基于图的评估框架，用于分析多智能体系统中的内部协作过程，通过建模智能体交互为有向无环图，提出两个过程级指标：信息多样性评分（IDS）和不必要路径比率（UPR），以评估协作质量。", "motivation": "现有的多智能体系统评估仅关注最终输出的正确性，忽视了低效沟通和不良协调导致的冗余推理和更高计算成本。", "method": "引入GEMMAS框架，通过建模智能体交互为有向无环图，提出IDS和UPR两个过程级指标来评估协作质量。", "result": "在GSM8K等五个基准测试中，准确率仅差2.1%的系统在IDS和UPR上分别有12.8%和80%的差异，揭示了内部协作的显著差异。", "conclusion": "仅基于结果的评估指标不足以全面评价多智能体性能，过程级诊断在设计更具解释性和资源效率的协作AI系统中至关重要。"}}
{"id": "2507.13285", "title": "Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis", "authors": ["Wang Xi", "Quan Shi", "Tian Yu", "Yujie Peng", "Jiayi Sun", "Mengxing Ren", "Zenghui Ding", "Ningguang Yao"], "abstract": "Automated generation of high-quality media presentations is challenging, requiring robust content extraction, narrative planning, visual design, and overall quality optimization. Existing methods often produce presentations with logical inconsistencies and suboptimal layouts, thereby struggling to meet professional standards. To address these challenges, we introduce RCPS (Reflective Coherent Presentation Synthesis), a novel framework integrating three key components: (1) Deep Structured Narrative Planning; (2) Adaptive Layout Generation; (3) an Iterative Optimization Loop. Additionally, we propose PREVAL, a preference-based evaluation framework employing rationale-enhanced multi-dimensional models to assess presentation quality across Content, Coherence, and Design. Experimental results demonstrate that RCPS significantly outperforms baseline methods across all quality dimensions, producing presentations that closely approximate human expert standards. PREVAL shows strong correlation with human judgments, validating it as a reliable automated tool for assessing presentation quality.", "subjects": "Computation and Language (cs.CL)", "comments": "22 pages, 7 figures, 3 tables. Submitted to an ACL-style conference", "pdf_url": "https://arxiv.org/pdf/2507.13285.pdf", "abstract_url": "https://arxiv.org/abs/2507.13285", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了RCPS框架，通过深度结构化叙事规划、自适应布局生成和迭代优化循环，解决了高质量媒体展示自动生成中的挑战。实验证明RCPS在内容、连贯性和设计方面显著优于基线方法。", "motivation": "解决现有方法在自动生成高质量媒体展示时存在的逻辑不一致和布局次优问题，以满足专业标准。", "method": "RCPS框架整合了深度结构化叙事规划、自适应布局生成和迭代优化循环三个关键组件，并提出了基于偏好的评估框架PREVAL。", "result": "RCPS在所有质量维度上显著优于基线方法，PREVAL与人类判断有强相关性，验证了其作为评估展示质量的可靠自动化工具。", "conclusion": "RCPS框架能够生成接近人类专家标准的展示，PREVAL框架为展示质量评估提供了可靠的自动化工具。"}}
{"id": "2507.13175", "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era", "authors": ["Matthew E. Brophy"], "abstract": "The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch. Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years. We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "42 pages. Supplementary material included at end of article", "pdf_url": "https://arxiv.org/pdf/2507.13175.pdf", "abstract_url": "https://arxiv.org/abs/2507.13175", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一套修订后的十项功能标准，用于评估基于大型语言模型（LLMs）的人工道德代理（AMAs），以应对LLMs不透明性和随机输出带来的挑战。", "motivation": "大型语言模型（LLMs）的强大但不透明的特性使得传统的道德代理评估标准不再适用，需要新的评估框架。", "method": "通过哲学技术核心主题的探讨，提出了一套十项功能标准，并通过假设情景（如自动驾驶公共汽车）展示其实际应用。", "result": "提出了十项功能标准，旨在指导LLMs模拟道德代理（SMA-LLS），以实现更好的社会融合和道德对齐。", "conclusion": "修订后的功能标准为评估和指导基于LLMs的人工道德代理提供了实用框架，有助于其在道德敏感情境中的更好应用。"}}
{"id": "2507.12486", "title": "On multiagent online problems with predictions", "authors": ["Gabriel Istrate", "Cosmin Bonchis", "Victor Bogdan"], "abstract": "We study the power of (competitive) algorithms with predictions in a multiagent setting. We introduce a two predictor framework, that assumes that agents use one predictor for their future (self) behavior, and one for the behavior of the other players. The main problem we are concerned with is understanding what are the best competitive ratios that can be achieved by employing such predictors, under various assumptions on predictor quality.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.12486.pdf", "abstract_url": "https://arxiv.org/abs/2507.12486", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Computer Science and Game Theory (cs.GT)"], "matching_keywords": ["agent"], "AI": {"tldr": "研究了在多智能体设置中，使用预测的（竞争性）算法的能力。引入了一个双预测器框架，该框架假设智能体使用一个预测器来预测自身未来的行为，另一个预测器来预测其他玩家的行为。主要关注的问题是理解在各种预测器质量假设下，通过使用这样的预测器可以实现的最佳竞争比。", "motivation": "解决在多智能体环境中，如何利用预测来提高算法的竞争性能的问题。", "method": "引入了一个双预测器框架，分别用于预测自身和其他智能体的行为。", "result": "探讨了在不同预测器质量假设下，可以实现的最佳竞争比。", "conclusion": "通过双预测器框架，可以在多智能体环境中有效地利用预测来提高算法的竞争性能，具体效果取决于预测器的质量。"}}
{"id": "2507.12475", "title": "Coarse Addition and the St. Petersburg Paradox: A Heuristic Perspective", "authors": ["Takashi Izumo"], "abstract": "The St. Petersburg paradox presents a longstanding challenge in decision theory. It describes a game whose expected value is infinite, yet for which no rational finite stake can be determined. Traditional solutions introduce auxiliary assumptions, such as diminishing marginal utility, temporal discounting, or extended number systems. These methods often involve mathematical refinements that may not correspond to how people actually perceive or process numerical information. This paper explores an alternative approach based on a modified operation of addition defined over coarse partitions of the outcome space. In this model, exact numerical values are grouped into perceptual categories, and each value is replaced by a representative element of its group before being added. This method allows for a phenomenon where repeated additions eventually cease to affect the outcome, a behavior described as inertial stabilization. Although this is not intended as a definitive resolution of the paradox, the proposed framework offers a plausible way to represent how agents with limited cognitive precision might handle divergent reward structures. We demonstrate that the St. Petersburg series can become inert under this coarse addition for a suitably constructed partition. The approach may also have broader applications in behavioral modeling and the study of machine reasoning under perceptual limitations.", "subjects": "Theoretical Economics (econ.TH); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)", "comments": "16 pages, no figure", "pdf_url": "https://arxiv.org/pdf/2507.12475.pdf", "abstract_url": "https://arxiv.org/abs/2507.12475", "categories": ["Theoretical Economics (econ.TH)", "Artificial Intelligence (cs.AI)", "Optimization and Control (math.OC)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了圣彼得堡悖论，提出了基于粗糙加法操作的替代解决方案，通过将数值分组并替换为代表元素来处理无限期望值问题，为有限认知能力的代理提供了一种处理发散奖励结构的可能方法。", "motivation": "解决圣彼得堡悖论这一决策理论中的长期挑战，该悖论描述了一个期望值无限但无法确定合理有限赌注的游戏。传统解决方案引入了辅助假设，但这些方法可能不符合人们实际处理数字信息的方式。", "method": "提出了一种基于粗糙加法操作的替代方法，即将精确数值分组到感知类别中，并在加法操作前用代表元素替换每个值，这种方法允许重复加法最终停止影响结果的现象，称为惯性稳定。", "result": "研究表明，圣彼得堡级数在适当构建的分区下可以变得惰性，这种方法也可能在行为建模和研究机器在感知限制下的推理中有更广泛的应用。", "conclusion": "虽然这不是对悖论的最终解决方案，但提出的框架为有限认知精度的代理处理发散奖励结构提供了一种合理的表示方法，并可能在其他领域有应用价值。"}}
{"id": "2507.12496", "title": "FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making", "authors": ["Yucen Wang", "Rui Yu", "Shenghua Wan", "Le Gan", "De-Chuan Zhan"], "abstract": "Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "Accepted by Forty-Second International Conference on Machine Learning (ICML 2025)", "pdf_url": "https://arxiv.org/pdf/2507.12496.pdf", "abstract_url": "https://arxiv.org/abs/2507.12496", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了FOUNDER框架，通过将基础模型（FMs）的通用知识与世界模型（WMs）的动态建模能力相结合，实现了在无奖励的具身环境中解决开放任务的能力。该方法通过学习一个映射函数，将FM表示映射到WM状态空间，从而在行为学习过程中通过想象学习目标条件策略，并利用预测到目标状态的时间距离作为奖励信号。FOUNDER在多种多任务离线视觉控制基准测试中表现出色，特别是在涉及复杂观察或领域差距的场景中，能够捕捉由文本或视频指定的任务的深层语义。", "motivation": "解决在具身环境中开放任务的无奖励学习问题，结合基础模型和世界模型的优势，以提高任务解决的通用性和适应性。", "method": "提出FOUNDER框架，学习一个映射函数将基础模型的表示映射到世界模型的状态空间，利用预测的时间距离作为奖励信号，通过想象学习目标条件策略。", "result": "FOUNDER在多种多任务离线视觉控制基准测试中表现优异，特别是在复杂观察或领域差距的场景中，能够有效捕捉任务的深层语义，且学习到的奖励函数与真实奖励一致。", "conclusion": "FOUNDER框架通过结合基础模型和世界模型的优势，成功实现了在无奖励的具身环境中解决开放任务的能力，为未来的研究提供了新的方向。"}}
{"id": "2507.13152", "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models", "authors": ["Xiangyu Dong", "Haoran Zhao", "Jiang Gao", "Haozhou Li", "Xiaoguang Ma", "Yaoming Zhou", "Fuhai Chen", "Juan Liu"], "abstract": "Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.13152.pdf", "abstract_url": "https://arxiv.org/abs/2507.13152", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于多模态大语言模型的自进化视觉语言导航框架（SE-VLN），旨在解决现有视觉语言导航（VLN）方法因固定知识库和推理能力而缺乏进化能力的问题。", "motivation": "现有的视觉语言导航方法虽然在大语言模型的帮助下展现出良好的泛化能力，但由于模型的知识库和推理能力固定，无法有效融入经验知识，限制了其进化能力。", "method": "SE-VLN框架包含三个核心模块：分层记忆模块用于将成功和失败案例转化为可重用知识，检索增强的基于思维的推理模块用于检索经验并实现多步决策，以及反思模块实现持续进化。", "result": "在未见过的环境中，SE-VLN在R2R和REVERSE数据集上的导航成功率分别达到57%和35.2%，比现有最先进方法分别提高了23.9%和15.0%。", "conclusion": "SE-VLN作为一种自进化代理框架，随着经验库的增加显示出性能提升，展现了其在视觉语言导航领域的巨大潜力。"}}
{"id": "2507.13334", "title": "A Survey of Context Engineering for Large Language Models", "authors": ["Lingrui Mei", "Jiayu Yao", "Yuyao Ge", "Yiwei Wang", "Baolong Bi", "Yujun Cai", "Jiazhi Liu", "Mingyu Li", "Zhong-Zhi Li", "Duzhen Zhang", "Chenlin Zhou", "Jiayi Mao", "Tianze Xia", "Jiafeng Guo", "Shenghua Liu"], "abstract": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.", "subjects": "Computation and Language (cs.CL)", "comments": "ongoing work; 165 pages, 1401 citations", "pdf_url": "https://arxiv.org/pdf/2507.13334.pdf", "abstract_url": "https://arxiv.org/abs/2507.13334", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本文介绍了上下文工程（Context Engineering），这是一个超越简单提示设计的正式学科，旨在系统优化大型语言模型（LLMs）的信息负载。通过分析1300多篇研究论文，本文不仅为该领域建立了技术路线图，还揭示了一个关键的研究空白：模型能力之间存在基本不对称。", "motivation": "解决大型语言模型（LLMs）在推理过程中上下文信息提供的系统优化问题，以及模型在理解复杂上下文和生成同等复杂长形式输出之间的能力不对称问题。", "method": "通过分解上下文工程的基础组件（上下文检索和生成、上下文处理和上下文管理）及其在智能系统中的高级实现（检索增强生成（RAG）、内存系统和工具集成推理、多代理系统），进行系统分析。", "result": "揭示了模型在理解复杂上下文和生成复杂长形式输出之间的能力不对称，提出了解决这一问题的未来研究方向。", "conclusion": "本文为研究人员和工程师提供了一个统一的框架，以推进上下文感知AI的发展，并指出了解决模型能力不对称的未来研究重点。"}}
{"id": "2507.12774", "title": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models", "authors": ["Weijieying Ren", "Jingxi Zhu", "Zehao Liu", "Tianxiang Zhao", "Vasant Honavar"], "abstract": "Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.12774.pdf", "abstract_url": "https://arxiv.org/abs/2507.12774", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文综述了电子健康记录（EHR）建模的最新进展，特别是深度学习和大型语言模型（LLMs）的应用，提出了一个统一的分类法，涵盖了数据为中心的方法、神经架构设计、学习策略、多模态学习和基于LLM的建模系统五个关键设计维度，并讨论了当前的挑战和未来方向。", "motivation": "电子健康记录（EHRs）数据的异质性、时间不规则性和领域特定性给AI在医疗保健领域的应用带来了独特的挑战，不同于视觉和自然语言任务。本文旨在提供一个全面的概述，以推动AI驱动的EHR建模和临床决策支持的发展。", "method": "本文提出了一种统一的分类法，涵盖了五个关键设计维度：数据为中心的方法、神经架构设计、学习策略、多模态学习和基于LLM的建模系统，并在每个维度下回顾了代表性的方法。", "result": "综述了EHR建模的最新进展，包括数据质量增强、结构和时间表示、自监督学习以及与临床知识的整合等方面的方法，并突出了基础模型、LLM驱动的临床代理和EHR到文本翻译等新兴趋势。", "conclusion": "本文旨在为推进AI驱动的EHR建模和临床决策支持提供结构化的路线图，同时指出了在基准测试、可解释性、临床对齐和跨不同临床环境的泛化等方面的开放挑战。"}}
{"id": "2507.12624", "title": "Pathology-Guided Virtual Staining Metric for Evaluation and Training", "authors": ["Qiankai Wang", "James E.D. Tweel", "Parsin Haji Reza", "Anita Layton"], "abstract": "Virtual staining has emerged as a powerful alternative to traditional histopathological staining techniques, enabling rapid, reagent-free image transformations. However, existing evaluation methods predominantly rely on full-reference image quality assessment (FR-IQA) metrics such as structural similarity, which are originally designed for natural images and often fail to capture pathology-relevant features. Expert pathology reviews have also been used, but they are inherently subjective and time-consuming.", "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)", "comments": "19 pages, 10 figures. Intended for submission to the Journal of Imaging Informatics in Medicine (JIIM)", "pdf_url": "https://arxiv.org/pdf/2507.12624.pdf", "abstract_url": "https://arxiv.org/abs/2507.12624", "categories": ["Image and Video Processing (eess.IV)", "Computer Vision and Pattern Recognition (cs.CV)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种病理学引导的虚拟染色评估和训练指标，以改进现有基于自然图像设计的评估方法。", "motivation": "现有的虚拟染色评估方法主要依赖为自然图像设计的全参考图像质量评估（FR-IQA）指标，这些指标往往无法捕捉到病理相关的特征，而专家病理学审查又具有主观性和耗时性。", "method": "提出了一种病理学引导的虚拟染色指标，用于评估和训练虚拟染色技术。", "result": "该方法旨在更准确地评估虚拟染色图像的质量，特别是在捕捉病理相关特征方面。", "conclusion": "病理学引导的虚拟染色指标为虚拟染色技术的评估和训练提供了一种更有效的方法，有助于提高虚拟染色图像的质量和实用性。"}}
{"id": "2507.12767", "title": "Autonomy for Older Adult-Agent Interaction", "authors": ["Jiaxin An"], "abstract": "As the global population ages, artificial intelligence (AI)-powered agents have emerged as potential tools to support older adults' caregiving. Prior research has explored agent autonomy by identifying key interaction stages in task processes and defining the agent's role at each stage. However, ensuring that agents align with older adults' autonomy preferences remains a critical challenge. Drawing on interdisciplinary conceptualizations of autonomy, this paper examines four key dimensions of autonomy for older adults: decision-making autonomy, goal-oriented autonomy, control autonomy, and social responsibility autonomy. This paper then proposes the following research directions: (1) Addressing social responsibility autonomy, which concerns the ethical and social implications of agent use in communal settings; (2) Operationalizing agent autonomy from the task perspective; and (3) Developing autonomy measures.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "comments": "7 pages", "pdf_url": "https://arxiv.org/pdf/2507.12767.pdf", "abstract_url": "https://arxiv.org/abs/2507.12767", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "随着全球人口老龄化，人工智能（AI）驱动的代理成为支持老年人护理的潜在工具。本文探讨了老年人代理交互中的自主权问题，提出了四个关键维度，并提出了研究方向。", "motivation": "解决AI代理在老年人护理中如何与老年人的自主权偏好对齐的关键挑战。", "method": "通过跨学科的自主权概念化，分析了老年人自主权的四个关键维度，并提出了三个研究方向。", "result": "提出了关于老年人代理交互中自主权的四个维度和三个研究方向，包括社会责任感自主权、从任务角度操作代理自主权以及开发自主权测量方法。", "conclusion": "本文强调了在老年人护理中AI代理的自主权问题的重要性，并提出了未来研究的方向，以确保代理能够更好地满足老年人的需求和偏好。"}}
{"id": "2507.13169", "title": "Prompt Injection 2.0: Hybrid AI Threats", "authors": ["Jeremy McHugh", "Kristina Šekrst", "Jon Cefalu"], "abstract": "Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape. Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.13169.pdf", "abstract_url": "https://arxiv.org/abs/2507.13169", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文分析了Prompt Injection 2.0，探讨了如何将提示注入与XSS、CSRF等网络安全漏洞结合，以绕过传统安全措施。", "motivation": "解决提示注入攻击与网络安全漏洞结合产生的混合威胁问题。", "method": "结合Preamble的基础研究和缓解技术，评估当代威胁，包括AI蠕虫、多代理感染和混合网络-AI攻击。", "result": "传统网络安全措施如Web应用防火墙、XSS过滤器和CSRF令牌对AI增强攻击无效。", "conclusion": "提出了结合提示隔离、运行时安全和权限分离与新型威胁检测能力的架构解决方案。"}}
{"id": "2507.12846", "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering", "authors": ["Muhammad Fadhil Ginting", "Dong-Ki Kim", "Xiangyun Meng", "Andrzej Reinke", "Bandi Jai Krishna", "Navid Kayhani", "Oriana Peltzer", "David D. Fan", "Amirreza Shaban", "Sung-Kyun Kim", "Mykel J. Kochenderfer", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "abstract": "As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.12846.pdf", "abstract_url": "https://arxiv.org/abs/2507.12846", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文研究了长期主动具身问答（LA-EQA）问题，提出了一种受认知科学中“记忆宫殿”方法启发的结构化记忆系统，用于机器人在长期操作中积累知识并有效回答复杂问题。", "motivation": "解决机器人在长期操作中如何有效积累和利用环境知识，以回答基于时间的复杂问题，克服传统EQA方法在上下文窗口限制、缺乏持久记忆和无法结合记忆检索与主动探索方面的不足。", "method": "提出了一种基于场景图的世界实例编码的叙事经验的结构化记忆系统，结合了推理和规划算法，以及基于信息价值的停止标准，以平衡探索与记忆检索的权衡。", "result": "在真实世界实验和新基准测试中，该方法在回答准确性和探索效率方面显著优于现有技术基线。", "conclusion": "通过结构化记忆系统和有效的探索-记忆检索平衡策略，机器人能够在长期操作中更有效地回答复杂问题，为未来的机器人辅助人类任务提供了新的可能性。"}}
{"id": "2507.13171", "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback", "authors": ["Suzie Kim", "Hye-Bin Shin", "Seong-Whan Lee"], "abstract": "Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.13171.pdf", "abstract_url": "https://arxiv.org/abs/2507.13171", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新颖的强化学习框架RLIHF，利用非侵入性脑电图（EEG）信号提供连续的隐式反馈，以解决稀疏奖励条件下的策略学习问题。", "motivation": "传统的强化学习方法在稀疏奖励条件下难以学习有效策略，且依赖显式反馈机制会增加用户认知负担。", "method": "采用预训练的解码器将原始EEG信号转化为概率奖励组件，结合隐式人类反馈进行策略学习。", "result": "在MuJoCo物理引擎的模拟环境中，使用Kinova Gen2机械臂进行的复杂拾放任务显示，基于EEG反馈训练的代理性能与密集手动设计奖励相当。", "conclusion": "研究验证了利用隐式神经反馈在交互式机器人中进行可扩展且与人类对齐的强化学习的潜力。"}}
