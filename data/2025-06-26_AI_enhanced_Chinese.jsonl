{"id": "2506.20045", "title": "Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception", "authors": ["Eric C. Joyce", "Qianwen Zhao", "Nathaniel Burgdorfer", "Long Wang", "Philippos Mordohai"], "abstract": "Deep object pose estimators are notoriously overconfident. A grasping agent that both estimates the 6-DoF pose of a target object and predicts the uncertainty of its own estimate could avoid task failure by choosing not to act under high uncertainty. Even though object pose estimation improves and uncertainty quantification research continues to make strides, few studies have connected them to the downstream task of robotic grasping. We propose a method for training lightweight, deep networks to predict whether a grasp guided by an image-based pose estimate will succeed before that grasp is attempted. We generate training data for our networks via object pose estimation on real images and simulated grasping. We also find that, despite high object variability in grasping trials, networks benefit from training on all objects jointly, suggesting that a diverse variety of objects can nevertheless contribute to the same goal.", "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20045.pdf", "abstract_url": "https://arxiv.org/abs/2506.20045", "categories": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种训练轻量级深度网络的方法，用于在尝试抓取之前预测基于图像姿态估计的抓取是否会成功。通过在实际图像上进行物体姿态估计和模拟抓取生成训练数据，研究发现尽管抓取试验中物体变异性高，但网络通过联合训练所有物体受益，表明多样化的物体仍可为同一目标做出贡献。", "motivation": "解决深度物体姿态估计器过于自信的问题，通过预测自身估计的不确定性，避免在高不确定性下执行抓取任务，从而减少任务失败。", "method": "提出了一种训练轻量级深度网络的方法，利用在实际图像上进行物体姿态估计和模拟抓取生成的训练数据，预测抓取成功率。", "result": "研究发现，尽管抓取试验中物体变异性高，但网络通过联合训练所有物体受益，表明多样化的物体仍可为同一目标做出贡献。", "conclusion": "通过预测抓取成功率，可以在高不确定性下避免执行抓取任务，减少任务失败，同时多样化的物体训练数据有助于提高网络的预测性能。"}}
{"id": "2506.19923", "title": "Prover Agent: An Agent-based Framework for Formal Mathematical Proofs", "authors": ["Kaito Baba", "Chaoran Liu", "Shuhei Kurita", "Akiyoshi Sannai"], "abstract": "We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas to assist in discovering the overall proof strategy. It achieves an 86.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present case studies illustrating how these generated lemmas contribute to solving challenging problems.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "22 pages, 2 figures", "pdf_url": "https://arxiv.org/pdf/2506.19923.pdf", "abstract_url": "https://arxiv.org/abs/2506.19923", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "Prover Agent是一个基于AI的自动化定理证明框架，它结合了大型语言模型(LLMs)和形式化证明助手Lean，通过协调非正式推理LLM、形式化证明模型及Lean的反馈，同时生成辅助引理来帮助发现整体证明策略。在MiniF2F基准测试中，它达到了86.1%的成功率，成为使用小型语言模型(SLMs)且样本预算远低于先前方法的新技术标杆。", "motivation": "解决自动化定理证明中的挑战，特别是在使用小型语言模型时如何提高成功率和效率。", "method": "整合大型语言模型(LLMs)与形式化证明助手Lean，通过协调非正式推理LLM、形式化证明模型及Lean的反馈，同时生成辅助引理来辅助证明策略的发现。", "result": "在MiniF2F基准测试中达到86.1%的成功率，成为使用小型语言模型(SLMs)且样本预算较低的技术标杆。", "conclusion": "Prover Agent通过整合LLMs和Lean，有效地提高了自动化定理证明的成功率和效率，特别是在使用小型语言模型时，展示了其在解决复杂数学问题中的潜力。"}}
{"id": "2506.20670", "title": "MMSearch-R1: Incentivizing LMMs to Search", "authors": ["Jinming Wu", "Zihao Deng", "Wei Li", "Yiding Liu", "Bo You", "Bo Li", "Zejun Ma", "Ziwei Liu"], "abstract": "Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2506.20670.pdf", "abstract_url": "https://arxiv.org/abs/2506.20670", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "MMSearch-R1是首个端到端强化学习框架，旨在激励大型多模态模型（LMMs）在现实互联网环境中进行按需、多轮搜索。该框架整合了图像和文本搜索工具，通过基于结果的奖励和搜索惩罚引导模型决定何时及如何调用这些工具。通过在知识密集型和信息寻求型VQA任务上的广泛实验，证明该模型不仅优于相同模型大小的基于RAG的基线，而且在减少30%以上的搜索调用的同时，匹配了更大RAG模型的性能。", "motivation": "现实世界信息的复杂性和动态性要求大型多模态模型（LMMs）能够访问外部知识源。现有的检索增强生成（RAG）和提示工程搜索代理等方法依赖于刚性流程，常导致搜索行为效率低下或过度。", "method": "提出了MMSearch-R1，一个端到端的强化学习框架，整合了图像和文本搜索工具，通过基于结果的奖励和搜索惩罚来引导模型决定何时及如何调用这些工具。为了支持训练，通过半自动化流程收集了一个多模态搜索VQA数据集，并策划了一个搜索平衡的子集。", "result": "在知识密集型和信息寻求型VQA任务上的实验显示，MMSearch-R1不仅优于相同模型大小的RAG基线，而且在减少30%以上的搜索调用的同时，匹配了更大RAG模型的性能。", "conclusion": "MMSearch-R1通过强化学习框架有效激励LMMs进行高效、按需的多模态搜索，为多模态搜索研究的进步提供了可行的见解。"}}
{"id": "2506.19967", "title": "Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs", "authors": ["Travis Thompson", "Seung-Hwan Lim", "Paul Liu", "Ruoying He", "Dongkuan Xu"], "abstract": "Large Language Models (LLMs) have achieved impressive capabilities in language understanding and generation, yet they continue to underperform on knowledge-intensive reasoning tasks due to limited access to structured context and multi-hop information. Retrieval-Augmented Generation (RAG) partially mitigates this by grounding generation in retrieved context, but conventional RAG and GraphRAG methods often fail to capture relational structure across nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel framework that enhances LLM-based graph reasoning by applying inference-time compute scaling. Our method combines sequential scaling with deep chain-of-thought graph traversal, and parallel scaling with majority voting over sampled trajectories within an interleaved reasoning-execution loop. Experiments on the GRBench benchmark demonstrate that our approach significantly improves multi-hop question answering performance, achieving substantial gains over both traditional GraphRAG and prior graph traversal baselines. These findings suggest that inference-time scaling is a practical and architecture-agnostic solution for structured knowledge reasoning with LLMs", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.19967.pdf", "abstract_url": "https://arxiv.org/abs/2506.19967", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了Inference-Scaled GraphRAG，一种通过推理时计算缩放增强基于LLM的图推理的新框架，显著提高了多跳问答性能。", "motivation": "大型语言模型（LLMs）在语言理解和生成方面表现出色，但在知识密集型推理任务上表现不佳，主要是因为缺乏对结构化上下文和多跳信息的访问。", "method": "提出了Inference-Scaled GraphRAG框架，结合了顺序缩放与深度思维链图遍历，以及并行缩放与在交错推理-执行循环中对采样轨迹的多数投票。", "result": "在GRBench基准测试中，该方法显著提高了多跳问答的性能，优于传统的GraphRAG和先前的图遍历基线。", "conclusion": "推理时缩放是一种实用且架构无关的解决方案，可用于LLMs的结构化知识推理。"}}
{"id": "2506.20100", "title": "MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations", "authors": ["Vardhan Dongre", "Chi Gui", "Shubham Garg", "Hooshang Nayyeri", "Gokhan Tur", "Dilek Hakkani-Tür", "Vikram S. Adve"], "abstract": "We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning and decision-making in consultative interaction settings. Designed for the agriculture domain, MIRAGE captures the full complexity of expert consultations by combining natural user queries, expert-authored responses, and image-based context, offering a high-fidelity benchmark for evaluating models on grounded reasoning, clarification strategies, and long-form generation in a real-world, knowledge-intensive domain. Grounded in over 35,000 real user-expert interactions and curated through a carefully designed multi-step pipeline, MIRAGE spans diverse crop health, pest diagnosis, and crop management scenarios. The benchmark includes more than 7,000 unique biological entities, covering plant species, pests, and diseases, making it one of the most taxonomically diverse benchmarks available for vision-language models, grounded in the real world. Unlike existing benchmarks that rely on well-specified user inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich scenarios with open-world settings, requiring models to infer latent knowledge gaps, handle rare entities, and either proactively guide the interaction or respond. Project Page:", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)", "comments": "66 pages, 32 figures, 23 tables", "pdf_url": "https://arxiv.org/pdf/2506.20100.pdf", "abstract_url": "https://arxiv.org/abs/2506.20100", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "MIRAGE是一个新的多模态基准测试，专注于农业领域的专家级推理和决策制定，通过结合自然用户查询、专家编写的响应和基于图像的上下文，为评估模型在真实世界知识密集型领域的推理、澄清策略和长文本生成能力提供了高保真度的基准。", "motivation": "解决现有基准测试在用户输入明确和封闭分类体系下的局限性，MIRAGE旨在捕捉专家咨询的复杂性，包括推理、澄清策略和长文本生成，特别是在农业领域。", "method": "基于超过35,000个真实用户与专家互动的数据，通过精心设计的多步骤流程进行筛选，MIRAGE涵盖了作物健康、害虫诊断和作物管理等多种场景，包含超过7,000个独特的生物实体。", "result": "MIRAGE成为视觉语言模型中分类最多样化的基准之一，支持开放世界设置下的未指定、上下文丰富的场景，要求模型推断潜在的知识差距、处理罕见实体并主动引导互动或响应。", "conclusion": "MIRAGE为多模态信息寻求和推理提供了一个真实世界、知识密集型的基准，特别是在农业领域，推动了模型在复杂交互和长文本生成方面的评估和发展。"}}
{"id": "2506.19998", "title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "abstract": "REST APIs play important roles in enriching the action space of web agents, yet most API-based agents rely on curated and uniform toolsets that do not reflect the complexity of real-world APIs. Building tool-using agents for arbitrary domains remains a major challenge, as it requires reading unstructured API documentation, testing APIs and inferring correct parameters. We propose Doc2Agent, a scalable pipeline to build agents that can call Python-based tools generated from API documentation. Doc2Agent generates executable tools from API documentations and iteratively refines them using a code agent. We evaluate our approach on real-world APIs, WebArena APIs, and research APIs, producing validated tools. We achieved a 55\\% relative performance improvement with 90\\% lower cost compared to direct API calling on WebArena benchmark. A domain-specific agent built for glycomaterial science further demonstrates the pipeline's adaptability to complex, knowledge-rich tasks. Doc2Agent offers a generalizable solution for building tool agents from unstructured API documentation at scale.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.19998.pdf", "abstract_url": "https://arxiv.org/abs/2506.19998", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "Doc2Agent是一个可扩展的管道，用于从API文档构建能够调用基于Python的工具的代理。它通过从API文档生成可执行工具，并使用代码代理迭代优化这些工具，显著提高了性能和降低了成本。", "motivation": "解决基于API的代理依赖于精心策划和统一的工具集，无法反映现实世界API复杂性的问题，以及为任意领域构建使用工具的代理的挑战。", "method": "提出Doc2Agent，一个从API文档生成可执行工具，并使用代码代理迭代优化这些工具的管道。", "result": "在真实世界的API、WebArena API和研究API上评估，实现了55%的相对性能提升和90%的成本降低。", "conclusion": "Doc2Agent为从非结构化API文档大规模构建工具代理提供了一个通用的解决方案，展示了在复杂、知识丰富的任务中的适应性。"}}
{"id": "2506.20009", "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks", "authors": ["Konstantinos Vrettos", "Michail E. Klontzas"], "abstract": "Background The increasing adoption of Artificial Intelligence (AI) in healthcare has sparked growing concerns about its environmental and ethical implications. Commercial Large Language Models (LLMs), such as ChatGPT and DeepSeek, require substantial resources, while the utilization of these systems for medical purposes raises critical issues regarding patient privacy and safety. Methods We developed a customizable Retrieval-Augmented Generation (RAG) framework for medical tasks, which monitors its energy usage and CO2 emissions. This system was then used to create RAGs based on various open-source LLMs. The tested models included both general purpose models like llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs o4-mini model. A dataset of medical questions was used for the evaluation. Results Custom RAG models outperformed commercial models in accuracy and energy consumption. The RAG model built on llama3.1:8B achieved the highest accuracy (58.5%) and was significantly better than other models, including o4-mini and DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption and CO2 footprint among all models, with a Performance per kWh of 0.52 and a total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x times more accuracy points per kWh and 172% less electricity usage while maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs can be leveraged to develop RAGs that outperform commercial, online LLMs in medical tasks, while having a smaller environmental impact. Our modular framework promotes sustainable AI development, reducing electricity usage and aligning with the UNs Sustainable Development Goals.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": "18 pages, 3 Figures", "pdf_url": "https://arxiv.org/pdf/2506.20009.pdf", "abstract_url": "https://arxiv.org/abs/2506.20009", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "研究表明，本地检索增强生成（RAG）模型在医疗任务中不仅准确性高于商业大型语言模型（LLM），而且能耗更低，对环境的影响更小。", "motivation": "解决商业LLM在医疗应用中资源消耗大、隐私和安全问题突出的问题。", "method": "开发了一个可定制的RAG框架，用于医疗任务，监测其能源使用和CO2排放，并基于多种开源LLM创建RAG模型。", "result": "基于llama3.1:8B的RAG模型在准确性和能源消耗方面表现最佳，准确率达到58.5%，能耗和CO2排放最低。", "conclusion": "本地LLM可以开发出在医疗任务中超越商业在线LLM的RAG模型，同时减少环境影响，支持可持续AI发展。"}}
{"id": "2506.20008", "title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "authors": ["Abdul Basit", "Minghao Shao", "Haider Asif", "Nouhaila Innan", "Muhammad Kashif", "Alberto Marchisio", "Muhammad Shafique"], "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum code generation using real-world challenges from the Quantum Hackathon (QHack). We introduce QHackBench, a novel benchmark dataset derived from QHack competitions, and evaluate model performance under vanilla prompting and Retrieval-Augmented Generation (RAG). Our structured evaluation framework assesses functional correctness, syntactic validity, and execution success across varying challenge difficulties. Results indicate that RAG-enhanced models, supplemented with an augmented PennyLane dataset, approximately generate similar results as the standard prompting, particularly in complex quantum algorithms. Additionally, we introduce a multi-agent evaluation pipeline that iteratively refines incorrect solutions, further enhancing execution success rates. To foster further research, we commit to publicly releasing QHackBench, along with our evaluation framework and experimental results, enabling continued advancements in AI-assisted quantum programming.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "pdf_url": "https://arxiv.org/pdf/2506.20008.pdf", "abstract_url": "https://arxiv.org/abs/2506.20008", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本文介绍了QHackBench，一个基于PennyLane的量子代码生成基准测试数据集，用于评估大型语言模型（LLMs）在量子计算中的表现。通过真实世界的量子黑客马拉松挑战，研究比较了普通提示和检索增强生成（RAG）方法的效果，并提出了一个多代理评估管道以提高执行成功率。", "motivation": "大型语言模型在代码生成方面显示出强大潜力，但在量子计算领域的有效性尚未充分探索。本文旨在填补这一空白，通过基准测试评估LLMs在量子代码生成中的表现。", "method": "研究使用了来自量子黑客马拉松（QHack）的真实挑战构建了QHackBench数据集，并采用了普通提示和检索增强生成（RAG）两种方法进行评估。此外，引入了一个多代理评估管道来迭代改进不正确的解决方案。", "result": "结果表明，RAG增强的模型在复杂量子算法中生成的结果与标准提示方法相似。多代理评估管道进一步提高了执行成功率。", "conclusion": "QHackBench及其评估框架和实验结果的公开发布将促进AI辅助量子编程的持续进步。研究强调了RAG和多代理方法在提高量子代码生成质量方面的潜力。"}}
{"id": "2506.20128", "title": "CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation", "authors": ["Aashiq Muhamed"], "abstract": "RAG systems enhance LLMs by incorporating external knowledge, which is crucial for domains that demand factual accuracy and up-to-date information. However, evaluating the multifaceted quality of RAG outputs, spanning aspects such as contextual coherence, query relevance, factual correctness, and informational completeness, poses significant challenges. Existing evaluation methods often rely on simple lexical overlap metrics, which are inadequate for capturing these nuances, or involve complex multi-stage pipelines with intermediate steps like claim extraction or require finetuning specialized judge models, hindering practical efficiency. To address these limitations, we propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five metrics that utilizes a single, powerful, pretrained LLM as a zero-shot, end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance (QR), Information Density (ID), Answer Correctness (AC), and Information Recall (IR). We apply CCRS to evaluate six diverse RAG system configurations on the challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively discriminates between system performances, confirming, for instance, that the Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of CCRS metric properties, including score distributions, convergent/discriminant validity, tie rates, population statistics, and discriminative power. Compared to the complex RAGChecker framework, CCRS offers comparable or superior discriminative power for key aspects like recall and faithfulness, while being significantly more computationally efficient. CCRS thus provides a practical, comprehensive, and efficient framework for evaluating and iteratively improving RAG systems.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "Accepted at LLM4Eval @ SIGIR 2025", "pdf_url": "https://arxiv.org/pdf/2506.20128.pdf", "abstract_url": "https://arxiv.org/abs/2506.20128", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "CCRS是一个零样本LLM-as-a-Judge框架，用于全面评估RAG系统，通过五个新颖的指标在BioASQ数据集上有效区分不同系统性能，相比现有方法更高效且具有可比或更优的判别力。", "motivation": "解决现有RAG系统评估方法在捕捉上下文连贯性、查询相关性、事实正确性和信息完整性等多方面质量时的不足，以及这些方法要么过于简单（如词汇重叠度量），要么效率低下（如需要微调专业评判模型或多阶段流程）的问题。", "method": "提出CCRS（上下文连贯性和相关性评分），一套五个指标的评估框架，利用单一预训练的大型语言模型（LLM）作为零样本、端到端的评判者，评估包括上下文连贯性（CC）、问题相关性（QR）、信息密度（ID）、答案正确性（AC）和信息召回（IR）。", "result": "在具有挑战性的BioASQ数据集上评估六种不同的RAG系统配置，CCRS有效区分了系统性能，例如确认Mistral-7B阅读器优于Llama变体，并在关键方面如召回和忠实度上提供了与复杂RAGChecker框架相当或更优的判别力，同时计算效率显著更高。", "conclusion": "CCRS为评估和迭代改进RAG系统提供了一个实用、全面且高效的框架，解决了现有评估方法的局限性，同时保持了或提高了判别力和效率。"}}
{"id": "2506.20249", "title": "Language Modeling by Language Models", "authors": ["Junyan Cheng", "Peter Clark", "Kyle Richardson"], "abstract": "Can we leverage LLMs to model the process of discovering novel language model (LM) architectures? Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system, Genesys, employs a Ladder of Scales approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\\sim$86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified through pre-training) and find the best designs to be highly competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks). We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20249.pdf", "abstract_url": "https://arxiv.org/abs/2506.20249", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种多代理大型语言模型（LLM）方法Genesys，用于模拟从构思到验证的研究过程，通过遗传编程骨干网络高效发现新的语言模型架构，并在实验中验证了其优于现有架构的性能。", "motivation": "探索是否可以利用大型语言模型（LLMs）来模拟发现新型语言模型（LM）架构的过程，以解决传统研究方法在效率和效果上的限制。", "method": "采用多代理LLM方法，结合缩放法则的阶梯式规模方法（Ladder of Scales）和遗传编程骨干网络，从提案阶段到验证阶段全流程模拟研究过程。", "result": "在1,162个新发现的设计中，1,062个通过预训练完全验证，最佳设计在6/9的常见基准测试中优于已知架构（如GPT2、Mamba2等）。", "conclusion": "Genesys系统不仅展示了在语言模型架构发现中的高效性和有效性，还为设计有效的自主发现系统提供了更广泛的见解。"}}
{"id": "2506.20274", "title": "Enterprise Large Language Model Evaluation Benchmark", "authors": ["Liya Wang", "David Yi", "Damien Jose", "John Passarelli", "James Gao", "Jordan Leventis", "Kang Li"], "abstract": "Large Language Models (LLMs) ) have demonstrated promise in boosting productivity across AI-powered tools, yet existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy to holistically evaluate LLM capabilities in enterprise contexts. To address challenges of noisy data and costly annotation, we develop a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six leading models shows open-source contenders like DeepSeek R1 rival proprietary models in reasoning tasks but lag in judgment-based scenarios, likely due to overthinking. Our benchmark reveals critical enterprise performance gaps and offers actionable insights for model optimization. This work provides enterprises a blueprint for tailored evaluations and advances practical LLM deployment.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2506.20274.pdf", "abstract_url": "https://arxiv.org/abs/2506.20274", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "提出了一个基于布鲁姆分类法的14任务框架，用于全面评估大型语言模型在企业环境中的能力，并开发了一个可扩展的管道来应对数据噪声和标注成本高的挑战。", "motivation": "现有的大型语言模型评估基准（如MMLU）不足以评估企业特定任务的复杂性。", "method": "开发了一个结合LLM-as-a-Labeler、LLM-as-a-Judge和纠正性检索增强生成（CRAG）的可扩展管道，构建了一个包含9,700个样本的稳健基准。", "result": "评估了六个领先模型，发现开源模型如DeepSeek R1在推理任务上与专有模型相媲美，但在基于判断的场景中表现不佳，可能是由于过度思考。", "conclusion": "这项工作为企业提供了量身定制评估的蓝图，并推动了大型语言模型的实际部署。"}}
{"id": "2506.20332", "title": "Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards", "authors": ["Jihao Gu", "Qihang Ai", "Yingyao Wang", "Pi Bu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Ziming Wang", "Yingxiu Zhao", "Ming-Liang Zhang", "Jun Song", "Yuning Jiang", "Bo Zheng"], "abstract": "Vision-language model-based mobile agents have gained the ability to not only understand complex instructions and mobile screenshots, but also optimize their action outputs via thinking and reasoning, benefiting from reinforcement learning, such as Group Relative Policy Optimization (GRPO). However, existing research centers on offline reinforcement learning training or online optimization using action-level rewards, which limits the agent's dynamic interaction with the environment. This often results in agents settling into local optima, thereby weakening their ability for exploration and error action correction. To address these challenges, we introduce an approach called Mobile-R1, which employs interactive multi-turn reinforcement learning with task-level rewards for mobile agents. Our training framework consists of three stages: initial format finetuning, single-step online training via action-level reward, followed by online training via task-level reward based on multi-turn trajectories. This strategy is designed to enhance the exploration and error correction capabilities of Mobile-R1, leading to significant performance improvements. Moreover, we have collected a dataset covering 28 Chinese applications with 24,521 high-quality manual annotations and established a new benchmark with 500 trajectories. We will open source all resources, including the dataset, benchmark, model weight, and codes:", "subjects": "Artificial Intelligence (cs.AI)", "comments": "14 pages, 12 figures", "pdf_url": "https://arxiv.org/pdf/2506.20332.pdf", "abstract_url": "https://arxiv.org/abs/2506.20332", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为Mobile-R1的方法，通过任务级奖励进行交互式多轮强化学习，以提升基于视觉语言模型的移动代理的探索和错误纠正能力。", "motivation": "现有的研究主要集中在离线强化学习训练或使用动作级奖励进行在线优化，这限制了代理与环境的动态交互，导致代理陷入局部最优，削弱了其探索和错误动作纠正的能力。", "method": "Mobile-R1采用交互式多轮强化学习与任务级奖励相结合的框架，包括初始格式微调、通过动作级奖励进行单步在线训练，以及基于多轮轨迹的任务级奖励在线训练三个阶段。", "result": "该方法显著提升了Mobile-R1的探索和错误纠正能力，并在包含28个中国应用程序和24,521个高质量手动注释的数据集上建立了新的基准。", "conclusion": "Mobile-R1通过任务级奖励的交互式强化学习，有效提升了移动代理的性能，所有资源包括数据集、基准、模型权重和代码将被开源。"}}
{"id": "2506.20504", "title": "Engineering Sentience", "authors": ["Konstantin Demin", "Taylor Webb", "Eric Elmoznino", "Hakwan Lau"], "abstract": "We spell out a definition of sentience that may be useful for designing and building it in machines. We propose that for sentience to be meaningful for AI, it must be fleshed out in functional, computational terms, in enough detail to allow for implementation. Yet, this notion of sentience must also reflect something essentially 'subjective', beyond just having the general capacity to encode perceptual content. For this specific functional notion of sentience to occur, we propose that certain sensory signals need to be both assertoric (persistent) and qualitative. To illustrate the definition in more concrete terms, we sketch out some ways for potential implementation, given current technology. Understanding what it takes for artificial agents to be functionally sentient can also help us avoid creating them inadvertently, or at least, realize that we have created them in a timely manner.", "subjects": "Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20504.pdf", "abstract_url": "https://arxiv.org/abs/2506.20504", "categories": ["Artificial Intelligence (cs.AI)", "Neurons and Cognition (q-bio.NC)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种可能有助于在机器中设计和构建感知的定义，强调感知在AI中的功能性、计算性表达，并探讨了实现这一概念的具体方法。", "motivation": "解决如何在人工智能中定义和实现感知（sentience）的问题，使其既具有功能性计算表达，又包含主观性。", "method": "提出感知必须具有断言性（持久性）和质性，并在当前技术条件下探讨了实现这一概念的具体方法。", "result": "提出了一个具体的功能性感知定义，并探讨了其在人工智能中的潜在实现路径。", "conclusion": "理解人工代理如何实现功能性感知，有助于避免无意中创建它们，或至少及时意识到它们的创建。"}}
{"id": "2506.20430", "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning", "authors": ["Weike Zhao", "Chaoyi Wu", "Yanjie Fan", "Xiaoman Zhang", "Pengcheng Qiu", "Yuze Sun", "Xiao Zhou", "Yanfeng Wang", "Ya Zhang", "Yongguo Yu", "Kun Sun", "Weidi Xie"], "abstract": "Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20430.pdf", "abstract_url": "https://arxiv.org/abs/2506.20430", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "DeepRare是首个基于大型语言模型（LLM）的罕见病诊断代理系统，能够处理异质性临床输入，为罕见病生成排名诊断假设，并提供透明的推理链。", "motivation": "罕见病在全球范围内影响超过3亿人，但由于其临床异质性、个体发病率低以及大多数临床医生对罕见病的了解有限，及时准确的诊断仍然是一个普遍存在的挑战。", "method": "利用大型语言模型（LLM）构建的代理系统DeepRare，处理异质性临床输入，生成排名诊断假设，并提供透明的推理链。", "result": "DeepRare能够为罕见病生成排名诊断假设，并提供透明的推理链，将中间分析步骤与可验证的医学证据联系起来。", "conclusion": "DeepRare作为一种新型的罕见病诊断工具，通过提供透明的推理链，有望提高罕见病诊断的准确性和及时性，对改善罕见病患者的诊疗具有重要意��。"}}
{"id": "2506.20409", "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging", "authors": ["Ekaterina Taktasheva", "Jeff Dalton"], "abstract": "Recent advancements in tool-augmented large language models have enabled them to interact with external tools, enhancing their ability to perform complex user tasks. However, existing approaches overlook the role of personalisation in guiding tool use. This work investigates how user preferences can be effectively integrated into goal-oriented dialogue agents. Through extensive analysis, we identify key weaknesses in the ability of LLMs to personalise tool use. To this end, we introduce \\name, a novel solution that enhances personalised tool use by leveraging a structured tagging tool and an uncertainty-based tool detector. TAPS significantly improves the ability of LLMs to incorporate user preferences, achieving the new state-of-the-art for open source models on the NLSI task.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20409.pdf", "abstract_url": "https://arxiv.org/abs/2506.20409", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了TAPS，一种通过结构化标记和基于不确定性的工具检测器增强个性化工具使用的新方法，显著提升了大型语言模型在目标导向对话代理中整合用户偏好的能力。", "motivation": "现有工具增强的大型语言模型在个性化工具使用方面的不足，特别是在如何有效整合用户偏好以指导工具使用方面。", "method": "提出TAPS方法，利用结构化标记工具和基于不确定性的工具检测器来增强个性化工具使用。", "result": "TAPS显著提升了大型语言模型在整合用户偏好方面的能力，在NLSI任务上达到了开源模型的新状态-of-the-art。", "conclusion": "TAPS通过结构化标记和不确定性检测有效提升了大型语言模型在个性化工具使用方面的性能，为目标导向对话代理中的用户偏好整合提供了新的解决方案。"}}
{"id": "2506.20476", "title": "Knowledge-Aware Diverse Reranking for Cross-Source Question Answering", "authors": ["Tong Zhou"], "abstract": "This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG competition. The competition's evaluation set, automatically generated by DataMorgana from internet corpora, encompassed a wide range of target topics, question types, question formulations, audience types, and knowledge organization methods. It offered a fair evaluation of retrieving question-relevant supporting documents from a 15M documents subset of the FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline achieved first place in the competition.", "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20476.pdf", "abstract_url": "https://arxiv.org/abs/2506.20476", "categories": ["Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了Team Marikarp为SIGIR 2025 LiveRAG竞赛提出的解决方案，该方案通过知识感知的多样化重排序RAG流程，在竞赛中获得了第一名。", "motivation": "解决从FineWeb语料库的15M文档子集中检索与问题相关的支持文档的挑战，特别是在目标主题、问题类型、问题表述、受众类型和知识组织方法多样化的背景下。", "method": "采用知识感知的多样化重排序RAG流程。", "result": "在SIGIR 2025 LiveRAG竞赛中获得了第一名。", "conclusion": "提出的知识感知多样化重排序RAG流程在多样化的评估集中表现优异，为跨源问答任务提供了一种有效的解决方案。"}}
{"id": "2506.20598", "title": "Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges", "authors": ["Alexander D. Kalian", "Jaewook Lee", "Stefan P. Johannesson", "Lennart Otte", "Christer Hogstrand", "Miao Guo"], "abstract": "The global demand for sustainable protein sources has accelerated the need for intelligent tools that can rapidly process and synthesise domain-specific scientific knowledge. In this study, we present a proof-of-concept multi-agent Artificial Intelligence (AI) framework designed to support sustainable protein production research, with an initial focus on microbial protein sources. Our Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based LLM agents: (1) a literature search agent that retrieves relevant scientific literature on microbial protein production for a specified microbial strain, and (2) an information extraction agent that processes the retrieved content to extract relevant biological and chemical information. Two parallel methodologies, fine-tuning and prompt engineering, were explored for agent optimisation. Both methods demonstrated effectiveness at improving the performance of the information extraction agent in terms of transformer-based cosine similarity scores between obtained and ideal outputs. Mean cosine similarity scores were increased by up to 25%, while universally reaching mean scores of $\\geq 0.89$ against ideal output text. Fine-tuning overall improved the mean scores to a greater extent (consistently of $\\geq 0.94$) compared to prompt engineering, although lower statistical uncertainties were observed with the latter approach. A user interface was developed and published for enabling the use of the multi-agent AI system, alongside preliminary exploration of additional chemical safety-based search capabilities", "subjects": "Artificial Intelligence (cs.AI); Systems and Control (eess.SY)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20598.pdf", "abstract_url": "https://arxiv.org/abs/2506.20598", "categories": ["Artificial Intelligence (cs.AI)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本研究提出了一个多代理人工智能框架，旨在支持可持续蛋白质生产研究，特别是微生物蛋白质来源。通过检索增强生成（RAG）导向的系统，结合微调和提示工程两种方法优化代理性能，提高了信息提取的准确性。", "motivation": "解决全球对可持续蛋白质来源的需求，加速领域特定科学知识的快速处理和合成。", "method": "采用基于GPT的大型语言模型（LLM）代理，包括文献搜索代理和信息提取代理，探索微调和提示工程两种优化方法。", "result": "微调和提示工程均有效提高了信息提取代理的性能，余弦相似度得分平均提高了25%，微调方法在提高平均得分方面表现更优。", "conclusion": "多代理AI系统在支持可持续蛋白质生产研究方面显示出潜力，微调在优化代理性能方面优于提示工程，但提示工程具有更低的统计不确定性。"}}
{"id": "2506.20606", "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm", "authors": ["Baixiang Huang", "Zhen Tan", "Haoran Wang", "Zijie Liu", "Dawei Li", "Ali Payani", "Huan Liu", "Tianlong Chen", "Kai Shu"], "abstract": "Agents based on Large Language Models (LLMs) have demonstrated strong capabilities across a wide range of tasks. However, deploying LLM-based agents in high-stakes domains comes with significant safety and ethical risks. Unethical behavior by these agents can directly result in serious real-world consequences, including physical harm and financial loss. To efficiently steer the ethical behavior of agents, we frame agent behavior steering as a model editing task, which we term Behavior Editing. Model editing is an emerging area of research that enables precise and efficient modifications to LLMs while preserving their overall capabilities. To systematically study and evaluate this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in psychological moral theories. This benchmark supports both the evaluation and editing of agent behaviors across a variety of scenarios, with each tier introducing more complex and ambiguous scenarios. We first demonstrate that Behavior Editing can dynamically steer agents toward the target behavior within specific scenarios. Moreover, Behavior Editing enables not only scenario-specific local adjustments but also more extensive shifts in an agent's global moral alignment. We demonstrate that Behavior Editing can be used to promote ethical and benevolent behavior or, conversely, to induce harmful or malicious behavior. Through comprehensive evaluations on agents based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior Editing across different models and scenarios. Our findings offer key insights into a new paradigm for steering agent behavior, highlighting both the promise and perils of Behavior Editing.", "subjects": "Computation and Language (cs.CL)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2506.20606.pdf", "abstract_url": "https://arxiv.org/abs/2506.20606", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了基于大型语言模型（LLM）的代理在高风险领域部署时的安全和伦理风险，提出了一种称为行为编辑的模型编辑任务，以高效引导代理的伦理行为。通过引入BehaviorBench基准，系统地评估和编辑代理行为，研究表明行为编辑不仅能动态引导特定场景中的目标行为，还能改变代理的全局道德倾向。", "motivation": "部署基于LLM的代理在高风险领域时，存在显著的安全和伦理风险，可能导致严重的现实后果，如人身伤害和财务损失。", "method": "将代理行为引导框架化为模型编辑任务，引入BehaviorBench基准，支持多种场景下的行为评估和编辑。", "result": "行为编辑能够动态引导代理在特定场景中的目标行为，并实现代理全局道德倾向的转变，既能促进伦理和仁慈行为，也能诱导有害或恶意行为。", "conclusion": "行为编辑为引导代理行为提供了新范式，既展示了其潜力，也揭示了其潜在危险。"}}
{"id": "2506.20640", "title": "Towards Community-Driven Agents for Machine Learning Engineering", "authors": ["Sijie Li", "Weiwei Sun", "Shanda Li", "Ameet Talwalkar", "Yiming Yang"], "abstract": "Large language model-based machine learning (ML) agents have shown great promise in automating ML research. However, existing agents typically operate in isolation on a given research problem, without engaging with the broader research community, where human researchers often gain insights and contribute by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live evaluation framework designed to assess an agent's ability to communicate with and leverage collective knowledge from a simulated Kaggle research community. Building on this framework, we propose CoMind, a novel agent that excels at exchanging insights and developing novel solutions within a community context. CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2% human competitors on average across four ongoing Kaggle competitions. Our code is released at", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20640.pdf", "abstract_url": "https://arxiv.org/abs/2506.20640", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了MLE-Live评估框架和CoMind代理，旨在通过模拟Kaggle研究社区评估和提升机器学习代理的社区互动与知识共享能力。CoMind在MLE-Live上表现出色，并在四个Kaggle竞赛中平均超越79.2%的人类竞争对手。", "motivation": "解决现有基于大型语言模型的机器学习代理在研究问题时孤立操作，缺乏与更广泛研究社区互动和知识共享的问题。", "method": "引入MLE-Live评估框架，并在此基础上提出CoMind代理，该代理能够在社区环境中交换见解和开发新解决方案。", "result": "CoMind在MLE-Live上实现了最先进的性能，并在四个进行中的Kaggle竞赛中平均超越79.2%的人类竞争对手。", "conclusion": "CoMind通过有效利用社区知识共享，展示了在机器学习研究中的潜力，为未来的社区驱动型机器学习代理开发提供了方向。"}}
{"id": "2506.20608", "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base", "authors": ["Barry Smith", "Junchao Zhang", "Hong Zhang", "Lois Curfman McInnes", "Murat Keceli", "Archit Vasan", "Satish Balay", "Toby Isaac", "Le Chen", "Venkatram Vishwanath"], "abstract": "Generative AI, especially through large language models (LLMs), is transforming how technical knowledge can be accessed, reused, and extended. PETSc, a widely used numerical library for high-performance scientific computing, has accumulated a rich but fragmented knowledge base over its three decades of development, spanning source code, documentation, mailing lists, GitLab issues, Discord conversations, technical papers, and more. Much of this knowledge remains informal and inaccessible to users and new developers. To activate and utilize this knowledge base more effectively, the PETSc team has begun building an LLM-powered system that combines PETSc content with custom LLM tools -- including retrieval-augmented generation (RAG), reranking algorithms, and chatbots -- to assist users, support developers, and propose updates to formal documentation. This paper presents initial experiences designing and evaluating these tools, focusing on system architecture, using RAG and reranking for PETSc-specific information, evaluation methodologies for various LLMs and embedding models, and user interface design. Leveraging the Argonne Leadership Computing Facility resources, we analyze how LLM responses can enhance the development and use of numerical software, with an initial focus on scalable Krylov solvers. Our goal is to establish an extensible framework for knowledge-centered AI in scientific software, enabling scalable support, enriched documentation, and enhanced workflows for research and development. We conclude by outlining directions for expanding this system into a robust, evolving platform that advances software ecosystems to accelerate scientific discovery.", "subjects": "Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20608.pdf", "abstract_url": "https://arxiv.org/abs/2506.20608", "categories": ["Artificial Intelligence (cs.AI)", "Numerical Analysis (math.NA)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了如何利用生成式AI，特别是大型语言模型（LLMs），来激活和利用PETSc（一个广泛用于高性能科学计算的数值库）的分散知识库。通过结合PETSc内容和自定义LLM工具，如检索增强生成（RAG）、重新排名算法和聊天机器人，旨在帮助用户、支持开发者并提出对正式文档的更新。", "motivation": "PETSc在其三十年的发展过程中积累了丰富但分散的知识库，包括源代码、文档、邮件列表、GitLab问题、Discord对话、技术论文等。这些知识大多是非正式的，对用户和新开发者难以访问。本文旨在解决如何更有效地激活和利用这一知识库的问题。", "method": "采用LLM驱动的系统，结合PETSc内容和自定义LLM工具，包括检索增强生成（RAG）、重新排名算法和聊天机器人，来辅助用户和开发者，并提出对正式文档的更新。", "result": "初步经验和评估表明，这些工具可以有效地增强数值软件的开发和使用的知识访问和利用，特别是在可扩展的Krylov求解器方面。", "conclusion": "目标是建立一个可扩展的知识中心AI框架，以支持科学软件的研究和开发，包括可扩展的支持、丰富的文档和增强的工作流程。未来方向包括将这一系统扩展为一个强大的、不断发展的平台，以加速科学发现。"}}
{"id": "2506.20664", "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind", "authors": ["Andrei Lupu", "Timon Willi", "Jakob Foerster"], "abstract": "As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the \"mental\" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Multiagent Systems (cs.MA)", "comments": "41 pages, 19 figures", "pdf_url": "https://arxiv.org/pdf/2506.20664.pdf", "abstract_url": "https://arxiv.org/abs/2506.20664", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Human-Computer Interaction (cs.HC)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了Decrypto基准测试，用于评估大型语言模型在多智能体推理和心理理论（ToM）方面的能力，旨在解决现有基准测试的局限性。", "motivation": "随着大型语言模型（LLMs）获得代理能力，它们需要在复杂的多智能体场景中导航，与人类用户和其他智能体在合作和竞争环境中互动。这需要新的推理技能，尤其是心理理论（ToM），即推理其他智能体“心理”状态的能力。然而，LLMs中的ToM和其他多智能体能力目前理解不足，因为现有基准测试存在范围狭窄、数据泄漏、饱和和缺乏交互性等问题。", "method": "本文提出了Decrypto，一个基于游戏的基准测试，用于多智能体推理和ToM，灵感来自认知科学、计算语用学和多智能体强化学习。它设计得尽可能简单，消除了其他基准测试中常见的混杂因素。", "result": "Decrypto是第一个设计用于交互式ToM实验的平台，旨在提供一个更全面、更准确的评估工具。", "conclusion": "Decrypto基准测试的提出，为评估和提升LLMs在多智能体推理和ToM方面的能力提供了一个新的、更有效的工具，有助于推动这一领域的研究和发展。"}}
{"id": "2506.20642", "title": "Memento: Note-Taking for Your Future Self", "authors": ["Chao Wan", "Albert Gong", "Mihir Mishra", "Carl-Leander Henneking", "Claas Beger", "Kilian Q. Weinberger"], "abstract": "Large language models (LLMs) excel at reasoning-only tasks, but struggle when reasoning must be tightly coupled with retrieval, as in multi-hop question answering. To overcome these limitations, we introduce a prompting strategy that first decomposes a complex question into smaller steps, then dynamically constructs a database of facts using LLMs, and finally pieces these facts together to solve the question. We show how this three-stage strategy, which we call Memento, can boost the performance of existing prompting strategies across diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the performance of chain-of-thought (CoT) when all information is provided in context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1 percentage points, demonstrating its utility in agentic settings.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20642.pdf", "abstract_url": "https://arxiv.org/abs/2506.20642", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic", "@RAG"], "AI": {"tldr": "本文介绍了Memento，一种针对大型语言模型（LLMs）在需要紧密耦合检索与推理的任务中表现不佳的问题提出的提示策略。Memento通过分解复杂问题、动态构建事实数据库并整合这些事实来解决问题，显著提升了现有提示策略的性能。", "motivation": "大型语言模型在纯推理任务中表现优异，但在需要紧密耦合检索与推理的多跳问答等任务中表现不佳。本文旨在解决这一问题。", "method": "提出了一种三阶段的提示策略Memento，包括分解复杂问题、动态构建事实数据库和整合事实以解决问题。", "result": "在PhantomWiki基准测试中，Memento将思维链（CoT）的性能提高了一倍；在2WikiMultiHopQA的开放域版本中，Memento结合CoT-RAG比普通CoT-RAG提高了超过20个F1百分点，比多跳RAG基线IRCoT提高了超过13个F1百分点；在MuSiQue数据集上，Memento比ReAct提高了超过3个F1百分点。", "conclusion": "Memento作为一种有效的提示策略，能够显著提升大型语言模型在需要紧密耦合检索与推理的任务中的性能，展示了其在多种设置下的实用性和有效性。"}}
{"id": "2506.20097", "title": "PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models", "authors": ["Wang Bill Zhu", "Miaosen Chai", "Ishika Singh", "Robin Jia", "Jesse Thomason"], "abstract": "We propose PSALM-V, the first autonomous neuro-symbolic learning system able to induce symbolic action semantics (i.e., pre- and post-conditions) in visual environments through interaction. PSALM-V bootstraps reliable symbolic planning without expert action definitions, using LLMs to generate heuristic plans and candidate symbolic semantics. Previous work has explored using large language models to generate action semantics for Planning Domain Definition Language (PDDL)-based symbolic planners. However, these approaches have primarily focused on text-based domains or relied on unrealistic assumptions, such as access to a predefined problem file, full observability, or explicit error messages. By contrast, PSALM-V dynamically infers PDDL problem files and domain action semantics by analyzing execution outcomes and synthesizing possible error explanations. The system iteratively generates and executes plans while maintaining a tree-structured belief over possible action semantics for each action, iteratively refining these beliefs until a goal state is reached. Simulated experiments of task completion in ALFRED demonstrate that PSALM-V increases the plan success rate from 37% (Claude-3.7) to 74% in partially observed setups. Results on two 2D game environments, RTFM and Overcooked-AI, show that PSALM-V improves step efficiency and succeeds in domain induction in multi-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions for real-world robot BlocksWorld tasks, despite low-level manipulation failures from the robot.", "subjects": "Robotics (cs.RO); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20097.pdf", "abstract_url": "https://arxiv.org/abs/2506.20097", "categories": ["Robotics (cs.RO)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "PSALM-V是首个能够在交互式视觉环境中通过交互诱导符号动作语义（即前条件和后条件）的自主神经符号学习系统。它利用大型语言模型（LLMs）生成启发式计划和候选符号语义，无需专家动作定义即可启动可靠的符号规划。", "motivation": "解决在视觉环境中自动化符号规划的问题，特别是在没有专家定义动作、部分可观察或多代理设置的情况下。", "method": "PSALM-V通过分析执行结果和合成可能的错误解释，动态推断PDDL问题文件和领域动作语义。系统迭代生成和执行计划，同时为每个动作维护一个树状结构的信念，直到达到目标状态。", "result": "在ALFRED的任务完成模拟实验中，PSALM-V将计划成功率从37%（Claude-3.7）提高到74%。在RTFM和Overcooked-AI两个2D游戏环境中，PSALM-V提高了步骤效率并在多代理设置中成功诱导了领域。", "conclusion": "PSALM-V能够在部分可观察和多代理设置中有效诱导符号动作语义，提高计划成功率和步骤效率，即使在现实世界的机器人BlocksWorld任务中也能正确诱导PDDL前条件和后条件。"}}
{"id": "2506.19997", "title": "TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design", "authors": ["Geonwoo Cho", "Jaegyun Im", "Jihwan Lee", "Hojun Yi", "Sejin Kim", "Sundong Kim"], "abstract": "Generalizing deep reinforcement learning agents to unseen environments remains a significant challenge. One promising solution is Unsupervised Environment Design (UED), a co-evolutionary framework in which a teacher adaptively generates tasks with high learning potential, while a student learns a robust policy from this evolving curriculum. Existing UED methods typically measure learning potential via regret, the gap between optimal and current performance, approximated solely by value-function loss. Building on these approaches, we introduce the transition prediction error as an additional term in our regret approximation. To capture how training on one task affects performance on others, we further propose a lightweight metric called co-learnability. By combining these two measures, we present Transition-aware Regret Approximation with Co-learnability for Environment Design (TRACED). Empirical evaluations show that TRACED yields curricula that improve zero-shot generalization across multiple benchmarks while requiring up to 2x fewer environment interactions than strong baselines. Ablation studies confirm that the transition prediction error drives rapid complexity ramp-up and that co-learnability delivers additional gains when paired with the transition prediction error. These results demonstrate how refined regret approximation and explicit modeling of task relationships can be leveraged for sample-efficient curriculum design in UED.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.19997.pdf", "abstract_url": "https://arxiv.org/abs/2506.19997", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了TRACED方法，一种通过结合转移预测误差和共学习能力来改进无监督环境设计（UED）中课程生成的方法，以提高深度强化学习代理在未见环境中的泛化能力。", "motivation": "解决深度强化学习代理在未见环境中泛化能力不足的问题，特别是在无监督环境设计（UED）框架下，如何更有效地生成有助于学习的课程。", "method": "提出TRACED方法，结合转移预测误差和共学习能力来近似遗憾，以生成更有助于学习的课程。转移预测误差用于捕捉任务间的性能影响，共学习能力则是一种轻量级度量，用于衡量任务间的关系。", "result": "实证评估显示，TRACED在多个基准测试中提高了零样本泛化能力，同时比强基线方法减少了多达2倍的环境交互次数。消融研究证实，转移预测误差推动了复杂度的快速提升，而共学习能力在与转移预测误差结合时带来了额外的增益。", "conclusion": "通过精细化的遗憾近似和明确建模任务关系，可以在UED中实现样本高效的课程设计，这对于提高深度强化学习代理的泛化能力具有重要意义。"}}
{"id": "2506.20062", "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents", "authors": ["Runlong Ye", "Zeling Zhang", "Boushra Almazroua", "Michael Liut"], "abstract": "AI-powered code assistants are widely used to generate code completions, significantly boosting developer productivity. However, these tools typically present suggestions without explaining their rationale, leaving their decision-making process inscrutable. This opacity hinders developers' ability to critically evaluate the output, form accurate mental models, and build calibrated trust in the system. To address this, we introduce CopilotLens, a novel interactive framework that reframes code completion from a simple suggestion into a transparent, explainable event. CopilotLens operates as an explanation layer that reveals the AI agent's \"thought process\" through a dynamic two-level interface, surfacing everything from its reconstructed high-level plans to the specific codebase context influencing the code. This paper presents the design and rationale of CopilotLens, offering a concrete framework for building future agentic code assistants that prioritize clarity of reasoning over speed of suggestion, thereby fostering deeper comprehension and more robust human-AI collaboration.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20062.pdf", "abstract_url": "https://arxiv.org/abs/2506.20062", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了CopilotLens，一个旨在提高AI编码助手透明度和可解释性的新型交互框架。", "motivation": "当前的AI代码助手在提供代码补全建议时，通常不解释其决策过程，这使得开发者难以评估输出、形成准确的心理模型并建立对系统的信任。", "method": "CopilotLens作为一个解释层，通过动态的两级界面揭示AI代理的'思考过程'，从重建的高级计划到影响代码的具体代码库上下文。", "result": "CopilotLens提供了一个具体的框架，用于构建未来的代理代码助手，这些助手优先考虑推理的清晰性而非建议的速度。", "conclusion": "CopilotLens通过提高透明度和可解释性，促进了更深层次的理解和更强大的人机协作。"}}
{"id": "2506.20039", "title": "Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning", "authors": ["Koorosh Moslemi", "Chi-Guhn Lee"], "abstract": "Team formation and the dynamics of team-based learning have drawn significant interest in the context of Multi-Agent Reinforcement Learning (MARL). However, existing studies primarily focus on unilateral groupings, predefined teams, or fixed-population settings, leaving the effects of algorithmic bilateral grouping choices in dynamic populations underexplored. To address this gap, we introduce a framework for learning two-sided team formation in dynamic multi-agent systems. Through this study, we gain insight into what algorithmic properties in bilateral team formation influence policy performance and generalization. We validate our approach using widely adopted multi-agent scenarios, demonstrating competitive performance and improved generalization in most scenarios.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)", "comments": "Accepted to the 2nd Coordination and Cooperation in Multi-Agent Reinforcement Learning (CoCoMARL) Workshop at RLC 2025", "pdf_url": "https://arxiv.org/pdf/2506.20039.pdf", "abstract_url": "https://arxiv.org/abs/2506.20039", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Computer Science and Game Theory (cs.GT)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种在动态多智能体系统中学习双边团队形成的框架，探讨了双边团队形成中算法特性对策略性能和泛化能力的影响，并在多种多智能体场景中验证了其竞争性能和更好的泛化能力。", "motivation": "现有的多智能体强化学习（MARL）研究主要关注单边分组、预定义团队或固定人口设置，而动态人口中算法双边分组选择的影响尚未充分探索。", "method": "引入了一个框架，用于在动态多智能体系统中学习双边团队形成。", "result": "在广泛采用的多智能体场景中验证了该方法，展示了在大多数场景中的竞争性能和改进的泛化能力。", "conclusion": "研究为双边团队形成中的算法特性如何影响策略性能和泛化能力提供了见解，为动态多智能体系统中的团队学习提供了新的研究方向。"}}
{"id": "2506.20031", "title": "Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning", "authors": ["Prithvi Poddar", "Ehsan Tarkesh Esfahani", "Karthik Dantu", "Souma Chowdhury"], "abstract": "Operations in disaster response, search \\& rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process. This paper presents a new theoretical formulation and computational framework to generate such diverse pools of COAs for operations with soft variations in agent-task compatibility. Key to the problem formulation is a graph abstraction of the task space and the pool of COAs itself to quantify its diversity. Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features. Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20031.pdf", "abstract_url": "https://arxiv.org/abs/2506.20031", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新的理论框架和计算方法，用于为多智能体操作生成多样化的行动方案（COA）池，特别是在灾害响应、搜索救援和军事任务中。通过图抽象和遗传算法，以及图神经网络，实现了在考虑智能体任务兼容性的同时，最大化COA池的多样性。", "motivation": "解决在多智能体操作中，如何自动化生成多样化的行动方案（COA）以应对环境变化和智能体能力差异的挑战。", "method": "使用图抽象化任务空间和COA池来量化多样性，采用遗传算法进行任务分配以最大化多样性和兼容性，并利用图神经网络进行单智能体任务排序。", "result": "在模拟环境中测试表明，与随机行走基线相比，COA生成过程显著提高了性能，任务排序的最优性差距小，且能在约50分钟内为5个智能体/100个任务的操作规划多达20个COA。", "conclusion": "提出的框架能够有效生成多样化的COA池，适应智能体能力差异和环境变化，为多智能体操作提供了实用的自动化规划工具。"}}
{"id": "2506.20415", "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models", "authors": ["Dipayan Saha", "Shams Tarek", "Hasan Al Shaikh", "Khan Thamid Hasan", "Pavan Sai Nalluri", "Md. Ajoad Hasan", "Nashmin Alam", "Jingbo Zhou", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "abstract": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical imperative, yet traditional verification techniques struggle to keep pace due to significant challenges in automation, scalability, comprehensiveness, and adaptability. The advent of large language models (LLMs), with their remarkable capabilities in natural language understanding, code generation, and advanced reasoning, presents a new paradigm for tackling these issues. Moving beyond monolithic models, an agentic approach allows for the creation of multi-agent systems where specialized LLMs collaborate to solve complex problems more effectively. Recognizing this opportunity, we introduce SV-LLM, a novel multi-agent assistant system designed to automate and enhance SoC security verification. By integrating specialized agents for tasks like verification question answering, security asset identification, threat modeling, test plan and property generation, vulnerability detection, and simulation-based bug validation, SV-LLM streamlines the workflow. To optimize their performance in these diverse tasks, agents leverage different learning paradigms, such as in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The system aims to reduce manual intervention, improve accuracy, and accelerate security analysis, supporting proactive identification and mitigation of risks early in the design cycle. We demonstrate its potential to transform hardware security practices through illustrative case studies and experiments that showcase its applicability and efficacy.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.20415.pdf", "abstract_url": "https://arxiv.org/abs/2506.20415", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic", "@RAG"], "AI": {"tldr": "SV-LLM是一种利用大型语言模型（LLMs）的多代理系统，旨在自动化和增强系统级芯片（SoC）的安全验证。通过集成专门代理执行不同任务，如验证问答、安全资产识别等，该系统旨在减少人工干预，提高准确性，并加速安全分析。", "motivation": "传统的SoC安全验证技术在自动化、可扩展性、全面性和适应性方面面临重大挑战，难以满足复杂SoC设计的安全需求。", "method": "采用基于大型语言模型（LLMs）的多代理系统，集成专门代理执行各种安全验证任务，并利用不同的学习范式（如上下文学习、微调和检索增强生成）优化代理性能。", "result": "通过案例研究和实验展示了SV-LLM在硬件安全实践中的潜在转变能力，证明了其适用性和有效性。", "conclusion": "SV-LLM通过其多代理系统和大型语言模型的应用，为SoC安全验证提供了一种创新的解决方案，有望在早期设计周期中主动识别和缓解风险，从而改变硬件安全实践。"}}
