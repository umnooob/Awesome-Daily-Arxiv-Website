{"id": "2505.05512", "title": "Occupancy World Model for Robots", "authors": ["Zhang Zhang", "Qiang Zhang", "Wei Cui", "Shuai Shi", "Yijie Guo", "Gang Han", "Wen Zhao", "Jingkai Sun", "Jiahang Cao", "Jiaxu Wang", "Hao Cheng", "Xiaozhu Ju", "Zhengping Che", "Renjing Xu", "Jian Tang"], "abstract": "Understanding and forecasting the scene evolutions deeply affect the exploration and decision of embodied agents. While traditional methods simulate scene evolutions through trajectory prediction of potential instances, current works use the occupancy world model as a generative framework for describing fine-grained overall scene dynamics. However, existing methods cluster on the outdoor structured road scenes, while ignoring the exploration of forecasting 3D occupancy scene evolutions for robots in indoor scenes. In this work, we explore a new framework for learning the scene evolutions of observed fine-grained occupancy and propose an occupancy world model based on the combined spatio-temporal receptive field and guided autoregressive transformer to forecast the scene evolutions, called RoboOccWorld. We propose the Conditional Causal State Attention (CCSA), which utilizes camera poses of next state as conditions to guide the autoregressive transformer to adapt and understand the indoor robotics scenarios. In order to effectively exploit the spatio-temporal cues from historical observations, Hybrid Spatio-Temporal Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive field based on multi-scale spatio-temporal windows. In addition, we restructure the OccWorld-ScanNet benchmark based on local annotations to facilitate the evaluation of the indoor 3D occupancy scene evolution prediction task. Experimental results demonstrate that our RoboOccWorld outperforms state-of-the-art methods in indoor 3D occupancy scene evolution prediction task. The code will be released soon.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05512.pdf", "abstract_url": "https://arxiv.org/abs/2505.05512", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为RoboOccWorld的新框架，用于学习观察到的细粒度占用场景的演变，并提出了一种基于时空感受野和引导自回归变换器的占用世界模型，以预测场景演变。", "motivation": "当前工作主要集中在室外结构化道路场景，而忽略了在室内场景中预测3D占用场景演变的探索。本文旨在解决这一问题。", "method": "提出了条件因果状态注意力（CCSA）和混合时空聚合（HSTA）方法，结合多尺度时空窗口，利用历史观察中的时空线索，预测室内3D占用场景的演变。", "result": "实验结果表明，RoboOccWorld在室内3D占用场景演变预测任务中优于现有最先进的方法。", "conclusion": "本文提出的RoboOccWorld框架和CCSA、HSTA方法在预测室内3D占用场景演变方面表现出色，为机器人理解和预测室内场景动态提供了新的解决方案。"}}
{"id": "2505.05495", "title": "Learning 3D Persistent Embodied World Models", "authors": ["Siyuan Zhou", "Yilun Du", "Yuncong Yang", "Lei Han", "Peihao Chen", "Dit-Yan Yeung", "Chuang Gan"], "abstract": "The ability to simulate the effects of future actions on the world is a crucial ability of intelligent embodied agents, enabling agents to anticipate the effects of their actions and make plans accordingly. While a large body of existing work has explored how to construct such world models using video models, they are often myopic in nature, without any memory of a scene not captured by currently observed images, preventing agents from making consistent long-horizon plans in complex environments where many parts of the scene are partially observed. We introduce a new persistent embodied world model with an explicit memory of previously generated content, enabling much more consistent long-horizon simulation. During generation time, our video diffusion model predicts RGB-D video of the future observations of the agent. This generation is then aggregated into a persistent 3D map of the environment. By conditioning the video model on this 3D spatial map, we illustrate how this enables video world models to faithfully simulate both seen and unseen parts of the world. Finally, we illustrate the efficacy of such a world model in downstream embodied applications, enabling effective planning and policy learning.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05495.pdf", "abstract_url": "https://arxiv.org/abs/2505.05495", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种新的持久性体现世界模型，该模型通过显式记忆先前生成的内容，支持更一致的长时程模拟，用于智能体现代理的未来行动效果预测和规划。", "motivation": "解决现有视频模型构建的世界模型在复杂环境中因缺乏对未被当前观察图像捕获的场景记忆而无法进行一致长时程规划的问题。", "method": "采用视频扩散模型预测代理未来观察的RGB-D视频，并将其聚合到环境的持久3D地图中，通过条件化视频模型于这一3D空间地图，实现世界模型对所见和未见世界部分的忠实模拟。", "result": "展示了该世界模型在下游体现应用中的有效性，支持有效的规划和策略学习。", "conclusion": "通过引入持久性体现世界模型和3D空间记忆，本文提出的方法能够更一致地模拟未来行动的效果，为智能体现代理的长时程规划提供了新的可能性。"}}
{"id": "2505.05635", "title": "VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models", "authors": ["Faizan Farooq Khan", "Jun Chen", "Youssef Mohamed", "Chun-Mei Feng", "Mohamed Elhoseiny"], "abstract": "Open-vocabulary recognition remains a challenging problem in computer vision, as it requires identifying objects from an unbounded set of categories. This is particularly relevant in nature, where new species are discovered every year. In this work, we focus on open-vocabulary bird species recognition, where the goal is to classify species based on their descriptions without being constrained to a predefined set of taxonomic categories. Traditional benchmarks like CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary paradigm, limiting their applicability to real-world scenarios where novel species continually emerge. We show that the performance of current systems when evaluated under settings closely aligned with open-vocabulary drops by a huge margin. To address this gap, we propose a scalable framework integrating structured textual knowledge from Wikipedia articles of 11,202 bird species distilled via GPT-4o into concise, discriminative summaries. We propose Visual Re-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented generation framework that uses visual similarities to rerank the top m candidates retrieved by a set of multimodal vision language encoders. This allows for the recognition of unseen taxa. Extensive experiments across five established classification benchmarks show that our approach is highly effective. By integrating VR-RAG, we improve the average performance of state-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five benchmarks. Our approach outperforms conventional VLM-based approaches, which struggle with unseen species. By bridging the gap between encyclopedic knowledge and visual recognition, our work advances open-vocabulary recognition, offering a flexible, scalable solution for biodiversity monitoring and ecological research.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "7 figures", "pdf_url": "https://arxiv.org/pdf/2505.05635.pdf", "abstract_url": "https://arxiv.org/abs/2505.05635", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种名为VR-RAG的新框架，用于解决开放词汇鸟类物种识别问题，通过结合视觉相似性重新排序和检索增强生成技术，显著提高了现有大型多模态模型在开放词汇设置下的性能。", "motivation": "开放词汇识别在计算机视觉中是一个具有挑战性的问题，尤其是在自然界中，新物种不断被发现。现有的封闭词汇评估范式限制了其在现实世界中的应用，特别是在需要识别新出现物种的场景中。", "method": "提出了一种可扩展的框架，整合了来自11,202种鸟类维基百科文章的结构化文本知识，并通过GPT-4o提炼出简洁、有区分度的摘要。引入了视觉重新排序检索增强生成（VR-RAG）框架，利用视觉相似性对多模态视觉语言编码器检索到的候选物种进行重新排序。", "result": "在五个已建立的分类基准上的广泛实验表明，该方法非常有效。通过整合VR-RAG，将最先进的大型多模态模型QWEN2.5-VL的平均性能提高了15.4%。", "conclusion": "通过桥接百科全书知识和视觉识别之间的差距，我们的工作推动了开放词汇识别的进步，为生物多样性监测和生态研究提供了一个灵活、可扩展的解决方案。"}}
{"id": "2505.05666", "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval", "authors": ["Alexander Most", "Joseph Winjum", "Ayan Biswas", "Shawn Jones", "Nishath Rajiv Ranasinghe", "Dan O'Malley", "Manish Bhattarai"], "abstract": "Retrieval-Augmented Generation (RAG) has become a popular technique for enhancing the reliability and utility of Large Language Models (LLMs) by grounding responses in external documents. Traditional RAG systems rely on Optical Character Recognition (OCR) to first process scanned documents into text. However, even state-of-the-art OCRs can introduce errors, especially in degraded or complex documents. Recent vision-language approaches, such as ColPali, propose direct visual embedding of documents, eliminating the need for OCR. This study presents a systematic comparison between a vision-based RAG system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2 (90B) and Nougat OCR across varying document qualities. Beyond conventional retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark to assess end-to-end question-answering performance. Our findings indicate that while vision-based RAG performs well on documents it has been fine-tuned on, OCR-based RAG is better able to generalize to unseen documents of varying quality. We highlight the key trade-offs between computational efficiency and semantic accuracy, offering practical guidance for RAG practitioners in selecting between OCR-dependent and vision-based document retrieval systems in production environments.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05666.pdf", "abstract_url": "https://arxiv.org/abs/2505.05666", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文比较了基于视觉的RAG系统（ColPali）与传统OCR基础的RAG系统在不同文档质量下的表现，发现视觉RAG在特定文档上表现良好，而OCR RAG在未见过文档上更具泛化能力。", "motivation": "解决OCR在处理退化或复杂文档时可能引入错误的问题，探索视觉语言方法在文档检索中的应用。", "method": "系统比较视觉基础的RAG系统与OCR基础的RAG系统，使用Llama 3.2 (90B)和Nougat OCR，并引入语义答案评估基准。", "result": "视觉RAG在特定文档上表现良好，OCR RAG在未见过文档上更具泛化能力。", "conclusion": "在计算效率和语义准确性之间存在权衡，为RAG实践者在生产环境中选择OCR依赖或视觉基础的文档检索系统提供实用指导。"}}
{"id": "2505.05681", "title": "Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos", "authors": ["Giulio Cesare Mastrocinque Santo", "Patrícia Izar", "Irene Delval", "Victor de Napole Gregolin", "Nina S. T. Hirata"], "abstract": "Video recordings of nonhuman primates in their natural habitat are a common source for studying their behavior in the wild. We fine-tune pre-trained video-text foundational models for the specific domain of capuchin monkeys, with the goal of developing useful computational models to help researchers to retrieve useful clips from videos. We focus on the challenging problem of training a model based solely on raw, unlabeled video footage, using weak audio descriptions sometimes provided by field collaborators. We leverage recent advances in Multimodal Large Language Models (MLLMs) and Vision-Language Models (VLMs) to address the extremely noisy nature of both video and audio content. Specifically, we propose a two-folded approach: an agentic data treatment pipeline and a fine-tuning process. The data processing pipeline automatically extracts clean and semantically aligned video-text pairs from the raw videos, which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model through Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of $167\\%$ for the 16 frames model and an uplift of $114\\%$ for the 8 frame model on our domain data. Moreover, based on $NDCG@K$ results, our model is able to rank well most of the considered behaviors, while the tested raw pre-trained models are not able to rank them at all. The code will be made available upon acceptance.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05681.pdf", "abstract_url": "https://arxiv.org/abs/2505.05681", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文通过微调预训练的视频-文本对比模型，针对卷尾猴的自然栖息地视频，开发了一种计算模型，帮助研究者从未标记的原始视频中检索有用片段。", "motivation": "研究非人灵长类动物在自然栖息地中的行为通常依赖于视频记录，但这些视频往往是未标记的，且包含大量噪声，这给研究者检索有用片段带来了挑战。", "method": "采用了两阶段方法：一是代理数据处理流程，从原始视频中自动提取干净且语义对齐的视频-文本对；二是通过低秩适应（LoRA）微调预训练的Microsoft X-CLIP模型。", "result": "在领域数据上，16帧模型的Hits@5提升了167%，8帧模型提升了114%。根据NDCG@K结果，模型能够很好地排名大多数考虑的行为，而原始预训练模型则完全无法排名。", "conclusion": "通过提出的方法，能够有效提升从原始视频中检索特定行为片段的能力，为灵长类动物行为研究提供了有价值的工具。"}}
{"id": "2505.05583", "title": "KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification", "authors": ["Qianbo Zang", "Christophe Zgrzendek", "Igor Tchappi", "Afshin Khadangi", "Johannes Sedlmeir"], "abstract": "Hierarchical Text Classification (HTC) involves assigning documents to labels organized within a taxonomy. Most previous research on HTC has focused on supervised methods. However, in real-world scenarios, employing supervised HTC can be challenging due to a lack of annotated data. Moreover, HTC often faces issues with large label spaces and long-tail distributions. In this work, we present Knowledge Graphs for zero-shot Hierarchical Text Classification (KG-HTC), which aims to address these challenges of HTC in applications by integrating knowledge graphs with Large Language Models (LLMs) to provide structured semantic context during classification. Our method retrieves relevant subgraphs from knowledge graphs related to the input text using a Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to understand label semantics at various hierarchy levels. We evaluate KG-HTC on three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental results show that KG-HTC significantly outperforms three baselines in the strict zero-shot setting, particularly achieving substantial improvements at deeper levels of the hierarchy. This evaluation demonstrates the effectiveness of incorporating structured knowledge into LLMs to address HTC's challenges in large label spaces and long-tailed label distributions. Our code is available at:", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05583.pdf", "abstract_url": "https://arxiv.org/abs/2505.05583", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "KG-HTC通过将知识图谱与大型语言模型（LLMs）结合，提出了一种有效的零样本层次文本分类方法，解决了层次文本分类中的大标签空间和长尾分布问题。", "motivation": "层次文本分类（HTC）在实际应用中面临标注数据不足、大标签空间和长尾分布等挑战，需要一种无需大量标注数据的有效方法。", "method": "使用检索增强生成（RAG）方法从知识图谱中检索与输入文本相关的子图，将知识图谱集成到LLMs中，以提供结构化的语义上下文。", "result": "在三个开源HTC数据集（WoS、DBpedia和Amazon）上的实验结果表明，KG-HTC在严格的零样本设置下显著优于三个基线，特别是在层次结构的更深层次上实现了显著改进。", "conclusion": "将结构化知识整合到LLMs中，可以有效解决HTC在大标签空间和长尾标签分布中的挑战，KG-HTC方法展示了这一整合的有效性。"}}
{"id": "2505.05736", "title": "Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications", "authors": ["Da Wu", "Zhanliang Wang", "Quan Nguyen", "Zhuoran Xu", "Kai Wang"], "abstract": "The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization.", "subjects": "Quantitative Methods (q-bio.QM); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "comments": "First Draft", "pdf_url": "https://arxiv.org/pdf/2505.05736.pdf", "abstract_url": "https://arxiv.org/abs/2505.05736", "categories": ["Quantitative Methods (q-bio.QM)", "Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了MINT框架，通过偏好优化将单模态大型解码器模型与多模态生物医学数据的领域特定决策模式对齐，以解决高质量多模态生物医学数据稀缺的问题。MINT利用上游多模态机器学习模型将领域特定知识转移到下游仅文本或仅图像的LLMs中，并在罕见遗传疾病预测和组织类型分类等应用中展示了其有效性。", "motivation": "高质量多模态生物医学数据的稀缺限制了预训练大型语言模型（LLMs）在专门生物医学任务中的有效微调能力。", "method": "MINT框架通过偏好优化（特别是使用Odds Ratio Preference Optimization（ORPO）框架）将单模态大型解码器模型与多模态生物医学数据的领域特定决策模式对齐。", "result": "在罕见遗传疾病预测和组织类型分类的应用中，MINT衍生的模型在仅依赖文本或图像输入的情况下，性能超过了使用SFT、RAG或DPO训练的模型，甚至在某些情况下超过了更大规模的模型。", "conclusion": "MINT通过偏好优化提供了一种有效的策略，将单模态LLMs与高质量多模态专业知识对齐，为生物医学领域的LLMs应用开辟了新途径。"}}
{"id": "2505.05949", "title": "NeoQA: Evidence-based Question Answering with Generated News Events", "authors": ["Max Glockner", "Xiang Jiang", "Leonardo F. R. Ribeiro", "Iryna Gurevych", "Markus Dreyer"], "abstract": "Evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs) is challenging because benchmarks can quickly become stale. Questions initially requiring retrieval may become answerable from pretraining knowledge as newer models incorporate more recent information during pretraining, making it difficult to distinguish evidence-based reasoning from recall. We introduce NeoQA (News Events for Out-of-training Question Answering), a benchmark designed to address this issue. To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring that no prior evidence exists in their training data. We propose our dataset as a new platform for evaluating evidence-based question answering, as it requires LLMs to generate responses exclusively from retrieved evidence and only when sufficient evidence is available. NeoQA enables controlled evaluation across various evidence scenarios, including cases with missing or misleading details. Our findings indicate that LLMs struggle to distinguish subtle mismatches between questions and evidence, and suffer from short-cut reasoning when key information required to answer a question is missing from the evidence, underscoring key limitations in evidence-based reasoning.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05949.pdf", "abstract_url": "https://arxiv.org/abs/2505.05949", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了NeoQA，一个旨在评估大型语言模型（LLMs）在基于证据的问答任务中表现的基准测试。通过生成虚构的新闻事件和实体时间线、知识库及问答对，NeoQA确保LLMs无法利用预训练知识，从而专门评估其从检索证据中生成回答的能力。研究发现，LLMs在区分问题与证据间微妙不匹配及关键信息缺失时存在困难。", "motivation": "评估大型语言模型（LLMs）在检索增强生成（RAG）中的表现面临挑战，因为随着时间的推移，基准测试可能变得过时，难以区分模型是基于证据推理还是依赖预训练知识的回忆。", "method": "通过生成虚构的新闻事件和实体的时间线、知识库、新闻文章及问答对，构建NeoQA基准测试，确保LLMs无法利用预训练知识，必须基于检索到的证据生成回答。", "result": "研究发现，LLMs在区分问题与证据间微妙不匹配及关键信息缺失时表现不佳，显示出在基于证据推理方面的关键限制。", "conclusion": "NeoQA为评估LLMs在基于证据的问答任务中的表现提供了一个新平台，揭示了LLMs在证据推理方面的局限性，为未来研究指明了方向。"}}
{"id": "2505.05863", "title": "Evolutionary ecology of words", "authors": ["Reiji Suzuki", "Takaya Arita"], "abstract": "We propose a model for the evolutionary ecology of words as one attempt to extend evolutionary game theory and agent-based models by utilizing the rich linguistic expressions of Large Language Models (LLMs). Our model enables the emergence and evolution of diverse and infinite options for interactions among agents. Within the population, each agent possesses a short word (or phrase) generated by an LLM and moves within a spatial environment. When agents become adjacent, the outcome of their interaction is determined by the LLM based on the relationship between their words, with the loser's word being replaced by the winner's. Word mutations, also based on LLM outputs, may occur. We conducted preliminary experiments assuming that ``strong animal species\" would survive. The results showed that from an initial population consisting of well-known species, many species emerged both gradually and in a punctuated equilibrium manner. Each trial demonstrated the unique evolution of diverse populations, with one type of large species becoming dominant, such as terrestrial animals, marine life, or extinct species, which were ecologically specialized and adapted ones across diverse extreme habitats. We also conducted a long-term experiment with a large population, demonstrating the emergence and coexistence of diverse species.", "subjects": "Populations and Evolution (q-bio.PE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": "8 pages, 5 figures. Preprint of the paper published in Proceedings of 2025 IEEE Symposium on Computational Intelligence in Artificial Life and Cooperative Intelligent Systems (ALIFE-CIS)", "pdf_url": "https://arxiv.org/pdf/2505.05863.pdf", "abstract_url": "https://arxiv.org/abs/2505.05863", "categories": ["Populations and Evolution (q-bio.PE)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于大型语言模型（LLMs）的词汇进化生态模型，旨在扩展进化博弈论和基于代理的模型。通过模拟代理在空间环境中的移动和互动，模型展示了词汇的多样化和无限进化可能。初步实验假设“强动物物种”会存活，结果显示从已知物种出发，逐渐和间断平衡地出现了许多新物种。长期实验还展示了多样物种的出现和共存。", "motivation": "解决如何利用大型语言模型（LLMs）丰富语言表达，扩展进化博弈论和基于代理的模型，以模拟词汇的进化生态。", "method": "构建一个模型，其中每个代理拥有由LLM生成的短词或短语，并在空间环境中移动。代理相邻时，LLM根据词汇关系决定互动结果，败者的词汇被胜者替换，并可能发生基于LLM输出的词汇突变。", "result": "初步实验显示，从已知物种出发，逐渐和间断平衡地出现了许多新物种。长期实验展示了多样物种的出现和共存，特定类型的大型物种（如陆地动物、海洋生物或灭绝物种）成为主导。", "conclusion": "该模型成功模拟了词汇的进化生态，展示了多样化和无限进化可能，为理解语言和文化的进化提供了新视角。"}}
{"id": "2505.06191", "title": "Neuro-Symbolic Concepts", "authors": ["Jiayuan Mao", "Joshua B. Tenenbaum", "Jiajun Wu"], "abstract": "This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)", "comments": "To appear in Communications of the ACM", "pdf_url": "https://arxiv.org/pdf/2505.06191.pdf", "abstract_url": "https://arxiv.org/abs/2505.06191", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种以概念为中心的范式，用于构建能够持续学习和灵活推理的智能体。这种智能体利用神经符号概念的词汇表，这些概念基于感官输入和驱动输出，具有组合性，能够通过结构组合创造新概念。", "motivation": "解决智能体在不同领域和任务中高效学习、组合概念以及实现数据效率、组合泛化、持续学习和零样本迁移的问题。", "method": "使用结合符号程序和神经网络表示的神经符号概念，这些概念被类型化，支持学习和推理。", "result": "智能体能够在2D图像、视频、3D场景和机器人操作任务等多个领域高效学习和重组概念，解决各种任务。", "conclusion": "概念为中心的框架提供了数据效率、组合泛化、持续学习和零样本迁移等多重优势，为构建更灵活、高效的智能体提供了新途径。"}}
{"id": "2505.06020", "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding", "authors": ["Shuai Wang", "Ivona Najdenkoska", "Hongyi Zhu", "Stevan Rudinac", "Monika Kackovic", "Nachoem Wijnberg", "Marcel Worring"], "abstract": "Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.", "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.06020.pdf", "abstract_url": "https://arxiv.org/abs/2505.06020", "categories": ["Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "ArtRAG是一个新颖的、无需训练的框架，结合了结构化知识和检索增强生成（RAG），用于多视角的艺术作品解释。它通过自动构建艺术上下文知识图（ACKG）来组织领域特定的文本源中的实体，如艺术家、运动、主题和历史事件，进而在推理时选择相关子图来指导生成，从而生成文化丰富、上下文基础的艺术描述。", "motivation": "理解视觉艺术需要跨越文化、历史和风格等多重视角的推理，而不仅仅是物体识别。尽管多模态大型语言模型（MLLMs）在一般图像描述上表现良好，但它们往往无法捕捉到精细艺术所需的细致解释。", "method": "ArtRAG框架自动从领域特定的文本源构建艺术上下文知识图（ACKG），组织实体如艺术家、运动、主题和历史事件成一个丰富的、可解释的图。在推理时，一个多粒度的结构化检索器选择语义和拓扑相关的子图来指导生成。", "result": "在SemArt和Artpedia数据集上的实验显示，ArtRAG优于几个经过大量训练的基线。人类评估进一步证实，ArtRAG生成了连贯、有洞察力且文化丰富的解释。", "conclusion": "ArtRAG通过结合结构化知识和检索增强生成，能够生成上下文基础、文化丰富的艺术描述，优于现有的方法，为视觉艺术的理解提供了新的视角。"}}
{"id": "2505.05602", "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics", "authors": ["Lennart Luettgau", "Harry Coppock", "Magda Dubois", "Christopher Summerfield", "Cozmin Ududec"], "abstract": "As Large Language Models (LLMs) and other AI systems evolve, robustly estimating their capabilities from inherently stochastic outputs while systematically quantifying uncertainty in these estimates becomes increasingly important. Further, advanced AI evaluations often have a nested hierarchical structure, exhibit high levels of complexity, and come with high costs in testing the most advanced AI systems. To address these challenges, we introduce HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI Evaluation Statistics. HiBayES supports robust inferences in classical question-answer benchmarks and advanced agentic evaluations, particularly in low-data scenarios (e.g., < 20 data points per evaluation). Built on Generalized Linear Models (GLMs), Bayesian data analysis, and formal model comparison, HiBayES provides principled uncertainty quantification and robust parameter estimation. This paper offers a comprehensive introduction to HiBayES, including illustrative examples, comparisons to conventional statistical methods, and practical guidance for implementing multilevel Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta version) for out-of-the-box implementation.", "subjects": "Artificial Intelligence (cs.AI); Applications (stat.AP)", "comments": "23 pages, 9 figures", "pdf_url": "https://arxiv.org/pdf/2505.05602.pdf", "abstract_url": "https://arxiv.org/abs/2505.05602", "categories": ["Artificial Intelligence (cs.AI)", "Applications (stat.AP)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了HiBayES，一个用于AI评估统计的分层贝叶斯建模框架，旨在从随机输出中稳健估计大型语言模型（LLMs）和其他AI系统的能力，并系统量化这些估计中的不确定性。", "motivation": "随着大型语言模型（LLMs）和其他AI系统的发展，从固有的随机输出中稳健估计其能力，并系统量化这些估计中的不确定性变得越来越重要。高级AI评估通常具有嵌套的层次结构，表现出高度的复杂性，并且在测试最先进的AI系统时成本高昂。", "method": "HiBayES是一个基于广义线性模型（GLMs）、贝叶斯数据分析和正式模型比较的分层贝叶斯建模框架，支持在经典问答基准和高级代理评估中进行稳健推断，特别是在低数据场景（如每个评估<20个数据点）中。", "result": "HiBayES提供了原则性的不确定性量化和稳健的参数估计，通过说明性示例、与传统统计方法的比较以及实现多层次贝叶斯GLMs的实用指南，全面介绍了HiBayES。", "conclusion": "HiBayES框架及其软件包（Beta版本）为AI评估统计提供了一个通用且强大的工具，特别是在处理复杂、高成本的AI系统评估时，能够提供稳健的推断和不确定性量化。"}}
{"id": "2505.05701", "title": "Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning", "authors": ["Jongchan Park", "Mingyu Park", "Donghwan Lee"], "abstract": "Offline reinforcement learning (RL) aims to learn a policy from a static dataset without further interactions with the environment. Collecting sufficiently large datasets for offline RL is exhausting since this data collection requires colossus interactions with environments and becomes tricky when the interaction with the environment is restricted. Hence, how an agent learns the best policy with a minimal static dataset is a crucial issue in offline RL, similar to the sample efficiency problem in online RL. In this paper, we propose a simple yet effective plug-and-play pretraining method to initialize a feature of a $Q$-network to enhance data efficiency in offline RL. Specifically, we introduce a shared $Q$-network structure that outputs predictions of the next state and $Q$-value. We pretrain the shared $Q$-network through a supervised regression task that predicts a next state and trains the shared $Q$-network using diverse offline RL methods. Through extensive experiments, we empirically demonstrate that our method enhances the performance of existing popular offline RL methods on the D4RL, Robomimic and V-D4RL benchmarks. Furthermore, we show that our method significantly boosts data-efficient offline RL across various data qualities and data distributions trough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of the dataset outperforms standard algorithms even with full datasets.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05701.pdf", "abstract_url": "https://arxiv.org/abs/2505.05701", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种简单有效的预训练方法，通过初始化Q网络的特征来增强离线强化学习的数据效率。该方法通过共享Q网络结构预测下一个状态和Q值，并通过监督回归任务进行预训练，显著提高了在D4RL、Robomimic和V-D4RL等基准测试中的性能。", "motivation": "离线强化学习（RL）旨在从静态数据集中学习策略，而无需与环境进一步交互。由于数据收集需要大量环境交互，且在环境交互受限时变得复杂，因此如何在最小静态数据集上学习最佳策略成为离线RL中的关键问题。", "method": "提出了一种共享Q网络结构，该结构输出下一个状态和Q值的预测。通过监督回归任务预训练共享Q网络，并使用多样化的离线RL方法进行训练。", "result": "实验证明，该方法在D4RL、Robomimic和V-D4RL基准测试中显著提高了现有流行离线RL方法的性能。特别是在仅使用10%数据集的情况下，该方法的表现优于使用完整数据集的标准算法。", "conclusion": "该方法通过预训练共享Q网络，有效提升了离线RL的数据效率，适用于不同数据质量和分布的场景，为离线RL领域提供了一种高效的解决方案。"}}
{"id": "2505.05516", "title": "AI-powered virtual eye: perspective, challenges and opportunities", "authors": ["Yue Wu", "Yibo Guo", "Yulong Yan", "Jiancheng Yang", "Xin Zhou", "Ching-Yu Cheng", "Danli Shi", "Mingguang He"], "abstract": "We envision the \"virtual eye\" as a next-generation, AI-powered platform that uses interconnected foundation models to simulate the eye's intricate structure and biological function across all scales. Advances in AI, imaging, and multiomics provide a fertile ground for constructing a universal, high-fidelity digital replica of the human eye. This perspective traces the evolution from early mechanistic and rule-based models to contemporary AI-driven approaches, integrating in a unified model with multimodal, multiscale, dynamic predictive capabilities and embedded feedback mechanisms. We propose a development roadmap emphasizing the roles of large-scale multimodal datasets, generative AI, foundation models, agent-based architectures, and interactive interfaces. Despite challenges in interpretability, ethics, data processing and evaluation, the virtual eye holds the potential to revolutionize personalized ophthalmic care and accelerate research into ocular health and disease.", "subjects": "Tissues and Organs (q-bio.TO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)", "comments": "30 Pages, 3 figures, 1 table", "pdf_url": "https://arxiv.org/pdf/2505.05516.pdf", "abstract_url": "https://arxiv.org/abs/2505.05516", "categories": ["Tissues and Organs (q-bio.TO)", "Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一个名为“虚拟眼”的下一代AI驱动平台概念，旨在通过互联的基础模型模拟眼睛的复杂结构和生物功能。", "motivation": "解决眼科护理和研究中对高保真数字眼睛模型的需求，以促进个性化眼科护理和加速眼健康和疾病研究。", "method": "整合AI、成像和多组学技术，构建一个具有多模态、多尺度、动态预测能力和嵌入式反馈机制的统一模型。", "result": "提出了一个发展路线图，强调大规模多模态数据集、生成AI、基础模型、基于代理的架构和交互式界面的作用。", "conclusion": "尽管在可解释性、伦理、数据处理和评估方面存在挑战，虚拟眼有潜力彻底改变个性化眼科护理并加速眼健康和疾病研究。"}}
{"id": "2505.05758", "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning", "authors": ["Azim Ospanov", "Roozbeh Yousefzadeh"], "abstract": "Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a modular, model-agnostic pipeline that combines the strengths of the Lean compiler with an LLM's reasoning abilities to achieve better proof-generation results at a low sampling budget. Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top-K budget. The repaired sub-proofs are recombined and reverified, iterating up to a user-controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state-of-the-art accuracy of 75.0% among 7B-parameter models while keeping the sampling budget below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40% accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving.", "subjects": "Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05758.pdf", "abstract_url": "https://arxiv.org/abs/2505.05758", "categories": ["Artificial Intelligence (cs.AI)", "Logic in Computer Science (cs.LO)"], "matching_keywords": ["agent"], "AI": {"tldr": "APOLLO是一个结合大型语言模型（LLMs）和Lean编译器的自动化证明修复管道，旨在提高在低采样预算下的证明生成效率和准确性。通过在miniF2F基准测试中达到75.0%的准确率，APOLLO展示了其在自动定理证明领域的先进性能。", "motivation": "解决在使用大型语言模型（LLMs）生成完全正确的形式化证明时面临的挑战，特别是在有限的采样预算下提高证明生成的效率和正确性。", "method": "APOLLO采用模块化、模型无关的管道，结合Lean编译器的优势和LLMs的推理能力，通过自动化过程生成、分析、修复和重新验证证明，包括使用自动求解器和在剩余目标上低预算地调用LLM。", "result": "在miniF2F基准测试中，APOLLO在7B参数模型中达到了75.0%的最新准确率，同时将采样预算控制在一千以下。此外，APOLLO将Goedel-Prover-SFT的准确率提高到65.6%，同时将样本复杂度从25,600降低到几百。通用模型（o3-mini, o4-mini）的准确率从3-7%跃升至40%以上。", "conclusion": "APOLLO通过针对性的、编译器引导的LLM输出修复，在效率和正确性方面取得了显著提升，为可扩展的自动定理证明提供了一个通用范式。"}}
{"id": "2505.05543", "title": "Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction", "authors": ["Ahdiyeh Alipour", "Tilo Hartmann", "Maryam Alimardani"], "abstract": "Trust is a fundamental component of human-agent interaction. With the increasing presence of artificial agents in daily life, it is essential to understand how people perceive and trust these agents. One of the key challenges affecting this perception is the Uncanny Valley Effect (UVE), where increasingly human-like artificial beings can be perceived as eerie or repelling. Despite growing interest in trust and the UVE, existing research varies widely in terms of how these concepts are defined and operationalized. This inconsistency raises important questions about how and under what conditions the UVE influences trust in agents. A systematic understanding of their relationship is currently lacking. This review aims to examine the impact of the UVE on human trust in agents and to identify methodological patterns, limitations, and gaps in the existing empirical literature. Following PRISMA guidelines, a systematic search identified 53 empirical studies that investigated both UVE-related constructs and trust or trust-related outcomes. Studies were analyzed based on a structured set of categories, including types of agents and interactions, methodological and measurement approaches, and key findings. The results of our systematic review reveal that most studies rely on static images or hypothetical scenarios with limited real-time interaction, and the majority use subjective trust measures. This review offers a novel framework for classifying trust measurement approaches with regard to the best-practice criteria for empirically investigating the UVE. As the first systematic attempt to map the intersection of UVE and trust, this review contributes to a deeper understanding of their interplay and offers a foundation for future research. Keywords: the uncanny valley effect, trust, human-likeness, affinity response, human-agent interaction", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "comments": "75 pages, Figure 11, Table 5", "pdf_url": "https://arxiv.org/pdf/2505.05543.pdf", "abstract_url": "https://arxiv.org/abs/2505.05543", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文通过系统综述探讨了恐怖谷效应对人机交互中信任的影响，分析了现有文献中的方法论模式和局限性，并提出了一个分类信任测量方法的新框架。", "motivation": "随着人工智能代理在日常生活中的日益普及，理解人们如何感知和信任这些代理变得至关重要。恐怖谷效应（UVE）是影响这种感知的关键挑战之一，但目前关于UVE如何以及在何种条件下影响代理信任的研究存在广泛的不一致性。", "method": "遵循PRISMA指南，通过系统搜索识别了53项同时研究UVE相关构念和信任或信任相关结果的实证研究，并基于一系列结构化类别对这些研究进行了分析。", "result": "系统综述的结果显示，大多数研究依赖于静态图像或假设情景，实时互动有限，且多数使用主观信任测量。", "conclusion": "作为首次系统性地尝试映射UVE与信任的交集，本综述不仅加深了对它们相互作用的理解，还为未来研究奠定了基础。"}}
{"id": "2505.05622", "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory", "authors": ["Weichen Zhang", "Chen Gao", "Shiquan Yu", "Ruiying Peng", "Baining Zhao", "Qian Zhang", "Jinqiang Cui", "Xinlei Chen", "Yong Li"], "abstract": "Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \\href{", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05622.pdf", "abstract_url": "https://arxiv.org/abs/2505.05622", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了CityNavAgent，一种基于大型语言模型（LLM）的代理，用于解决无人机在城市环境中基于视觉和语言导航（VLN）的挑战。通过分层语义规划模块（HSPM）和全局记忆模块，该方法显著降低了导航复杂性，并在基准测试中实现了最先进的性能。", "motivation": "解决无人机在城市复杂环境中基于自然语言指令进行导航的挑战，特别是在缺乏预定义导航图和长距离探索动作空间急剧扩展的情况下。", "method": "设计了分层语义规划模块（HSPM）将长距离任务分解为不同语义层次的子目标，并开发了全局记忆模块来存储历史轨迹，形成拓扑图以简化导航。", "result": "基准测试显示，该方法在连续城市环境中的空中VLN任务上实现了显著的性能提升，达到了最先进的水平。", "conclusion": "CityNavAgent通过分层语义规划和全局记忆有效解决了空中VLN的复杂性问题，为无人机在城市环境中的导航提供了新的解决方案。"}}
{"id": "2505.05753", "title": "Towards Embodiment Scaling Laws in Robot Locomotion", "authors": ["Bo Ai", "Liu Dai", "Nico Bohlinger", "Dichen Li", "Tongzhou Mu", "Zhanxin Wu", "K. Fay", "Henrik I. Christensen", "Jan Peters", "Hao Su"], "abstract": "Developing generalist agents that can operate across diverse tasks, environments, and physical embodiments is a grand challenge in robotics and artificial intelligence. In this work, we focus on the axis of embodiment and investigate embodiment scaling laws$\\unicode{x2013}$the hypothesis that increasing the number of training embodiments improves generalization to unseen ones. Using robot locomotion as a test bed, we procedurally generate a dataset of $\\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and hexapods, and train generalist policies capable of handling diverse observation and action spaces on random subsets. We find that increasing the number of training embodiments improves generalization to unseen ones, and scaling embodiments is more effective in enabling embodiment-level generalization than scaling data on small, fixed sets of embodiments. Notably, our best policy, trained on the full dataset, zero-shot transfers to novel embodiments in the real world, such as Unitree Go2 and H1. These results represent a step toward general embodied intelligence, with potential relevance to adaptive control for configurable robots, co-design of morphology and control, and beyond.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2505.05753.pdf", "abstract_url": "https://arxiv.org/abs/2505.05753", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了机器人运动中的体现缩放定律，即增加训练体现数量可以提高对未见体现的泛化能力。通过程序生成约1,000种不同的体现数据集，并训练能够处理多样化观察和动作空间的通用策略，研究发现增加训练体现数量确实改善了泛化能力，且比在固定小规模体现集上扩展数据更有效。最佳策略在真实世界中实现了对新型体现的零射击转移。", "motivation": "开发能够在多样化任务、环境和物理体现中操作的通用代理是机器人和人工智能领域的一大挑战。本研究专注于体现轴，探索体现缩放定律，即增加训练体现数量是否能提高对未见体现的泛化能力。", "method": "使用机器人运动作为测试平台，程序化生成约1,000种不同的体现数据集，涵盖人形、四足和六足机器人，并训练能够处理多样化观察和动作空间的通用策略。", "result": "研究发现，增加训练体现数量确实提高了对未见体现的泛化能力，且比在固定小规模体现集上扩展数据更有效。最佳策略在真实世界中实现了对新型体现（如Unitree Go2和H1）的零射击转移。", "conclusion": "这些结果代表了向通用体现智能迈出的一步，对可配置机器人的自适应控制、形态与控制协同设计等领域具有潜在意义。"}}
{"id": "2505.05762", "title": "Multi-Agent Systems for Robotic Autonomy with LLMs", "authors": ["Junhong Chen", "Ziqi Yang", "Haoyuan G Xu", "Dandan Zhang", "George Mylonas"], "abstract": "Since the advent of Large Language Models (LLMs), various research based on such models have maintained significant academic attention and impact, especially in AI and robotics. In this paper, we propose a multi-agent framework with LLMs to construct an integrated system for robotic task analysis, mechanical design, and path generation. The framework includes three core agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer. Outputs are formatted as multimodal results, such as code files or technical reports, for stronger understandability and usability. To evaluate generalizability comparatively, we conducted experiments with models from both GPT and DeepSeek. Results demonstrate that the proposed system can design feasible robots with control strategies when appropriate task inputs are provided, exhibiting substantial potential for enhancing the efficiency and accessibility of robotic system development in research and industrial applications.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": "11 pages, 2 figures, 5 tables, submitted for publication", "pdf_url": "https://arxiv.org/pdf/2505.05762.pdf", "abstract_url": "https://arxiv.org/abs/2505.05762", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种基于大型语言模型（LLMs）的多智能体框架，用于机器人任务分析、机械设计和路径生成。该系统包含三个核心智能体：任务分析师、机器人设计师和强化学习设计师，能够生成多模态输出，如代码文件或技术报告，以提高理解和使用的便利性。通过GPT和DeepSeek模型的实验比较，证明了该系统在提供适当任务输入时能够设计出可行的机器人及控制策略，显示出在研究和工业应用中提高机器人系统开发效率和可访问性的巨大潜力。", "motivation": "解决机器人系统开发中的任务分析、机械设计和路径生成的集成问题，提高开发效率和可访问性。", "method": "提出一个包含任务分析师、机器人设计师和强化学习设计师三个核心智能体的多智能体框架，利用LLMs生成多模态输出。", "result": "实验结果表明，该系统能够在提供适当任务输入时设计出可行的机器人及控制策略。", "conclusion": "该多智能体框架显示出在机器人系统开发中提高效率和可访问性的潜力，适用于研究和工业应用。"}}
{"id": "2505.05849", "title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents", "authors": ["Zhun Wang", "Vincent Siu", "Zhe Ye", "Tianneng Shi", "Yuzhou Nie", "Xuandong Zhao", "Chenguang Wang", "Wenbo Guo", "Dawn Song"], "abstract": "The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capable of leveraging external tools and interacting with increasingly complex environments. However, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulating contextual information rather than direct user prompts. In this work, we propose a generic black-box fuzzing framework, AgentXploit, designed to automatically discover and exploit indirect prompt injection vulnerabilities across diverse LLM agents. Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo and VWA-adv, where it achieves 71% and 70% success rates against agents based on o3-mini and GPT-4o, respectively, nearly doubling the performance of baseline attacks. Moreover, AgentXploit exhibits strong transferability across unseen tasks and internal LLMs, as well as promising results against defenses. Beyond benchmark evaluations, we apply our attacks in real-world environments, successfully misleading agents to navigate to arbitrary URLs, including malicious sites.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05849.pdf", "abstract_url": "https://arxiv.org/abs/2505.05849", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了AgentXploit，一个用于自动发现和利用大型语言模型（LLM）代理中间接提示注入漏洞的黑盒模糊测试框架。通过在两个公共基准测试上的评估，AgentXploit展示了高效的攻击成功率，并在实际环境中成功误导代理导航至恶意网站。", "motivation": "大型语言模型（LLM）的规划和推理能力推动了基于代理的系统的发展，但这些能力也引入了间接提示注入的安全风险，这种攻击通过操纵上下文信息而非直接用户提示来危害LLM。", "method": "提出了一个基于蒙特卡洛树搜索（MCTS）的种子选择算法，通过迭代优化输入来构建高质量的初始种子库，以最大化发现代理弱点的可能性。", "result": "在AgentDojo和VWA-adv两个公共基准测试上，AgentXploit对基于o3-mini和GPT-4o的代理分别达到了71%和70%的成功率，几乎是基线攻击性能的两倍。", "conclusion": "AgentXploit不仅在不同任务和内部LLM之间表现出强大的迁移能力，而且对防御措施也显示出有希望的结果，突显了当前LLM代理在安全性方面的脆弱性。"}}
