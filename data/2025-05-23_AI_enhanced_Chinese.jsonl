{"id": "2505.16007", "title": "Position: Agentic Systems Constitute a Key Component of Next-Generation Intelligent Image Processing", "authors": ["Jinjin Gu"], "abstract": "This position paper argues that the image processing community should broaden its focus from purely model-centric development to include agentic system design as an essential complementary paradigm. While deep learning has significantly advanced capabilities for specific image processing tasks, current approaches face critical limitations in generalization, adaptability, and real-world problem-solving flexibility. We propose that developing intelligent agentic systems, capable of dynamically selecting, combining, and optimizing existing image processing tools, represents the next evolutionary step for the field. Such systems would emulate human experts' ability to strategically orchestrate different tools to solve complex problems, overcoming the brittleness of monolithic models. The paper analyzes key limitations of model-centric paradigms, establishes design principles for agentic image processing systems, and outlines different capability levels for such agents.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16007.pdf", "abstract_url": "https://arxiv.org/abs/2505.16007", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文主张图像处理社区应从纯模型中心开发扩展到包括代理系统设计，作为下一代智能图像处理的关键组成部分。", "motivation": "解决当前深度学习在图像处理任务中面临的泛化性、适应性和现实问题解决灵活性方面的关键限制。", "method": "提出开发能够动态选择、结合和优化现有图像处理工具的智能代理系统。", "result": "分析了模型中心范式的主要限制，建立了代理图像处理系统的设计原则，并概述了此类代理的不同能力水平。", "conclusion": "智能代理系统能够模拟人类专家战略性地协调不同工具解决复杂问题的能力，代表了图像处理领域的下一步进化。"}}
{"id": "2505.15916", "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law", "authors": ["Juvenal Domingos Júnior", "Augusto Faria", "E. Seiti de Oliveira", "Erick de Brito", "Matheus Teotonio", "Andre Assumpção", "Diedre Carmo", "Roberto Lotufo", "Jayr Pereira"], "abstract": "This paper presents BR-TaxQA-R, a novel dataset designed to support question answering with references in the context of Brazilian personal income tax law. The dataset contains 715 questions from the 2024 official Q\\&A document published by Brazil's Internal Revenue Service, enriched with statutory norms and administrative rulings from the Conselho Administrativo de Recursos Fiscais (CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using OpenAI embeddings for searching and GPT-4o-mini for answer generation. We compare different text segmentation strategies and benchmark our system against commercial tools such as ChatGPT and", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.15916.pdf", "abstract_url": "https://arxiv.org/abs/2505.15916", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了BR-TaxQA-R数据集，旨在支持巴西个人所得税法背景下的带参考问答。数据集包含715个问题，来自巴西国税局2024年官方问答文件，并丰富了CARF的法定规范和行政裁决。", "motivation": "解决在巴西个人所得税法背景下，缺乏高质量、带参考的问答数据集的问题。", "method": "采用检索增强生成（RAG）流程，使用OpenAI嵌入进行搜索，GPT-4o-mini进行答案生成，并比较不同的文本分割策略。", "result": "创建了一个包含715个问题的数据集，并实现了基于RAG的问答系统，与ChatGPT等商业工具进行了对比。", "conclusion": "BR-TaxQA-R数据集和RAG流程为巴西个人所得税法领域的问答系统提供了有价值的资源和基准。"}}
{"id": "2505.15928", "title": "ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation", "authors": ["Tony Montes", "Fernando Lozano"], "abstract": "Recent advancements in Video Question Answering (VideoQA) have introduced LLM-based agents, modular frameworks, and procedural solutions, yielding promising results. These systems use dynamic agents and memory-based mechanisms to break down complex tasks and refine answers. However, significant improvements remain in tracking objects for grounding over time and decision-making based on reasoning to better align object references with language model outputs, as newer models get better at both tasks. This work presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA) that combines a Chain-of-Thought framework with grounding reasoning alongside YOLO-World to enhance object tracking and alignment. This approach establishes a new state-of-the-art in VideoQA and Video Understanding, showing enhanced performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also enables cross-checking of grounding timeframes, improving accuracy and providing valuable support for verification and increased output reliability across multiple video domains. The code is available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.15928.pdf", "abstract_url": "https://arxiv.org/abs/2505.15928", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了ViQAgent，一个基于LLM的零样本视频问答代理，通过结合思维链框架和接地推理以及YOLO-World来增强对象跟踪和对齐，在VideoQA和视频理解领域建立了新的最先进技术。", "motivation": "解决视频问答中对象跟踪和基于推理的决策问题，以更好地对齐对象引用与语言模型输出。", "method": "使用LLM-brained代理，结合思维链框架和接地推理，以及YOLO-World进行对象跟踪和对齐。", "result": "在NExT-QA、iVQA和ActivityNet-QA基准测试中表现出增强的性能，实现了视频问答和视频理解的新最先进技术。", "conclusion": "该框架不仅提高了准确性，还支持跨领域视频的验证和输出可靠性，为视频问答领域提供了有价值的工具和方法。"}}
{"id": "2505.16229", "title": "CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering", "authors": ["Yuren Mao", "Wenyi Xu", "Yuyang Qin", "Yunjun Gao"], "abstract": "Computed Tomography (CT) scan, which produces 3D volumetric medical data that can be viewed as hundreds of cross-sectional images (a.k.a. slices), provides detailed anatomical information for diagnosis. For radiologists, creating CT radiology reports is time-consuming and error-prone. A visual question answering (VQA) system that can answer radiologists' questions about some anatomical regions on the CT scan and even automatically generate a radiology report is urgently needed. However, existing VQA systems cannot adequately handle the CT radiology question answering (CTQA) task for: (1) anatomic complexity makes CT images difficult to understand; (2) spatial relationship across hundreds slices is difficult to capture. To address these issues, this paper proposes CT-Agent, a multimodal agentic framework for CTQA. CT-Agent adopts anatomically independent tools to break down the anatomic complexity; furthermore, it efficiently captures the across-slice spatial relationship with a global-local token compression strategy. Experimental results on two 3D chest CT datasets, CT-RATE and RadGenome-ChestCT, verify the superior performance of CT-Agent.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16229.pdf", "abstract_url": "https://arxiv.org/abs/2505.16229", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了一种名为CT-Agent的多模态代理框架，旨在解决3D CT放射学问答任务中的解剖复杂性和跨切片空间关系捕获问题。通过采用解剖学独立工具和全局-局部令牌压缩策略，CT-Agent在两个3D胸部CT数据集上验证了其优越性能。", "motivation": "CT扫描产生的3D体积医学数据为诊断提供了详细的解剖信息，但放射科医生创建CT放射学报告既耗时又容易出错。急需一种视觉问答系统，能够回答放射科医生关于CT扫描上某些解剖区域的问题，甚至自动生成放射学报告。", "method": "CT-Agent采用解剖学独立工具来分解解剖复杂性，并通过全局-局部令牌压缩策略有效捕获跨切片的空间关系。", "result": "在两个3D胸部CT数据集CT-RATE和RadGenome-ChestCT上的实验结果验证了CT-Agent的优越性能。", "conclusion": "CT-Agent通过其创新的方法有效解决了CT放射学问答任务中的主要挑战，展示了在自动生成放射学报告方面的潜力。"}}
{"id": "2505.15922", "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition", "authors": ["Dong Won Lee", "Hae Won Park", "Cynthia Breazeal", "Louis-Philippe Morency"], "abstract": "We propose a large language model based reward decomposition framework for aligning dialogue agents using only a single session-level feedback signal. We leverage the reasoning capabilities of a frozen, pretrained large language model (LLM) to infer fine-grained local implicit rewards by decomposing global, session-level feedback. Our first text-only variant prompts the LLM to perform reward decomposition using only the dialogue transcript. The second multimodal variant incorporates additional behavioral cues, such as pitch, gaze, and facial affect, expressed as natural language descriptions. These inferred turn-level rewards are distilled into a lightweight reward model, which we utilize for RL-based fine-tuning for dialogue generation. We evaluate both text-only and multimodal variants against state-of-the-art reward decomposition methods and demonstrate notable improvements in human evaluations of conversation quality, suggesting that LLMs are strong reward decomposers that obviate the need for manual reward shaping and granular human feedback.", "subjects": "Computation and Language (cs.CL)", "comments": "9 pages, 3 figures, 3 tables", "pdf_url": "https://arxiv.org/pdf/2505.15922.pdf", "abstract_url": "https://arxiv.org/abs/2505.15922", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "提出了一种基于大型语言模型（LLM）的奖励分解框架，用于仅使用会话级反馈信号对齐对话代理。通过利用预训练LLM的推理能力，从全局反馈中推断细粒度的局部隐式奖励，并将这些奖励提炼成轻量级奖励模型，用于基于强化学习的对话生成微调。", "motivation": "解决对话代理对齐问题，避免需要手动奖励塑造和细粒度人类反馈。", "method": "使用冻结的预训练LLM进行奖励分解，包括仅文本和多模态（结合行为线索）两种变体，并将推断的回合级奖励蒸馏成轻量级奖励模型。", "result": "在对话质量的人类评估中，两种变体均优于最先进的奖励分解方法，表明LLM是强大的奖励分解器。", "conclusion": "LLM能够有效分解全局反馈为局部奖励，简化对话代理的对齐过程，提高对话生成质量。"}}
{"id": "2505.16282", "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay", "authors": ["Fanbin Lu", "Zhisheng Zhong", "Shu Liu", "Chi-Wing Fu", "Jiaya Jia"], "abstract": "Training large language models (LLMs) as interactive agents for controlling graphical user interfaces (GUIs) presents a unique challenge to optimize long-horizon action sequences with multimodal feedback from complex environments. While recent works have advanced multi-turn reinforcement learning (RL) for reasoning and tool-using capabilities in LLMs, their application to GUI-based agents remains relatively underexplored due to the difficulty of sparse rewards, delayed feedback, and high rollout costs. In this paper, we investigate end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an end-to-end RL approach that augments Group Relative Policy Optimization (GRPO) with a replay buffer to reuse the successful experience across training iterations. To further stabilize the training process, we propose a task selection strategy that filters tasks based on baseline agent performance, allowing the agent to focus on learning from informative interactions. Additionally, we compare ARPO with offline preference optimization approaches, highlighting the advantages of policy-based methods in GUI environments. Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive results, establishing a new performance baseline for LLM-based GUI agents trained via reinforcement learning. Our findings underscore the effectiveness of reinforcement learning for training multi-turn, vision-language GUI agents capable of managing complex real-world UI interactions. Codes and models:", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16282.pdf", "abstract_url": "https://arxiv.org/abs/2505.16282", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了ARPO（Agentic Replay Policy Optimization），一种端到端的强化学习方法，用于优化基于视觉语言的GUI代理在复杂、长期计算机任务中的表现。通过结合GRPO和重放缓冲区，ARPO能够跨训练迭代重用成功经验，并通过任务选择策略稳定训练过程。在OSWorld基准测试中，ARPO展示了其竞争力，为基于LLM的GUI代理设立了新的性能基准。", "motivation": "训练大型语言模型（LLMs）作为交互式代理控制图形用户界面（GUIs）时，面临优化长序列动作和复杂环境多模态反馈的挑战。现有方法在GUI代理中的应用尚未充分探索，主要由于稀疏奖励、延迟反馈和高滚动成本的困难。", "method": "提出了ARPO方法，这是一种端到端的强化学习（RL）方法，通过增强GRPO（Group Relative Policy Optimization）并引入重放缓冲区来跨训练迭代重用成功经验。此外，提出了一种任务选择策略，基于基线代理性能过滤任务，以专注于从信息丰富的交互中学习。", "result": "在OSWorld基准测试中，ARPO取得了竞争性结果，为基于LLM的GUI代理通过强化学习训练设立了新的性能基准。", "conclusion": "研究结果强调了强化学习在训练能够管理复杂现实世界UI交互的多轮、视觉语言GUI代理中的有效性。"}}
{"id": "2505.16067", "title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior", "authors": ["Zidi Xiong", "Yuping Lin", "Wenya Xie", "Pengfei He", "Jiliang Tang", "Himabindu Lakkaraju", "Zhen Xiang"], "abstract": "Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory operations that are widely used by many agent frameworks-addition, which incorporates new experiences into the memory base, and deletion, which selectively removes past experiences-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where outdated or irrelevant experiences negatively influence current tasks. Through controlled experiments, we show that combining selective addition and deletion strategies can help mitigate these negative effects, yielding an average absolute performance gain of 10% compared to naive memory growth. Furthermore, we highlight how memory management choices affect agents' behavior under challenging conditions such as task distribution shifts and constrained memory resources. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance. We also release our code to facilitate further study.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16067.pdf", "abstract_url": "https://arxiv.org/abs/2505.16067", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文通过实证研究探讨了内存管理选择对大型语言模型（LLM）代理行为的影响，特别是其长期性能。研究发现，LLM代理表现出经验跟随特性，并揭示了与此特性相关的两个主要挑战：错误传播和未对齐的经验回放。通过实验，研究表明选择性添加和删除策略可以减轻这些负面影响，平均绝对性能提升10%。", "motivation": "解决大型语言模型（LLM）代理在长期任务执行中，由于内存管理选择不当导致的性能下降问题。", "method": "通过定量分析，研究两种基本内存操作（添加和删除）对代理行为的影响，并通过控制实验验证选择性内存管理策略的有效性。", "result": "发现LLM代理具有经验跟随特性，且选择性添加和删除策略能有效减轻错误传播和未对齐经验回放的负面影响，提升性能。", "conclusion": "内存管理选择对LLM代理的长期性能有显著影响，合理设计内存组件可以支持稳健的长期代理性能。研究结果为设计高效内存系统提供了实用指导。"}}
{"id": "2505.16014", "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "authors": ["Yash Saxena", "Anpur Padia", "Mandar S Chaudhary", "Kalpa Gunaratna", "Srinivasan Parthasarathy", "Manas Gaur"], "abstract": "Traditional Retrieval-Augmented Generation (RAG) pipelines rely on similarity-based retrieval and re-ranking, which depend on heuristics such as top-k, and lack explainability, interpretability, and robustness against adversarial content. To address this gap, we propose a novel method METEORA that replaces re-ranking in RAG with a rationale-driven selection approach. METEORA operates in two stages. First, a general-purpose LLM is preference-tuned to generate rationales conditioned on the input query using direct preference optimization. These rationales guide the evidence chunk selection engine, which selects relevant chunks in three stages: pairing individual rationales with corresponding retrieved chunks for local relevance, global selection with elbow detection for adaptive cutoff, and context expansion via neighboring chunks. This process eliminates the need for top-k heuristics. The rationales are also used for consistency check using a Verifier LLM to detect and filter poisoned or misleading content for safe generation. The framework provides explainable and interpretable evidence flow by using rationales consistently across both selection and verification. Our evaluation across six datasets spanning legal, financial, and academic research domains shows that METEORA improves generation accuracy by 33.34% while using approximately 50% fewer chunks than state-of-the-art re-ranking methods. In adversarial settings, METEORA significantly improves the F1 score from 0.10 to 0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating strong resilience to poisoning attacks. Code available at:", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16014.pdf", "abstract_url": "https://arxiv.org/abs/2505.16014", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种名为METEORA的新方法，用于改进传统的检索增强生成（RAG）流程，通过用基于理性的选择方法替代重新排名，以提高在敏感领域的生成准确性、解释性和对抗性内容的鲁棒性。", "motivation": "传统的RAG流程依赖于基于相似性的检索和重新排名，这些方法依赖于启发式方法（如top-k），缺乏可解释性、可理解性以及对对抗性内容的鲁棒性。", "method": "METEORA采用两阶段方法：首先，通过直接偏好优化调整通用大型语言模型（LLM）以生成基于输入查询的理性；然后，这些理性指导证据块选择引擎，通过局部相关性、全局选择和上下文扩展三个阶段选择相关块，无需top-k启发式方法。", "result": "在六个涵盖法律、金融和学术研究领域的数据集上的评估显示，METEORA将生成准确性提高了33.34%，同时使用的块数比最先进的重新排名方法少约50%。在对抗性设置中，METEORA将F1分数从0.10显著提高到0.44，显示出对中毒攻击的强大抵抗力。", "conclusion": "METEORA通过引入基于理性的选择和验证机制，提供了一种更安全、更可解释的RAG流程，显著提高了在敏感领域的生成准确性和对抗性内容的鲁棒性。"}}
{"id": "2505.16086", "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "abstract": "We have seen remarkable progress in large language models (LLMs) empowered multi-agent systems solving complex tasks necessitating cooperation among experts with diverse skills. However, optimizing LLM-based multi-agent systems remains challenging. In this work, we perform an empirical case study on group optimization of role-based multi-agent systems utilizing natural language feedback for challenging software development tasks under various evaluation dimensions. We propose a two-step agent prompts optimization pipeline: identifying underperforming agents with their failure explanations utilizing textual feedback and then optimizing system prompts of identified agents utilizing failure explanations. We then study the impact of various optimization settings on system performance with two comparison groups: online against offline optimization and individual against group optimization. For group optimization, we study two prompting strategies: one-pass and multi-pass prompting optimizations. Overall, we demonstrate the effectiveness of our optimization method for role-based multi-agent systems tackling software development tasks evaluated on diverse evaluation dimensions, and we investigate the impact of diverse optimization settings on group behaviors of the multi-agent systems to provide practical insights for future development.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16086.pdf", "abstract_url": "https://arxiv.org/abs/2505.16086", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文通过实证案例研究，探讨了基于大型语言模型（LLM）的多代理系统在软件开发任务中的群体优化问题，提出了一种利用自然语言反馈的两步代理提示优化流程，并研究了不同优化设置对系统性能的影响。", "motivation": "尽管基于LLM的多代理系统在解决需要多样化技能专家合作的复杂任务方面取得了显著进展，但其优化仍面临挑战。本文旨在通过自然语言反馈优化角色基础的多代理系统，以提升软件开发任务的执行效率。", "method": "提出了一种两步代理提示优化流程：首先利用文本反馈识别表现不佳的代理及其失败解释，然后基于这些解释优化已识别代理的系统提示。此外，研究了在线与离线优化、个体与群体优化等不同优化设置的影响。", "result": "研究证明了所提出的优化方法在多样化评估维度上对角色基础的多代理系统处理软件开发任务的有效性，并探讨了不同优化设置对多代理系统群体行为的影响。", "conclusion": "本文不仅展示了优化方法的有效性，还为未来多代理系统的开发提供了实用的见解，特别是在如何利用自然语言反馈进行系统优化方面。"}}
{"id": "2505.16100", "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "abstract": "Validating scientific hypotheses is a central challenge in biomedical research, and remains difficult for artificial intelligence (AI) agents due to the complexity of real-world data analysis and evidence interpretation. In this work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans, curated from over 300 published biomedical studies to reflect the structure and reasoning found in authentic research workflows. Each task includes a structured hypothesis derived from the original study's conclusions, expressed in the affirmative to reflect the language of scientific reporting, and one or more pieces of supporting evidence grounded in empirical data tables. While these hypotheses mirror published claims, they remain testable using standard statistical or machine learning methods. The benchmark enables evaluation along four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and conclusion, (3) correctness of the reasoning process, and (4) executability of the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable hypotheses: cases where the available data are insufficient to support or refute a claim, reflecting a common yet underexplored scenario in real-world science. We propose BioDSA-1K as a foundation for building and evaluating generalizable, trustworthy AI agents for biomedical discovery.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16100.pdf", "abstract_url": "https://arxiv.org/abs/2505.16100", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "BioDSA-1K是一个旨在评估AI代理在真实、数据驱动的生物医学假设验证任务上表现的基准，包含1,029个假设中心任务和1,177个分析计划，源自300多项已发表的生物医学研究。", "motivation": "解决生物医学研究中验证科学假设的挑战，特别是AI代理在处理真实世界数据分析和证据解释方面的复杂性。", "method": "创建BioDSA-1K基准，包含从已发表研究中提取的结构化假设和支持证据，评估AI代理在假设决策准确性、证据与结论对齐、推理过程正确性及生成代码可执行性四个方面的表现。", "result": "BioDSA-1K提供了一个评估AI代理在生物医学发现中通用性和可信赖性的基础，包括处理数据不足以支持或反驳假设的常见情况。", "conclusion": "BioDSA-1K为构建和评估用于生物医学发现的通用、可信赖AI代理奠定了基础，特别是在处理复杂和不确定的科学假设时。"}}
{"id": "2505.16120", "title": "LLM-Powered AI Agent Systems and Their Applications in Industry", "authors": ["Guannan Liang", "Qianqian Tong"], "abstract": "The emergence of Large Language Models (LLMs) has reshaped agent systems. Unlike traditional rule-based agents with limited task scope, LLM-powered agents offer greater flexibility, cross-domain reasoning, and natural language interaction. Moreover, with the integration of multi-modal LLMs, current agent systems are highly capable of processing diverse data modalities, including text, images, audio, and structured tabular data, enabling richer and more adaptive real-world behavior. This paper comprehensively examines the evolution of agent systems from the pre-LLM era to current LLM-powered architectures. We categorize agent systems into software-based, physical, and adaptive hybrid systems, highlighting applications across customer service, software development, manufacturing automation, personalized education, financial trading, and healthcare. We further discuss the primary challenges posed by LLM-powered agents, including high inference latency, output uncertainty, lack of evaluation metrics, and security vulnerabilities, and propose potential solutions to mitigate these concerns.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "This is the author's accepted version of the paper accepted to appear at IEEE AIIoT 2025. The final version will be available via IEEE Xplore. \\c{opyright}2025 IEEE. Personal use of this material is permitted", "pdf_url": "https://arxiv.org/pdf/2505.16120.pdf", "abstract_url": "https://arxiv.org/abs/2505.16120", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了大型语言模型（LLMs）如何重塑代理系统，从传统的基于规则的代理转变为提供更大灵活性、跨领域推理和自然语言交互的LLM驱动代理。文章还讨论了多模态LLMs的集成如何使代理系统能够处理多种数据模态，以及在多个行业中的应用和面临的挑战。", "motivation": "解决传统基于规则的代理系统在任务范围、灵活性和交互方式上的限制，以及探索LLM驱动代理系统在多行业应用中的潜力和挑战。", "method": "通过分类代理系统（软件基础、物理和自适应混合系统）和讨论多模态LLMs的集成，全面考察了代理系统从LLM前时代到当前LLM驱动架构的演变。", "result": "LLM驱动的代理系统在客户服务、软件开发、制造自动化、个性化教育、金融交易和医疗保健等多个领域展现出广泛的应用潜力，但也面临高推理延迟、输出不确定性、缺乏评估指标和安全漏洞等挑战。", "conclusion": "尽管LLM驱动的代理系统在多行业应用中展现出巨大潜力，但仍需解决高推理延迟、输出不确定性等挑战，以充分发挥其价值。文章提出了可能的解决方案以缓解这些担忧。"}}
{"id": "2505.16102", "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "abstract": "While bariatric and metabolic surgery (MBS) is considered the gold standard treatment for severe and morbid obesity, its therapeutic efficacy hinges upon active and longitudinal engagement with multidisciplinary providers, including surgeons, dietitians/nutritionists, psychologists, and endocrinologists. This engagement spans the entire patient journey, from preoperative preparation to long-term postoperative management. However, this process is often hindered by numerous healthcare disparities, such as logistical and access barriers, which impair easy patient access to timely, evidence-based, clinician-endorsed information. To address these gaps, we introduce bRAGgen, a novel adaptive retrieval-augmented generation (RAG)-based model that autonomously integrates real-time medical evidence when response confidence dips below dynamic thresholds. This self-updating architecture ensures that responses remain current and accurate, reducing the risk of misinformation. Additionally, we present bRAGq, a curated dataset of 1,302 bariatric surgery--related questions, validated by an expert bariatric surgeon. bRAGq constitutes the first large-scale, domain-specific benchmark for comprehensive MBS care. In a two-phase evaluation, bRAGgen is benchmarked against state-of-the-art models using both large language model (LLM)--based metrics and expert surgeon review. Across all evaluation dimensions, bRAGgen demonstrates substantially superior performance in generating clinically accurate and relevant responses.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16102.pdf", "abstract_url": "https://arxiv.org/abs/2505.16102", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "介绍了一种名为bRAGgen的新型自适应检索增强生成模型，旨在通过自主整合实时医学证据来解决减肥和代谢手术（MBS）领域的信息获取障碍，确保提供的信息既及时又准确。同时，提出了一个包含1,302个问题的bRAGq数据集，作为MBS护理的首个大规模、特定领域的基准。", "motivation": "解决减肥和代谢手术（MBS）患者因医疗差异（如物流和访问障碍）难以及时获取基于证据的、临床认可的信息的问题。", "method": "开发了bRAGgen，一种自适应检索增强生成（RAG）模型，当响应信心低于动态阈值时，自主整合实时医学证据，并引入了bRAGq数据集作为评估基准。", "result": "在两阶段评估中，bRAGgen在使用大型语言模型（LLM）基于的指标和专家外科医生评审的所有评估维度上，均显示出在生成临床准确和相关响应方面的显著优越性能。", "conclusion": "bRAGgen通过其自我更新架构确保了信息的时效性和准确性，减少了错误信息的风险，为MBS领域提供了一个有效的解决方案。"}}
{"id": "2505.16199", "title": "Velocity Completion Task and Method for Event-based Player Positional Data in Soccer", "authors": ["Rikuhei Umemoto", "Keisuke Fujii"], "abstract": "In many real-world complex systems, the behavior can be observed as a collection of discrete events generated by multiple interacting agents. Analyzing the dynamics of these multi-agent systems, especially team sports, often relies on understanding the movement and interactions of individual agents. However, while providing valuable snapshots, event-based positional data typically lacks the continuous temporal information needed to directly calculate crucial properties such as velocity. This absence severely limits the depth of dynamic analysis, preventing a comprehensive understanding of individual agent behaviors and emergent team strategies. To address this challenge, we propose a new method to simultaneously complete the velocity of all agents using only the event-based positional data from team sports. Based on this completed velocity information, we investigate the applicability of existing team sports analysis and evaluation methods. Experiments using soccer event data demonstrate that neural network-based approaches outperformed rule-based methods regarding velocity completion error, considering the underlying temporal dependencies and graph structure of player-to-player or player-to-ball interaction. Moreover, the space evaluation results obtained using the completed velocity are closer to those derived from complete tracking data, highlighting our method's potential for enhanced team sports system analysis.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "24 pages, 7 figures", "pdf_url": "https://arxiv.org/pdf/2505.16199.pdf", "abstract_url": "https://arxiv.org/abs/2505.16199", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种新方法，用于仅基于足球等团队运动中的事件位置数据同时完成所有代理的速度计算，以解决事件位置数据缺乏连续时间信息的问题。通过实验证明，基于神经网络的方法在速度完成误差上优于基于规则的方法，并且使用完成速度获得的空间评估结果更接近完整跟踪数据的结果。", "motivation": "团队运动等多代理系统中，事件位置数据虽然提供了有价值的快照，但缺乏连续时间信息，无法直接计算速度等关键属性，这限制了对个体代理行为和团队策略的深入分析。", "method": "提出了一种新方法，利用神经网络基于事件位置数据同时完成所有代理的速度计算，考虑了时间依赖性和代理间或代理与球之间的图结构。", "result": "实验表明，基于神经网络的方法在速度完成误差上优于基于规则的方法，且使用完成速度获得的空间评估结果更接近完整跟踪数据的结果。", "conclusion": "该方法为团队运动系统分析提供了增强的潜力，能够更全面地理解个体代理行为和团队策略。"}}
{"id": "2505.16288", "title": "No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery", "authors": ["Xiaoxue Han", "Pengfei Hu", "Jun-En Ding", "Chang Lu", "Feng Liu", "Yue Ning"], "abstract": "Deep learning models trained on extensive Electronic Health Records (EHR) data have achieved high accuracy in diagnosis prediction, offering the potential to assist clinicians in decision-making and treatment planning. However, these models lack two crucial features that clinicians highly value: interpretability and interactivity. The ``black-box'' nature of these models makes it difficult for clinicians to understand the reasoning behind predictions, limiting their ability to make informed decisions. Additionally, the absence of interactive mechanisms prevents clinicians from incorporating their own knowledge and experience into the decision-making process. To address these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal discovery framework that integrates personalized knowledge databases and agentic LLMs. II-KEA enhances interpretability through explicit reasoning and causal analysis, while also improving interactivity by allowing clinicians to inject their knowledge and experience through customized knowledge bases and prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating superior performance along with enhanced interpretability and interactivity, as evidenced by its strong results from extensive case studies.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16288.pdf", "abstract_url": "https://arxiv.org/abs/2505.16288", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了II-KEA框架，旨在解决医疗预测中深度学习模型的不可解释性和缺乏交互性问题，通过结合知识增强和因果发现技术，提高模型的解释性和交互性。", "motivation": "解决电子健康记录(EHR)数据训练的深度学习模型在诊断预测中缺乏解释性和交互性的问题，这些问题限制了临床医生在决策过程中的应用。", "method": "提出了II-KEA框架，结合个性化知识数据库和代理性大型语言模型(LLMs)，通过显式推理和因果分析增强解释性，同时允许临床医生通过定制知识库和提示注入他们的知识和经验，提高交互性。", "result": "在MIMIC-III和MIMIC-IV数据集上的评估显示，II-KEA在保持高性能的同时，显著提高了模型的解释性和交互性。", "conclusion": "II-KEA框架通过增强医疗预测模型的解释性和交互性，为临床医生提供了更有效的决策支持工具，有望在医疗领域产生重要影响。"}}
{"id": "2505.16455", "title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events", "authors": ["Mengzhu Liu", "Zhengqiu Zhu", "Chuan Ai", "Chen Gao", "Xinghong Li", "Lingnan He", "Kaisheng Lai", "Yingfeng Chen", "Xin Lu", "Yong Li", "Quanjun Yin"], "abstract": "During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque \"data-driven fitting\" to transparent \"role-based simulation with mechanistic interpretation\" for panic emotion prediction during emergencies. Our implementation is publicly available at:", "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16455.pdf", "abstract_url": "https://arxiv.org/abs/2505.16455", "categories": ["Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)"], "matching_keywords": ["agent"], "AI": {"tldr": "该论文提出了一种心理学驱动的生成代理框架（PsychoAgent），用于在突发灾害事件中基于情绪唤起理论进行可解释的恐慌预测。通过构建细粒度的开放恐慌情绪数据集（COPE）和整合跨领域异构数据，该框架提高了恐慌情绪预测的准确性和可解释性。", "motivation": "解决在突发灾害事件中，社交媒体上公众恐慌情绪预测面临的三个主要挑战：缺乏精细标注的数据、未建模的风险感知导致预测不准确，以及恐慌形成机制的解释不足。", "method": "首先通过人类与大型语言模型（LLMs）合作构建细粒度的开放恐慌情绪数据集（COPE），然后开发一个基于心理机制的框架整合跨领域异构数据来建模风险感知和情绪生成的认知差异，最后设计一个基于LLM的角色扮演代理来模拟个体心理链。", "result": "在标注的数据集上，PsychoAgent相比基线模型将恐慌情绪预测性能提高了12.6%至21.7%，并且验证了方法的可解释性和泛化能力。", "conclusion": "这代表了一种从“数据驱动拟合”到“基于角色的模拟与机制解释”的范式转变，为紧急情况下的恐慌情绪预测提供了透明的方法。"}}
{"id": "2505.16422", "title": "Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach", "authors": ["Xiaoran Yin", "Xu Luo", "Hao Wu", "Lianli Gao", "Jingkuan Song"], "abstract": "The automatic control of mobile devices is essential for efficiently performing complex tasks that involve multiple sequential steps. However, these tasks pose significant challenges due to the limited environmental information available at each step, primarily through visual observations. As a result, current approaches, which typically rely on reactive policies, focus solely on immediate observations and often lead to suboptimal decision-making. To address this problem, we propose \\textbf{Foresighted Planning with World Model-Driven Code Execution (FPWC)},a framework that prioritizes natural language understanding and structured reasoning to enhance the agent's global understanding of the environment by developing a task-oriented, refinable \\emph{world model} at the outset of the task. Foresighted actions are subsequently generated through iterative planning within this world model, executed in the form of executable code. Extensive experiments conducted in simulated environments and on real mobile devices demonstrate that our method outperforms previous approaches, particularly achieving a 44.4\\% relative improvement in task success rate compared to the state-of-the-art in the simulated environment. Code and demo are provided in the supplementary material.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16422.pdf", "abstract_url": "https://arxiv.org/abs/2505.16422", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为FPWC的框架，通过前瞻性规划和世界模型驱动的代码执行方法，提升移动设备自动控制的效率和准确性。", "motivation": "移动设备的自动控制在执行涉及多个顺序步骤的复杂任务时面临挑战，主要是因为每一步只能通过视觉观察获得有限的环境信息，导致当前依赖反应性策略的方法往往做出次优决策。", "method": "FPWC框架通过自然语言理解和结构化推理，在任务开始时建立一个可细化的面向任务的世界模型，然后在这个世界模型内通过迭代规划生成前瞻性动作，并以可执行代码的形式执行。", "result": "在模拟环境和真实移动设备上进行的广泛实验表明，该方法优于以往的方法，特别是在模拟环境中，任务成功率相比最先进技术提高了44.4%。", "conclusion": "FPWC框架通过增强代理对环境的全局理解和前瞻性规划，显著提高了移动设备自动控制的性能，为复杂任务的执行提供了更有效的解决方案。"}}
{"id": "2505.16232", "title": "MuseRAG: Idea Originality Scoring At Scale", "authors": ["Ali Sarosh Bangash", "Krish Veera", "Ishfat Abrar Islam", "Raiyan Abdul Baten"], "abstract": "An objective, face-valid way to assess the originality of creative ideas is to measure how rare each idea is within a population -- an approach long used in creativity research but difficult to automate at scale. Tabulating response frequencies via manual bucketing of idea rephrasings is labor-intensive, error-prone, and brittle under large corpora. We introduce a fully automated, psychometrically validated pipeline for frequency-based originality scoring. Our method, MuseRAG, combines large language models (LLMs) with an externally orchestrated retrieval-augmented generation (RAG) framework. Given a new idea, the system retrieves semantically similar prior idea buckets and zero-shot prompts the LLM to judge whether the new idea belongs to an existing bucket or forms a new one. The resulting buckets enable computation of frequency-based originality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG matches human annotators in idea clustering structure and resolution (AMI = 0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong convergent and external validity. Our work enables intent-sensitive, human-aligned originality scoring at scale to aid creativity research.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16232.pdf", "abstract_url": "https://arxiv.org/abs/2505.16232", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "MuseRAG是一种自动化、心理测量学验证的流程，用于基于频率的创意原创性评分，结合大型语言模型和检索增强生成框架，有效匹配人类注释者的评分。", "motivation": "解决传统手动评估创意想法原创性方法在大规模语料库下劳动密集、易出错且脆弱的问题。", "method": "结合大型语言模型(LLMs)和外部协调的检索增强生成(RAG)框架，自动检索语义相似的先前想法桶，并零射击提示LLM判断新想法是否属于现有桶或形成新桶。", "result": "在五个数据集上，MuseRAG在想法聚类结构和分辨率(AMI = 0.59)及参与者级别评分(r = 0.89)上与人类注释者匹配，同时展现出强大的收敛和外部效度。", "conclusion": "MuseRAG工作实现了意图敏感、与人类对齐的大规模原创性评分，以辅助创造力研究。"}}
{"id": "2505.16237", "title": "Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation", "authors": ["Derong Xu", "Pengyue Jia", "Xiaopeng Li", "Yingyi Zhang", "Maolin Wang", "Qidong Liu", "Xiangyu Zhao", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Enhong Chen", "Tong Xu"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but still struggle with issues like hallucinations and outdated information. Retrieval-augmented generation (RAG) addresses these issues by grounding LLM outputs in external knowledge with an Information Retrieval (IR) system. Building on this foundation, graph-based RAG systems go a step further by retrieving subgraphs, which preserve the relationships between knowledge entities and provide more comprehensive context. However, graph RAG faces two challenges: (1) Retrieving relevant information introduces irrelevant nodes (especially in dense graph databases, where retrieval usually extends to adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2) The representation gap between graph and language during generation with LLMs limits the ability to fully leverage graph structures for enhanced understanding. To address these limitations, we propose Align-GRAG, a novel reasoning-guided dual alignment framework in post-retrieval phrase. It first formulates a subgraph by retrieving nodes and edges. Then an Aligner is proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It achieves dual alignment of graph node and representation by leveraging KL divergence loss and contrastive loss, facilitating efficient pruning of irrelevant knowledge and establishing a unified semantic space. The Generator integrates the aligned graph data with LLM to produce coherent and accurate answers. Experiments on GraphQA benchmark across three tasks (including common sense reasoning, scene graph understanding, and knowledge graph reasoning) validate the effectiveness of our method. The code will be available upon accepted.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16237.pdf", "abstract_url": "https://arxiv.org/abs/2505.16237", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "Align-GRAG是一种新颖的推理引导双对齐框架，旨在解决图检索增强生成（RAG）系统中的两个主要挑战：检索相关信息和图与语言之间的表示差距。通过联合优化图编码器和LLM总结的推理，实现了图节点和表示的双对齐，从而提高了生成答案的准确性和连贯性。", "motivation": "大型语言模型（LLMs）虽然表现出色，但仍存在幻觉和过时信息的问题。图基础的RAG系统通过检索子图来提供更全面的上下文，但面临检索不相关节点和图与语言表示差距的挑战。", "method": "提出Align-GRAG框架，在检索后阶段通过KL散度损失和对比损失联合优化图编码器与LLM总结的推理，实现图节点和表示的双对齐，从而有效修剪不相关知识并建立统一的语义空间。", "result": "在GraphQA基准测试上的三个任务（包括常识推理、场景图理解和知识图谱推理）中验证了该方法的有效性。", "conclusion": "Align-GRAG通过推理引导的双对齐框架，有效解决了图RAG系统中的挑战，提高了生成答案的准确性和连贯性，为未来的研究提供了有价值的参考。"}}
{"id": "2505.16781", "title": "Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making", "authors": ["Qianlei Jia", "Xinliang Zhou", "Ondrej Krejcar", "Enrique Herrera-Viedma"], "abstract": "In group decision-making (GDM) scenarios, uncertainty, dynamic social structures, and vague information present major challenges for traditional opinion dynamics models. To address these issues, this study proposes a novel social network group decision-making (SNGDM) framework that integrates three-way decision (3WD) theory, dynamic network reconstruction, and linguistic opinion representation. First, the 3WD mechanism is introduced to explicitly model hesitation and ambiguity in agent judgments, thereby preventing irrational decisions. Second, a connection adjustment rule based on opinion similarity is developed, enabling agents to adaptively update their communication links and better reflect the evolving nature of social relationships. Third, linguistic terms are used to describe agent opinions, allowing the model to handle subjective, vague, or incomplete information more effectively. Finally, an integrated multi-agent decision-making framework is constructed, which simultaneously considers individual uncertainty, opinion evolution, and network dynamics. The proposed model is applied to a multi-UAV cooperative decision-making scenario, where simulation results and consensus analysis demonstrate its effectiveness. Experimental comparisons further verify the advantages of the algorithm in enhancing system stability and representing realistic decision-making behaviors.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16781.pdf", "abstract_url": "https://arxiv.org/abs/2505.16781", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本研究提出了一种新颖的社交网络群体决策框架，结合三支决策理论、动态网络重构和语言意见表示，以解决群体决策中的不确定性、动态社交结构和模糊信息问题。", "motivation": "传统意见动态模型在处理群体决策中的不确定性、动态社交结构和模糊信息时面临重大挑战。", "method": "引入三支决策机制明确模拟代理判断中的犹豫和模糊性，开发基于意见相似性的连接调整规则，使用语言术语描述代理意见，构建集成多代理决策框架。", "result": "模拟结果和共识分析证明了该模型的有效性，实验比较进一步验证了算法在增强系统稳定性和表示现实决策行为方面的优势。", "conclusion": "提出的模型能够有效处理群体决策中的不确定性、意见演化和网络动态，适用于多无人机协同决策等场景。"}}
{"id": "2505.16700", "title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "authors": ["Xuanqi Gao", "Siyi Xie", "Juan Zhai", "Shqing Ma", "Chao Shen"], "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of tool interaction, the Model Context Protocol (MCP) has emerged as a standardized framework for dynamic tool discovery and orchestration. Despite widespread industry adoption, existing evaluation methodologies fail to adequately assess tool utilization capabilities within this new paradigm. This paper introduces MCP-RADAR, the first comprehensive benchmark specifically designed to evaluate LLM performance in the MCP framework through a novel five-dimensional approach measuring: answer accuracy, tool selection efficiency, computational resource efficiency, parameter construction accuracy, and execution speed. Unlike conventional benchmarks that rely on subjective human evaluations or binary success metrics, MCP-RADAR employs objective, quantifiable measurements across multiple task domains including software engineering, mathematical reasoning, and general problem-solving. Our evaluations of leading commercial and open-source LLMs reveal distinctive capability profiles with significant trade-offs between accuracy, efficiency, and speed, challenging traditional single-metric performance rankings. Besides, we provide valuable guidance for developers to optimize their tools for maximum model compatibility and effectiveness. While focused on MCP due to its standardized approach, our methodology remains applicable across all LLM agent tool integration frameworks, providing valuable insights for both LLM developers and tool creators to optimize the entire LLM-tool interaction ecosystem. The implementation, configurations, and datasets used in our evaluation are publicly available at", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16700.pdf", "abstract_url": "https://arxiv.org/abs/2505.16700", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了MCP-RADAR，这是一个专门设计用于评估大型语言模型（LLMs）在模型上下文协议（MCP）框架下工具使用性能的综合基准测试。通过五个维度的新颖方法，包括答案准确性、工具选择效率、计算资源效率、参数构建准确性和执行速度，MCP-RADAR提供了客观、可量化的测量，覆盖了软件工程、数学推理和一般问题解决等多个任务领域。", "motivation": "随着大型语言模型（LLMs）从被动的文本生成器发展为能够进行工具交互的主动推理代理，现有的评估方法未能充分评估在这一新范式下的工具利用能力。", "method": "MCP-RADAR采用了一个五维度的评估方法，包括答案准确性、工具选择效率、计算资源效率、参数构建准确性和执行速度，通过客观、可量化的测量来评估LLMs在MCP框架下的性能。", "result": "对领先的商业和开源LLMs的评估揭示了在准确性、效率和速度之间存在显著的权衡，挑战了传统的单一指标性能排名。", "conclusion": "MCP-RADAR不仅为开发者提供了优化工具以实现最大模型兼容性和有效性的宝贵指导，而且其方法论适用于所有LLM代理工具集成框架，为LLM开发者和工具创造者优化整个LLM-工具交互生态系统提供了有价值的见解。"}}
{"id": "2505.16827", "title": "GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent", "authors": ["Bin Xie", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Jie Liu", "Min Zhang", "Liqiang Nie"], "abstract": "GUI automation faces critical challenges in dynamic environments. MLLMs suffer from two key issues: misinterpreting UI components and outdated knowledge. Traditional fine-tuning methods are costly for app-specific knowledge updates. We propose GUI-explorer, a training-free GUI agent that incorporates two fundamental mechanisms: (1) Autonomous Exploration of Function-aware Trajectory. To comprehensively cover all application functionalities, we design a Function-aware Task Goal Generator that automatically constructs exploration goals by analyzing GUI structural information (e.g., screenshots and activity hierarchies). This enables systematic exploration to collect diverse trajectories. (2) Unsupervised Mining of Transition-aware Knowledge. To establish precise screen-operation logic, we develop a Transition-aware Knowledge Extractor that extracts effective screen-operation logic through unsupervised analysis the state transition of structured interaction triples (observation, action, outcome). This eliminates the need for human involvement in knowledge extraction. With a task success rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant improvements over SOTA agents. It requires no parameter updates for new apps. GUI-explorer is open-sourced and publicly available at", "subjects": "Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2505.16827.pdf", "abstract_url": "https://arxiv.org/abs/2505.16827", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "GUI-explorer是一种无需训练的GUI代理，通过自主探索功能感知轨迹和无监督挖掘转换感知知识，解决了GUI自动化在动态环境中的关键挑战。", "motivation": "解决GUI自动化在动态环境中面临的挑战，特别是MLLMs在误解UI组件和知识过时方面的问题，以及传统微调方法在应用特定知识更新上的高成本。", "method": "提出了两种基本机制：功能感知任务目标生成器，通过分析GUI结构信息自动构建探索目标；转换感知知识提取器，通过无监督分析结构化交互三元组的状态转换来提取有效的屏幕操作逻辑。", "result": "在SPA-Bench和AndroidWorld上的任务成功率分别为53.7%和47.4%，显著优于现有技术代理，且无需为新应用更新参数。", "conclusion": "GUI-explorer通过自主探索和无监督知识挖掘，有效提高了GUI自动化的效率和准确性，且无需人工参与知识提取或参数更新，具有广泛的应用潜力。"}}
{"id": "2505.16281", "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "abstract": "The advancement of Large Language Models (LLMs) enables flexible and interpretable automatic evaluations. In the field of machine translation evaluation, utilizing LLMs with translation error annotations based on Multidimensional Quality Metrics (MQM) yields more human-aligned judgments. However, current LLM-based evaluation methods still face challenges in accurately identifying error spans and assessing their severity. In this paper, we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation Evaluation. We argue that existing approaches inadequately exploit the fine-grained structural and semantic information within the MQM hierarchy. To address this, we develop a hierarchical multi-agent system grounded in the MQM error typology, enabling granular evaluation of subtype errors. Two key strategies are incorporated to further mitigate systemic hallucinations within the framework: the utilization of the model's self-reflection capability and the facilitation of agent discussion involving asymmetric information. Empirically, HiMATE outperforms competitive baselines across different datasets in conducting human-aligned evaluations. Further analyses underscore its significant advantage in error span detection and severity assessment, achieving an average F1-score improvement of 89% over the best-performing baseline. We make our code and data publicly available at", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16281.pdf", "abstract_url": "https://arxiv.org/abs/2505.16281", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "HiMATE是一个基于层次多代理框架的机器翻译评估方法，旨在通过利用MQM错误类型学的细粒度结构信息，提高翻译错误识别和严重性评估的准确性。", "motivation": "当前基于大型语言模型（LLMs）的机器翻译评估方法在准确识别错误跨度和评估其严重性方面仍面临挑战。", "method": "提出HiMATE框架，采用层次多代理系统，结合MQM错误类型学，实现子类型错误的细粒度评估，并利用模型自反能力和代理讨论来减少系统幻觉。", "result": "HiMATE在不同数据集上优于竞争基线，特别是在错误跨度检测和严重性评估方面，平均F1分数比最佳基线提高了89%。", "conclusion": "HiMATE框架通过层次多代理系统和策略优化，显著提高了机器翻译评估的准确性和人类对齐度，为相关领域提供了有价值的工具和数据。"}}
{"id": "2505.16293", "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA", "authors": ["Rishabh Maheshwary", "Masoud Hashemi", "Khyati Mahajan", "Shiva Krishna Reddy Malay", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Spandana Gella", "Vikas Yadav"], "abstract": "Iterative RAG for multi-hop question answering faces challenges with lengthy contexts and the buildup of irrelevant information. This hinders a model's capacity to process and reason over retrieved content and limits performance. While recent methods focus on compressing retrieved information, they are either restricted to single-round RAG, require finetuning or lack scalability in iterative RAG. To address these challenges, we propose Notes Writing, a method that generates concise and relevant notes from retrieved documents at each step, thereby reducing noise and retaining only essential information. This indirectly increases the effective context length of Large Language Models (LLMs), enabling them to reason and plan more effectively while processing larger volumes of input text. Notes Writing is framework agnostic and can be integrated with different iterative RAG methods. We demonstrate its effectiveness with three iterative RAG methods, across two models and four evaluation datasets. Notes writing yields an average improvement of 15.6 percentage points overall, with minimal increase in output tokens.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16293.pdf", "abstract_url": "https://arxiv.org/abs/2505.16293", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出了一种名为‘笔记写作’的方法，用于在迭代RAG中生成简洁相关的笔记，以减少噪音并保留关键信息，从而提高大型语言模型在复杂问答中的推理能力。", "motivation": "解决迭代RAG在多跳问答中面临的上下文过长和无关信息积累问题，这些问题限制了模型处理检索内容的能力和性能。", "method": "提出‘笔记写作’方法，在每一步从检索到的文档中生成简洁相关的笔记，减少噪音并保留关键信息。", "result": "在三种迭代RAG方法、两个模型和四个评估数据集上，‘笔记写作’平均提高了15.6个百分点，且输出标记的增加最小。", "conclusion": "‘笔记写作’是一种框架无关的方法，可以集成到不同的迭代RAG方法中，有效提高大型语言模型在复杂问答中的推理和规划能力。"}}
{"id": "2505.16348", "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance", "authors": ["Taeyoon Kwon", "Dongwook Choi", "Sunghwan Kim", "Hyojun Kim", "Seungjun Moon", "Beong-woo Kwak", "Kuan-Hao Huang", "Jinyoung Yeo"], "abstract": "Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website:", "subjects": "Computation and Language (cs.CL)", "comments": "Work in progress", "pdf_url": "https://arxiv.org/pdf/2505.16348.pdf", "abstract_url": "https://arxiv.org/abs/2505.16348", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了MEMENTO，一个评估个性化具身代理记忆利用能力的框架，旨在通过两阶段记忆评估过程量化记忆利用对任务性能的影响，揭示了当前大型语言模型在记忆利用上的局限性。", "motivation": "解决具身代理在提供个性化辅助时，如何利用记忆理解用户独特语义和模式的问题，当前研究在这一领域的探索不足。", "method": "提出MEMENTO框架，通过两阶段记忆评估过程设计，评估代理在目标对象识别和用户模式推断中的记忆利用能力。", "result": "实验显示，即使是GPT-4o这样的前沿模型，在需要参考多个记忆时性能下降30.5%，特别是在涉及用户模式的任务中表现更差。", "conclusion": "研究结果为未来开发更有效的个性化具身代理提供了宝贵见解，强调了记忆利用在个性化辅助中的重要性及其当前局限性。"}}
{"id": "2505.16349", "title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization", "authors": ["Pierre Achkar", "Tim Gollub", "Martin Potthast"], "abstract": "The exponential growth of scientific publications has made it increasingly difficult for researchers to stay updated and synthesize knowledge effectively. This paper presents XSum, a modular pipeline for multi-document summarization (MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The pipeline includes two core components: a question-generation module and an editor module. The question-generation module dynamically generates questions adapted to the input papers, ensuring the retrieval of relevant and accurate information. The editor module synthesizes the retrieved content into coherent and well-structured summaries that adhere to academic standards for proper citation. Evaluated on the SurveySum dataset, XSum demonstrates strong performance, achieving considerable improvements in metrics such as CheckEval, G-Eval and Ref-F1 compared to existing approaches. This work provides a transparent, adaptable framework for scientific summarization with potential applications in a wide range of domains. Code available at", "subjects": "Computation and Language (cs.CL)", "comments": "Accepted at SCOLIA@ECIR 2025 Workshop", "pdf_url": "https://arxiv.org/pdf/2505.16349.pdf", "abstract_url": "https://arxiv.org/abs/2505.16349", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了XSum，一个用于科学领域多文档摘要（MDS）的模块化管道，采用检索增强生成（RAG）技术，包括问题生成模块和编辑模块，旨在有效合成知识并保持学术引用标准。", "motivation": "科学出版物的指数增长使得研究人员难以有效更新和合成知识。", "method": "使用检索增强生成（RAG）技术的模块化管道，包括动态生成问题的问题生成模块和合成检索内容的编辑模块。", "result": "在SurveySum数据集上评估，XSum在CheckEval、G-Eval和Ref-F1等指标上表现出色，优于现有方法。", "conclusion": "XSum提供了一个透明、适应性强的科学摘要框架，具有广泛的应用潜力。"}}
{"id": "2505.16899", "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships", "authors": ["Kerem Oktar", "Katherine M. Collins", "Jose Hernandez-Orallo", "Diane Coyle", "Stephen Cave", "Adrian Weller", "Ilia Sucholutsky"], "abstract": "Artificial Intelligence (AI) systems have historically been used as tools that execute narrowly defined tasks. Yet recent advances in AI have unlocked possibilities for a new class of models that genuinely collaborate with humans in complex reasoning, from conceptualizing problems to brainstorming solutions. Such AI thought partners enable novel forms of collaboration and extended cognition, yet they also pose major risks-including and beyond risks of typical AI tools and agents. In this commentary, we systematically identify risks of AI thought partners through a novel framework that identifies risks at multiple levels of analysis, including Real-time, Individual, and Societal risks arising from collaborative cognition (RISc). We leverage this framework to propose concrete metrics for risk evaluation, and finally suggest specific mitigation strategies for developers and policymakers. As AI thought partners continue to proliferate, these strategies can help prevent major harms and ensure that humans actively benefit from productive thought partnerships.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16899.pdf", "abstract_url": "https://arxiv.org/abs/2505.16899", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了AI思维伙伴的风险，提出了一个多层面分析框架（RISc）来识别、评估和缓解这些风险，并为开发者和政策制定者提供了具体的缓解策略。", "motivation": "随着AI技术的发展，AI系统不再仅限于执行狭义定义的任务，而是能够与人类在复杂推理中进行真正合作。这种AI思维伙伴虽然开启了新的合作形式，但也带来了重大风险，包括但不限于典型AI工具和代理的风险。", "method": "通过一个新颖的多层面分析框架（RISc），系统地识别AI思维伙伴在实时、个人和社会层面上由协作认知产生的风险，并利用该框架提出具体的风险评估指标。", "result": "提出了具体的风险缓解策略，旨在帮助开发者和政策制定者预防重大危害，确保人类从富有成效的思维伙伴关系中积极受益。", "conclusion": "随着AI思维伙伴的不断增多，采取这些策略可以有效防止重大危害，确保人类能够从中获得积极利益。"}}
{"id": "2505.16832", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "abstract": "While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "comments": "16 pages; 7 figures", "pdf_url": "https://arxiv.org/pdf/2505.16832.pdf", "abstract_url": "https://arxiv.org/abs/2505.16832", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了EduVisBench基准和EduVisAgent多智能体框架，旨在评估和提升基础模型在教育可视化中的表现。通过多领域、多层次的基准测试和多智能体协作框架，显著提高了教育可视化效果。", "motivation": "基础模型在教育环境中的应用虽广，但在生成教育有效的视觉解释方面能力有限。现有方法多集中于文本推理，忽视了结构化、可解释的可视化在支持概念理解中的关键作用。", "method": "提出了EduVisBench，一个多领域、多层次的基准，以及EduVisAgent，一个多智能体协作框架，包括教学规划、推理分解、元认知提示和可视化设计等专门智能体的协调。", "result": "实验结果显示，EduVisAgent显著优于所有基线方法，实现了40.2%的改进，并提供了更符合教育需求的可视化。", "conclusion": "EduVisBench和EduVisAgent为评估和提升基础模型在教育可视化中的表现提供了有效的工具和方法，推动了教育可视化技术的发展。"}}
{"id": "2505.16928", "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "authors": ["Bosung Kim", "Prithviraj Ammanabrolu"], "abstract": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16928.pdf", "abstract_url": "https://arxiv.org/abs/2505.16928", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.16938", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2505.16938.pdf", "abstract_url": "https://arxiv.org/abs/2505.16938", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "NovelSeek是一个统一的闭环多智能体框架，用于跨多个科学研究领域进行自主科学研究，展示了在12个科学研究任务中的可扩展性、交互性和效率。", "motivation": "解决科学研究中复杂问题的速度和精度问题，通过AI加速科研范式的转变。", "method": "采用闭环多智能体框架，支持人类专家反馈和多智能体交互，实现端到端的自动化流程。", "result": "在多个科学领域实现了性能提升，如反应收率预测从27.6%提高到35.4%，增强子活性预测准确率从0.52提高到0.79，2D语义分割精度从78.8%提高到81.0%。", "conclusion": "NovelSeek框架通过其可扩展性、交互性和效率，为科学研究提供了新的可能性，显著提高了研究效率和创新性。"}}
{"id": "2505.16944", "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16944.pdf", "abstract_url": "https://arxiv.org/abs/2505.16944", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了AgentIF，首个系统评估大型语言模型（LLMs）在代理场景中指令跟随能力的基准。AgentIF基于50个真实世界代理应用构建，具有长且复杂的指令特点，并包含多种约束类型。通过评估现有先进LLMs，发现它们在处理复杂约束结构和工具规范时表现不佳。", "motivation": "随着LLM在代理应用中的能力展示，如何评估其在涉及长指令和复杂约束的代理场景中的指令跟随能力成为一个未充分探索的挑战。", "method": "构建AgentIF基准，包含707条人工标注的指令，覆盖50个代理任务，每条指令平均1723字，最多15630字，平均11.9个约束。采用代码、LLM及混合方法进行评估。", "result": "现有LLMs在AgentIF上普遍表现不佳，特别是在处理复杂约束结构和工具规范时。错误分析和实验揭示了现有模型的失败模式。", "conclusion": "AgentIF为未来研究提供了评估LLMs在代理场景中指令跟随能力的重要工具，揭示了当前模型的局限性，并促进了进一步的研究和改进。"}}
{"id": "2505.16979", "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design", "authors": ["Zhenkun Li", "Lingyao Li", "Shuhang Lin", "Yongfeng Zhang"], "abstract": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle domain transfer. Conventional multi-agent fixes soften those edges yet expose fresh pains: ill-posed decompositions, fuzzy contracts, and verification overhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a framework that converts domain priors into an algorithmic blueprint hierarchy, in which tasks are recursively split into typed, controller-mediated subtasks, each solved zero-shot or with the lightest viable boost (e.g., chain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch theorem, KtR trades the chase for a universal prompt for disciplined decomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents raise accuracy from 3% zero-shot to 95% on size-5 instances after patching a single bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a six-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15, versus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation thus turns modest models into reliable collaborators--no ever-larger monoliths required.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16979.pdf", "abstract_url": "https://arxiv.org/abs/2505.16979", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了Know-The-Ropes (KtR)框架，通过将领域先验知识转化为算法蓝图层次结构，解决单代理LLM的局限性及多代理系统中的新问题。KtR通过递归分解任务并针对性增强，使中等模型成为可靠协作伙伴。", "motivation": "单代理LLM存在有限上下文、角色过载和脆弱的领域转移等硬限制，而传统的多代理解决方案虽缓解了这些问题，却引入了新的挑战，如不明确的分解、模糊的合约和验证开销。本文旨在解决这些挑战。", "method": "提出Know-The-Ropes (KtR)框架，将领域先验知识转化为算法蓝图层次结构，递归分解任务为类型化、控制器介导的子任务，每个子任务零射解决或使用最轻量级的增强方法（如思维链、微调、自检）。", "result": "在背包问题（3-8项）上，三个GPT-4o-mini代理将准确率从零射的3%提升到大小5实例的95%。在更难的Task-Assignment问题（6-15个作业）上，六代理o3-mini蓝图在大小10及以下达到100%准确率，大小13-15达到84%，而零射仅为11%。", "conclusion": "算法感知的分解加针对性增强可以将中等模型转变为可靠的协作伙伴，无需越来越大的单体模型。"}}
{"id": "2505.16982", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "abstract": "Large Language Models (LLMs) show promise in biomedicine but lack true causal understanding, relying instead on correlations. This paper envisions causal LLM agents that integrate multimodal data (text, images, genomics, etc.) and perform intervention-based reasoning to infer cause-and-effect. Addressing this requires overcoming key challenges: designing safe, controllable agentic frameworks; developing rigorous benchmarks for causal evaluation; integrating heterogeneous data sources; and synergistically combining LLMs with structured knowledge (KGs) and formal causal inference tools. Such agents could unlock transformative opportunities, including accelerating drug discovery through automated hypothesis generation and simulation, enabling personalized medicine through patient-specific causal models. This research agenda aims to foster interdisciplinary efforts, bridging causal concepts and foundation models to develop reliable AI partners for biomedical progress.", "subjects": "Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16982.pdf", "abstract_url": "https://arxiv.org/abs/2505.16982", "categories": ["Artificial Intelligence (cs.AI)", "Medical Physics (physics.med-ph)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文探讨了在生物医学领域超越相关性的因果大型语言模型（LLM）代理的潜力，提出了整合多模态数据和干预推理以实现因果理解的愿景。", "motivation": "解决大型语言模型在生物医学应用中缺乏真正因果理解，仅依赖相关性的问题。", "method": "设计安全可控的代理框架，开发严格的因果评估基准，整合异构数据源，以及结合LLMs与结构化知识（KGs）和正式因果推理工具。", "result": "提出了因果LLM代理的概念框架，旨在加速药物发现和实现个性化医疗。", "conclusion": "通过跨学科努力，结合因果概念和基础模型，开发可靠的AI伙伴以促进生物医学进步。"}}
{"id": "2505.16997", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by enabling cooperation among multiple specialized agents. However, most existing MAS frameworks rely on a single LLM to drive all agents, constraining the system's intelligence to the limit of that model. This paper explores the paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by diverse LLMs, elevating the system's potential to the collective intelligence of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to evaluate the performance of various LLMs across different domains and MAS-related functions. As an extensive empirical study, we assess 27 LLMs across 5 domains (encompassing 21 test sets) and 5 functions, conducting over 1.7 million evaluations to identify optimal model selections for each domain-function combination. Building on these findings, we demonstrate that transitioning from homogeneous to heterogeneous LLM-driven MAS can significantly enhance system performance without requiring structural redesign. Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration yields up to 8.4\\% performance improvement on the MATH dataset. In a mixed chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable 47\\% performance boost on the AIME dataset. Our results underscore the transformative potential of heterogeneous LLMs in MAS, highlighting a promising avenue for advancing scalable, collaborative AI systems.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)", "comments": "19 pages, 5 figures", "pdf_url": "https://arxiv.org/pdf/2505.16997.pdf", "abstract_url": "https://arxiv.org/abs/2505.16997", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了基于异构LLM驱动的多智能体系统（X-MAS），通过引入X-MAS-Bench测试平台，评估了27种LLM在5个领域的性能，证明了异构配置能显著提升系统性能。", "motivation": "现有基于LLM的多智能体系统大多依赖单一LLM驱动所有智能体，限制了系统的智能水平。本文旨在探索异构LLM驱动的多智能体系统，以提升系统的集体智能。", "method": "引入X-MAS-Bench测试平台，评估27种LLM在5个领域（涵盖21个测试集）和5种功能上的表现，进行了超过170万次评估，以确定每个领域-功能组合的最佳模型选择。", "result": "在仅聊天机器人的多智能体系统场景中，异构配置在MATH数据集上实现了8.4%的性能提升；在混合聊天机器人-推理器场景中，异构多智能体系统在AIME数据集上实现了47%的性能提升。", "conclusion": "异构LLM在多智能体系统中具有变革性潜力，为推进可扩展、协作的AI系统提供了有前景的途径。"}}
{"id": "2505.16663", "title": "CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation", "authors": ["Haihong Hao", "Mingfei Han", "Changlin Li", "Zhihui Li", "Xiaojun Chang"], "abstract": "Embodied navigation demands comprehensive scene understanding and precise spatial reasoning. While image-text models excel at interpreting pixel-level color and lighting cues, 3D-text models capture volumetric structure and spatial relationships. However, unified fusion approaches that jointly fuse 2D images, 3D point clouds, and textual instructions face challenges in limited availability of triple-modality data and difficulty resolving conflicting beliefs among modalities. In this work, we introduce CoNav, a collaborative cross-modal reasoning framework where a pretrained 3D-text model explicitly guides an image-text navigation agent by providing structured spatial-semantic knowledge to resolve ambiguities during navigation. Specifically, we introduce Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance by simply sharing textual hypotheses from the 3D-text model to the navigation agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the navigation agent learns to integrate visual cues with spatial-semantic knowledge derived from the 3D-text model, enabling effective reasoning in embodied navigation. CoNav achieves significant improvements on four standard embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success Rate, CoNav often generates shorter paths compared to other methods (as measured by SPL), showcasing the potential and challenges of fusing data from different modalities in embodied navigation. Project Page:", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16663.pdf", "abstract_url": "https://arxiv.org/abs/2505.16663", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Multimedia (cs.MM)"], "matching_keywords": ["agent"], "AI": {"tldr": "CoNav是一个协作跨模态推理框架，通过预训练的3D文本模型指导图像文本导航代理，解决导航中的模糊性问题，显著提升了多个导航和空间推理基准的性能。", "motivation": "解决在具身导航中统一融合2D图像、3D点云和文本指令时面临的多模态数据有限和模态间信念冲突的挑战。", "method": "引入跨模态信念对齐，通过共享3D文本模型的文本假设给导航代理，轻量级微调小规模2D-3D-文本语料库，使导航代理能够整合视觉线索和空间语义知识。", "result": "在四个标准具身导航基准（R2R、CVDN、REVERIE、SOON）和两个空间推理基准（ScanQA、SQA3D）上取得显著改进，且在导航成功率下常生成更短路径。", "conclusion": "CoNav展示了在具身导航中融合不同模态数据的潜力和挑战，为跨模态推理提供了有效解决方案。"}}
{"id": "2505.16415", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "abstract": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "Work in process", "pdf_url": "https://arxiv.org/pdf/2505.16415.pdf", "abstract_url": "https://arxiv.org/abs/2505.16415", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了一种基于Jensen-Shannon散度的新方法ARC-JSD，用于在检索增强生成（RAG）中高效准确地识别关键上下文句子，无需额外微调或代理建模。", "motivation": "解决在检索增强生成（RAG）中，由于当前方法计算密集且常需大量微调或人工标注，可靠地将生成内容归因于特定上下文段落的挑战。", "method": "提出了一种新颖的Jensen-Shannon Divergence驱动的方法ARC-JSD，用于无需额外微调或代理建模的情况下，高效准确地识别关键上下文句子。", "result": "在多种RAG基准测试（如TyDi QA、Hotpot QA和Musique）上的评估显示，与之前的基于代理的方法相比，ARC-JSD在准确性和计算效率上均有显著提升。", "conclusion": "ARC-JSD方法不仅提高了RAG模型的性能，还通过机制分析揭示了负责上下文归因的特定注意力头和MLP层，为理解RAG模型的内部工作机制提供了宝贵见解。"}}
{"id": "2505.16429", "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "abstract": "Evaluating and iterating upon recommender systems is crucial, yet traditional A/B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16429.pdf", "abstract_url": "https://arxiv.org/abs/2505.16429", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了RecInter，一个新颖的基于代理的推荐系统模拟平台，通过强大的交互机制动态更新项目属性，提高模拟的真实性。", "motivation": "解决传统A/B测试资源密集和离线方法难以处理动态用户-平台交互的问题。", "method": "引入RecInter平台，采用多维用户画像模块、高级代理架构和基于链式思维（CoT）增强的交互数据微调的LLM。", "result": "平台显著提高了模拟的可信度，并成功复制了品牌忠诚度和马太效应等涌现现象。", "conclusion": "RecInter平台作为一个可信的推荐系统研究测试床，其交互机制对于模拟现实系统演化至关重要。"}}
{"id": "2505.16421", "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "abstract": "While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.", "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": "Preprint", "pdf_url": "https://arxiv.org/pdf/2505.16421.pdf", "abstract_url": "https://arxiv.org/abs/2505.16421", "categories": ["Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "WebAgent-R1是一个简单有效的端到端多轮强化学习框架，用于训练网络代理。它通过异步生成多样化的轨迹，直接从与网络环境的在线互动中学习，完全由任务成功的二元奖励指导。在WebArena-Lite基准测试中，WebAgent-R1显著提高了任务成功率，优于现有最先进方法和强大的专有模型。", "motivation": "尽管强化学习在增强大型语言模型方面取得了显著成功，但它主要集中于单轮任务，如解决数学问题。训练有效的网络代理进行多轮互动仍然具有挑战性，因为需要在动态网络界面中进行长视野决策。", "method": "WebAgent-R1是一个端到端的多轮强化学习框架，通过异步生成多样化的轨迹，直接从与网络环境的在线互动中学习，完全由任务成功的二元奖励指导。", "result": "在WebArena-Lite基准测试中，WebAgent-R1将Qwen-2.5-3B的任务成功率从6.1%提高到33.9%，Llama-3.1-8B从8.5%提高到44.8%，显著优于现有最先进方法和强大的专有模型。", "conclusion": "WebAgent-R1通过思维基础的提示策略和通过增加互动进行测试时间扩展，展示了其在网络任务中的有效性。此外，通过引入WebAgent-R1-Zero和WebAgent-R1-CoT两个变体，研究了不同的强化学习初始化策略，强调了热身训练阶段的重要性，并提供了在网络代理中融入长链思维推理的见解。"}}
{"id": "2505.15836", "title": "Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning", "authors": ["Aarav Lala", "Kalyan Cherukuri"], "abstract": "As artificial intelligence continues to drive innovation in complex, decentralized environments, the need for scalable, adaptive, and privacy-preserving decision-making systems has become critical. This paper introduces a novel framework combining quantum-inspired neural networks with evolutionary algorithms to optimize real-time decision-making in multi-agent systems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN) leverages quantum computing principles -- such as quantum superposition and entanglement -- to enhance learning speed and decision accuracy, while integrating evolutionary optimization to continually refine agent behaviors in dynamic, uncertain environments. By utilizing federated learning, QE-NN ensures privacy preservation, enabling decentralized agents to collaborate without sharing sensitive data. The framework is designed to allow agents to adapt in real-time to their environments, optimizing decision-making processes for applications in areas such as autonomous systems, smart cities, and healthcare. This research represents a breakthrough in merging quantum computing, evolutionary optimization, and privacy-preserving techniques to solve complex problems in multi-agent decision-making systems, pushing the boundaries of AI in real-world, privacy-sensitive applications.", "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.15836.pdf", "abstract_url": "https://arxiv.org/abs/2505.15836", "categories": ["Neural and Evolutionary Computing (cs.NE)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种结合量子启发神经网络和进化算法的新框架QE-NN，用于优化多智能体系统中的实时决策，同时通过联邦学习确保隐私保护。", "motivation": "解决在复杂、去中心化环境中需要可扩展、自适应且保护隐私的决策系统的问题。", "method": "利用量子计算原理（如量子叠加和纠缠）增强学习速度和决策准确性，结合进化优化不断优化动态不确定环境中的智能体行为，并通过联邦学习实现隐私保护。", "result": "QE-NN框架在自主系统、智能城市和医疗保健等领域的多智能体决策系统中实现了突破。", "conclusion": "本研究在量子计算、进化优化和隐私保护技术的融合方面取得了突破，推动了AI在现实世界隐私敏感应用中的边界。"}}
{"id": "2505.15859", "title": "AutoData: A Multi-Agent System for Open Web Data Collection", "authors": ["Tianyi Ma", "Yiyue Qian", "Zheyuan Zhang", "Zehong Wang", "Xiaoye Qian", "Feifan Bai", "Yifan Ding", "Xuwei Luo", "Shinan Zhang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "abstract": "The exponential growth of data-driven systems and AI technologies has intensified the demand for high-quality web-sourced datasets. While existing datasets have proven valuable, conventional web data collection approaches face significant limitations in terms of human effort and scalability. Current data-collecting solutions fall into two categories: wrapper-based methods that struggle with adaptability and reproducibility, and large language model (LLM)-based approaches that incur substantial computational and financial costs. To address these challenges, we propose AutoData, a novel multi-agent system for Automated web Data collection, that requires minimal human intervention, i.e., only necessitating a natural language instruction specifying the desired dataset. In addition, AutoData is designed with a robust multi-agent architecture, featuring a novel oriented message hypergraph coordinated by a central task manager, to efficiently organize agents across research and development squads. Besides, we introduce a novel hypergraph cache system to advance the multi-agent collaboration process that enables efficient automated data collection and mitigates the token cost issues prevalent in existing LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark dataset supporting live data collection from web sources across three domains: academic, finance, and sports. Comprehensive evaluations over Instruct2DS and three existing benchmark datasets demonstrate AutoData's superior performance compared to baseline methods. Case studies on challenging tasks such as picture book collection and paper extraction from surveys further validate its applicability. Our source code and dataset are available at", "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.15859.pdf", "abstract_url": "https://arxiv.org/abs/2505.15859", "categories": ["Information Retrieval (cs.IR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "AutoData是一个多代理系统，旨在通过最小化人工干预自动收集网络数据，仅需自然语言指令指定所需数据集。", "motivation": "解决现有网络数据收集方法在适应性和可扩展性方面的限制，以及基于大型语言模型(LLM)方法的高计算和财务成本问题。", "method": "采用多代理架构，包括由中央任务管理器协调的定向消息超图和超图缓存系统，以提高多代理协作效率。", "result": "在Instruct2DS和三个现有基准数据集上的综合评估显示，AutoData性能优于基线方法。", "conclusion": "AutoData通过其创新的多代理系统和超图缓存技术，有效实现了高质量网络数据的自动化收集，降低了成本，拓宽了应用范围。"}}
{"id": "2505.16518", "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "authors": ["Lovisa Hagström", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "abstract": "Incorporating external knowledge is crucial for knowledge-intensive tasks, such as question answering and fact checking. However, language models (LMs) may ignore relevant information that contradicts outdated parametric memory or be distracted by irrelevant contexts. While many context utilisation manipulation techniques (CMTs) that encourage or suppress context utilisation have recently been proposed to alleviate these issues, few have seen systematic comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to help practitioners within retrieval-augmented generation (RAG) identify the best CMT for their needs. CUB allows for rigorous testing on three distinct context types, observed to capture key challenges in realistic context utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art methods, representative of the main categories of CMTs, across three diverse datasets and tasks, applied to nine LMs. Our results show that most of the existing CMTs struggle to handle the full set of types of contexts that may be encountered in real-world retrieval-augmented scenarios. Moreover, we find that many CMTs display an inflated performance on simple synthesised datasets, compared to more realistic datasets with naturally occurring samples. Altogether, our results show the need for holistic tests of CMTs and the development of CMTs that can handle multiple context types.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "27 pages", "pdf_url": "https://arxiv.org/pdf/2505.16518.pdf", "abstract_url": "https://arxiv.org/abs/2505.16518", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了CUB（Context Utilisation Benchmark），一个用于评估语言模型在知识密集型任务中利用外部上下文技术的基准测试。通过测试七种最先进的方法，研究发现现有技术在处理真实世界检索增强场景中的多样化上下文时存在挑战。", "motivation": "解决语言模型在处理知识密集型任务时可能忽略相关或受无关上下文干扰的问题，以及现有上下文利用技术缺乏系统比较的现状。", "method": "开发了CUB基准测试，对七种代表不同类别的上下文利用技术（CMTs）在三种不同的上下文类型、三个数据集和任务上进行了评估，应用于九种语言模型。", "result": "大多数现有的CMTs难以处理真实世界检索增强场景中可能遇到的所有上下文类型，且许多CMTs在简单合成数据集上的表现优于自然发生样本的更现实数据集。", "conclusion": "研究表明需要对CMTs进行全面的测试，并开发能够处理多种上下文类型的CMTs，以满足检索增强生成（RAG）实践者的需求。"}}
{"id": "2505.16902", "title": "RealEngine: Simulating Autonomous Driving in Realistic Context", "authors": ["Junzhe Jiang", "Nan Song", "Jingyu Li", "Xiatian Zhu", "Li Zhang"], "abstract": "Driving simulation plays a crucial role in developing reliable driving agents by providing controlled, evaluative environments. To enable meaningful assessments, a high-quality driving simulator must satisfy several key requirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with realistic scene rendering to minimize observational discrepancies; closed-loop evaluation to support free-form trajectory behaviors; highly diverse traffic scenarios for thorough evaluation; multi-agent cooperation to capture interaction dynamics; and high computational efficiency to ensure affordability and scalability. However, existing simulators and benchmarks fail to comprehensively meet these fundamental criteria. To bridge this gap, this paper introduces RealEngine, a novel driving simulation framework that holistically integrates 3D scene reconstruction and novel view synthesis techniques to achieve realistic and flexible closed-loop simulation in the driving context. By leveraging real-world multi-modal sensor data, RealEngine reconstructs background scenes and foreground traffic participants separately, allowing for highly diverse and realistic traffic scenarios through flexible scene composition. This synergistic fusion of scene reconstruction and view synthesis enables photorealistic rendering across multiple sensor modalities, ensuring both perceptual fidelity and geometric accuracy. Building upon this environment, RealEngine supports three essential driving simulation categories: non-reactive simulation, safety testing, and multi-agent interaction, collectively forming a reliable and comprehensive benchmark for evaluating the real-world performance of driving agents.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16902.pdf", "abstract_url": "https://arxiv.org/abs/2505.16902", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "RealEngine是一个新型的驾驶模拟框架，通过整合3D场景重建和新视角合成技术，实现了在驾驶背景下的现实和灵活的闭环模拟。它利用真实世界的多模态传感器数据，分别重建背景场景和前景交通参与者，通过灵活的场景组合实现高度多样化和真实的交通场景。", "motivation": "现有的模拟器和基准测试未能全面满足高质量驾驶模拟器的关键要求，如多模态感知能力、闭环评估、高度多样化的交通场景、多代理合作以及高计算效率。RealEngine旨在填补这一空白。", "method": "RealEngine通过结合3D场景重建和新视角合成技术，利用真实世界的多模态传感器数据，分别重建背景场景和前景交通参与者，实现高度多样化和真实的交通场景。", "result": "RealEngine支持三种基本的驾驶模拟类别：非反应性模拟、安全测试和多代理交互，共同构成了一个可靠和全面的基准，用于评估驾驶代理在现实世界中的性能。", "conclusion": "RealEngine通过其创新的方法，为驾驶代理的开发和评估提供了一个高度现实、灵活和全面的模拟环境，有望推动自动驾驶技术的发展。"}}
{"id": "2505.16576", "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions", "authors": ["Spencer Hong", "Meng Luo", "Xinyi Wan"], "abstract": "Determining the veracity of atomic claims is an imperative component of many recently proposed fact-checking systems. Many approaches tackle this problem by first retrieving evidence by querying a search engine and then performing classification by providing the evidence set and atomic claim to a large language model, but this process deviates from what a human would do in order to perform the task. Recent work attempted to address this issue by proposing iterative evidence retrieval, allowing for evidence to be collected several times and only when necessary. Continuing along this line of research, we propose a novel claim verification system, called EMULATE, which is designed to better emulate human actions through the use of a multi-agent framework where each agent performs a small part of the larger task, such as ranking search results according to predefined criteria or evaluating webpage content. Extensive experiments on several benchmarks show clear improvements over prior work, demonstrating the efficacy of our new multi-agent framework.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16576.pdf", "abstract_url": "https://arxiv.org/abs/2505.16576", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "EMULATE是一个多智能体框架，旨在通过模拟人类行为来确定原子声明的真实性，相比之前的工作，它在多个基准测试中显示出明显的改进。", "motivation": "解决现有事实核查系统中确定原子声明真实性方法的不足，这些方法通常通过检索证据并使用大型语言模型进行分类，但这一过程与人类执行任务的方式不同。", "method": "提出一个名为EMULATE的新型声明验证系统，采用多智能体框架，每个智能体执行大任务中的一小部分，如根据预定义标准排名搜索结果或评估网页内容。", "result": "在多个基准测试上的广泛实验显示，EMULATE相比之前的工作有显著改进，证明了新多智能体框架的有效性。", "conclusion": "EMULATE通过更接近人类行为的多智能体框架，提高了确定原子声明真实性的准确性和效率，为事实核查系统的发展提供了新的方向。"}}
{"id": "2505.16582", "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "abstract": "Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O$^2$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from model's sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O$^2$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O$^2$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "25 pages, 9 figures", "pdf_url": "https://arxiv.org/pdf/2505.16582.pdf", "abstract_url": "https://arxiv.org/abs/2505.16582", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "O$^2$-Searcher是一种基于搜索的代理模型，用于开放领域的开放式问答，通过强化学习和本地模拟搜索环境有效解决开放和封闭式问题。", "motivation": "大型语言模型（LLMs）由于静态参数知识的限制，在处理需要开放领域最新信息的任务时表现不佳，尤其是在开放式问题上。", "method": "利用强化学习构建O$^2$-Searcher代理，通过本地模拟搜索环境动态获取知识，并设计精细的奖励函数来适应不同类型的问题和答案生成策略。", "result": "O$^2$-Searcher在O$^2$-QA基准测试中显著超越领先的LLM代理，并在多种封闭式QA基准测试中达到最先进水平。", "conclusion": "O$^2$-Searcher展示了在处理开放和封闭式问题上的高效能力，尤其是在资源有限的情况下，与更大模型表现相当。"}}
{"id": "2505.16834", "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language models (LLMs) in complex deep search scenarios requiring multi-step reasoning and iterative information retrieval. However, existing approaches face critical limitations that lack high-quality training trajectories or suffer from the distributional mismatches in simulated environments and prohibitive computational costs for real-world deployment. This paper introduces SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap through strategic data engineering rather than complex training paradigms. Our approach synthesizes high-quality training data by simulating realistic user interactions in live web search environments, coupled with a multi-criteria curation strategy that optimizes the diversity and quality of input and output side. Experiments on five benchmarks across diverse domains demonstrate that SFT on only 871 curated samples yields significant improvements over RL-based baselines. Our work establishes SFT as a viable pathway by systematically addressing the data-scarce bottleneck, offering practical insights for efficient deep search systems. Our code is available at", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16834.pdf", "abstract_url": "https://arxiv.org/abs/2505.16834", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了SimpleDeepSearcher，一个轻量级但有效的框架，通过战略数据工程而非复杂训练范式，解决了检索增强生成（RAG）系统在复杂深度搜索场景中的限制。", "motivation": "现有的检索增强生成（RAG）系统在需要多步推理和迭代信息检索的复杂深度搜索场景中面临关键限制，如缺乏高质量的训练轨迹或模拟环境中的分布不匹配，以及实际部署中的高昂计算成本。", "method": "通过模拟实时网络搜索环境中的真实用户交互，结合多标准筛选策略，合成高质量的训练数据，优化输入和输出侧的多样性和质量。", "result": "在五个不同领域的基准测试中，仅使用871个精选样本的SFT（监督微调）就显著优于基于强化学习的基线方法。", "conclusion": "本研究通过系统解决数据稀缺瓶颈，确立了SFT作为一种可行途径，为高效的深度搜索系统提供了实用见解。"}}
{"id": "2505.16986", "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities as intelligent agents capable of solving complex problems. However, effective planning in scenarios involving dependencies between API or tool calls-particularly in multi-turn conversations-remains a significant challenge. To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn conversational dataset specifically designed to capture and manage inter-tool dependencies across diverse domains. T1 enables rigorous evaluation of agents' ability to coordinate tool use across nine distinct domains (4 single domain and 5 multi-domain) with the help of an integrated caching mechanism for both short- and long-term memory, while supporting dynamic replanning-such as deciding whether to recompute or reuse cached results. Beyond facilitating research on tool use and planning, T1 also serves as a benchmark for evaluating the performance of open-source language models. We present results powered by T1-Agent, highlighting their ability to plan and reason in complex, tool-dependent scenarios.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "Preprint", "pdf_url": "https://arxiv.org/pdf/2505.16986.pdf", "abstract_url": "https://arxiv.org/abs/2505.16986", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了T1，一个专门设计用于在多轮对话中捕获和管理跨领域工具间依赖关系的工具增强型多领域对话数据集。T1旨在评估智能代理在复杂、依赖工具的场景中的规划和推理能力，并作为开源语言模型性能的基准。", "motivation": "解决大型语言模型在多轮对话中涉及API或工具调用依赖关系时的有效规划问题。", "method": "引入T1数据集，包含九个不同领域（4个单领域和5个多领域）的对话，集成了短期和长期记忆的缓存机制，支持动态重新规划。", "result": "通过T1-Agent展示了在复杂、依赖工具的场景中规划和推理的能力。", "conclusion": "T1不仅促进了工具使用和规划的研究，还为评估开源语言模型的性能提供了基准。"}}
{"id": "2505.16988", "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential in enhancing single LLMs to address complex and diverse tasks in practical applications. Despite considerable advancements, the field lacks a unified codebase that consolidates existing methods, resulting in redundant re-implementation efforts, unfair comparisons, and high entry barriers for researchers. To address these challenges, we introduce MASLab, a unified, comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab integrates over 20 established methods across multiple domains, each rigorously validated by comparing step-by-step outputs with its official implementation. (2) MASLab provides a unified environment with various benchmarks for fair comparisons among methods, ensuring consistent inputs and standardized evaluation protocols. (3) MASLab implements methods within a shared streamlined structure, lowering the barriers for understanding and extension. Building on MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models, offering researchers a clear and comprehensive view of the current landscape of MAS methods. MASLab will continue to evolve, tracking the latest developments in the field, and invite contributions from the broader open-source community.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "18 pages, 11 figures", "pdf_url": "https://arxiv.org/pdf/2505.16988.pdf", "abstract_url": "https://arxiv.org/abs/2505.16988", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "MASLab是一个统一且全面的代码库，旨在整合现有的基于LLM的多智能体系统方法，减少冗余工作，促进公平比较，并降低研究门槛。", "motivation": "解决基于LLM的多智能体系统领域缺乏统一代码库的问题，该问题导致了重复实现、不公平比较和研究门槛高。", "method": "引入MASLab，一个集成了20多种方法、提供统一环境和多种基准测试的代码库，确保公平比较和易于理解与扩展。", "result": "通过覆盖10多个基准测试和8个模型的广泛实验，为研究者提供了当前MAS方法现状的清晰全面视图。", "conclusion": "MASLab将持续发展，跟踪领域最新进展，并邀请开源社区贡献，以促进基于LLM的多智能体系统研究。"}}
{"id": "2505.17012", "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding", "authors": ["Haoning Wu", "Xiao Huang", "Yaohui Chen", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "abstract": "Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This work investigates a critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, a benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with a carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as a rigorous benchmark for the next evolution of MLLMs.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2505.17012.pdf", "abstract_url": "https://arxiv.org/abs/2505.17012", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了SpatialScore，一个用于评估多模态大型语言模型（MLLMs）空间理解能力的统一评估框架。通过创建VGBench和整合11个现有数据集的数据，提出了迄今为止最全面和多样化的多模态空间理解基准SpatialScore，并开发了SpatialAgent系统来支持空间理解任务。", "motivation": "尽管多模态大型语言模型在问答任务中取得了显著成功，但其在空间理解方面的能力尚未得到充分探索。本文旨在填补这一空白，通过创建一个全面的评估框架来测试和提升MLLMs的空间感知和理解能力。", "method": "本文提出了VGBench来评估MLLMs的视觉几何感知能力，并通过整合11个现有数据集的数据，构建了SpatialScore基准。此外，开发了SpatialAgent系统，一个包含9个专门工具的多智能体系统，支持Plan-Execute和ReAct推理范式。", "result": "广泛的评估揭示了空间推理中的持续挑战，同时证明了SpatialAgent的有效性。SpatialScore基准包含了28K个样本，覆盖了各种空间理解任务、模态和问答格式，以及一个精心策划的挑战子集SpatialScore-Hard。", "conclusion": "SpatialScore为MLLMs的下一阶段发展提供了有价值的见解和严格的基准，有望推动多模态空间理解能力的进一步提升。"}}
{"id": "2505.15872", "title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "authors": ["Yunjia Xi", "Jianghao Lin", "Menghui Zhu", "Yongzhao Xiao", "Zhuoying Ou", "Jiaqi Liu", "Tong Wan", "Bo Chen", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding responses with retrieved information. As an emerging paradigm, Agentic RAG further enhances this process by introducing autonomous LLM agents into the information seeking process. However, existing benchmarks fall short in evaluating such systems, as they are confined to a static retrieval environment with a fixed, limited corpus} and simple queries that fail to elicit agentic behavior. Moreover, their evaluation protocols assess information seeking effectiveness by pre-defined gold sets of documents, making them unsuitable for the open-ended and dynamic nature of real-world web environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with challenging questions designed for assessing agentic information seeking in real-world, dynamic web environments. We propose a systematic methodology for constructing challenging queries satisfying the criteria of determinacy, difficulty, and diversity. Based on this, we develop the first evaluation framework tailored to dynamic agentic information seeking, including fine-grained metrics about the accuracy, utility, and compactness of information seeking outcomes. Through extensive experiments across LLMs, search engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and offers actionable insights for future research.", "subjects": "Information Retrieval (cs.IR); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.15872.pdf", "abstract_url": "https://arxiv.org/abs/2505.15872", "categories": ["Information Retrieval (cs.IR)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic", "@RAG"], "AI": {"tldr": "本文介绍了InfoDeepSeek，一个用于评估在动态网络环境中代理式信息寻求的新基准，旨在解决现有基准在评估检索增强生成（RAG）系统中的不足。", "motivation": "现有基准在评估代理式检索增强生成系统时存在局限，无法适应开放和动态的真实网络环境，InfoDeepSeek旨在填补这一空白。", "method": "提出了一种系统性方法构建具有确定性、难度和多样性标准的挑战性查询，并开发了首个针对动态代理式信息寻求的评估框架。", "result": "通过跨大型语言模型、搜索引擎和问题类型的广泛实验，InfoDeepSeek揭示了代理行为的细微差别，并为未来研究提供了可操作的见解。", "conclusion": "InfoDeepSeek为评估代理式信息寻求提供了一个有效的工具，有助于推动相关领域的研究和发展。"}}
{"id": "2505.17005", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "abstract": "Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.17005.pdf", "abstract_url": "https://arxiv.org/abs/2505.17005", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "R1-Searcher++是一个新颖的框架，旨在训练大型语言模型（LLMs）自适应地利用内部和外部知识源，通过强化学习动态获取知识，以提高检索增强推理的效率。", "motivation": "解决大型语言模型因静态知识而产生的幻觉问题，以及当前检索增强生成（RAG）方法成本高、泛化能力差或忽视模型内部知识的问题。", "method": "采用两阶段训练策略：初始的SFT冷启动阶段用于初步格式学习，随后是用于动态知识获取的强化学习（RL）阶段，包括结果监督、内部知识利用的奖励机制和记忆机制。", "result": "实验表明，R1-Searcher++在检索增强推理方面优于之前的RAG和推理方法，实现了高效的检索。", "conclusion": "R1-Searcher++通过结合内部知识和外部搜索引擎，持续提升模型能力，为检索增强推理提供了一种高效的解决方案。"}}
{"id": "2505.16470", "title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "authors": ["Kuicai Dong", "Yujing Chang", "Shijie Huang", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "abstract": "Document Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and", "subjects": "Information Retrieval (cs.IR); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)", "comments": "}", "pdf_url": "https://arxiv.org/pdf/2505.16470.pdf", "abstract_url": "https://arxiv.org/abs/2505.16470", "categories": ["Information Retrieval (cs.IR)", "Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了MMDocRAG，一个针对文档视觉问答（DocVQA）的综合基准，包含4,055个专家标注的QA对，旨在解决多模态证据选择和整合的评估问题。", "motivation": "文档视觉问答在处理长多模态文档和跨模态推理方面面临双重挑战，现有的文档检索增强生成方法因以文本为中心而经常忽略关键视觉信息，且缺乏评估多模态证据选择和整合的强健基准。", "method": "引入MMDocRAG基准，采用创新指标评估多模态引用选择，并支持答案中穿插相关视觉元素。通过60个VLM/LLM模型和14个检索系统的大规模实验。", "result": "实验揭示了在多模态证据检索、选择和整合方面的持续挑战。", "conclusion": "MMDocRAG为评估多模态证据选择和整合提供了全面基准，揭示了当前方法在多模态处理上的局限性，为未来研究指明了方向。"}}
{"id": "2505.15935", "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security", "authors": ["Omer Hofman", "Oren Rachmil", "Shamik Bose", "Vikas Pahuja", "Jonathan Brokman", "Toshiya Shimizu", "Trisha Starostina", "Kelly Marchisio", "Seraphina Goldfarb-Tarrant", "Roman Vainshtein"], "abstract": "Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the global accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into ten diverse languages, resulting in 805 unique tasks and 8,855 total language-specific instances. Our benchmark suite enables a systematic analysis of how multilingual contexts affect agent performance and robustness. Empirically, we observe consistent degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. Building on these findings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes a standardized evaluation framework, encouraging future research towards equitable, reliable, and globally accessible agentic AI. MAPS benchmark suite is publicly available at", "subjects": "Databases (cs.DB); Computation and Language (cs.CL); Cryptography and Security (cs.CR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.15935.pdf", "abstract_url": "https://arxiv.org/abs/2505.15935", "categories": ["Databases (cs.DB)", "Computation and Language (cs.CL)", "Cryptography and Security (cs.CR)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了MAPS，一个多语言基准测试套件，旨在评估跨多种语言和任务的代理AI系统性能和安全性的全球可访问性。", "motivation": "解决大型语言模型（LLMs）在多语言环境中性能下降和安全问题，以及现有基准测试仅关注英语而忽略多语言设置的问题。", "method": "基于四个广泛使用的代理基准测试（GAIA、SWE-bench、MATH和Agent Security Benchmark），将每个数据集翻译成十种不同的语言，创建了805个独特任务和8,855个语言特定实例。", "result": "实证观察显示，从英语过渡到其他语言时，性能和安全性均出现一致下降，严重程度因任务和翻译输入量而异。", "conclusion": "本研究建立了一个标准化的评估框架，为未来研究提供了可操作的推荐，以促进公平、可靠和全球可访问的代理AI系统发展。"}}
{"id": "2505.16377", "title": "VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving", "authors": ["Yansong Qu", "Zilin Huang", "Zihao Sheng", "Jiancong Chen", "Sikai Chen", "Samuel Labi"], "abstract": "Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods including safe RL often fail to capture the true semantic meaning of \"safety\" in complex driving contexts, leading to either overly conservative driving behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as-safety-guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets containing data collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving. The demo video and code can be accessed at:", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16377.pdf", "abstract_url": "https://arxiv.org/abs/2505.16377", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "VL-SAFE是一种基于世界模型的安全强化学习框架，结合视觉语言模型（VLM）作为安全指导范式，专为离线安全策略学习设计，旨在解决自动驾驶中强化学习样本效率低和泛化能力差的问题。", "motivation": "解决基于强化学习的自动驾驶策略学习在安全关键场景中样本效率低、泛化能力差以及现有安全强化学习方法难以在复杂驾驶环境中准确捕捉“安全”语义的问题。", "method": "构建包含专家代理收集数据并由VLM衍生安全分数标注的离线数据集，训练世界模型生成想象滚动和安全估计，使代理能在不与真实环境交互的情况下进行安全规划，并基于这些想象轨迹和安全评估，在VLM安全指导下进行演员-评论家学习，以更安全高效地优化驾驶策略。", "result": "广泛评估表明，VL-SAFE在样本效率、泛化能力、安全性和整体性能上优于现有基线方法。", "conclusion": "VL-SAFE是首个引入VLM指导的世界模型方法用于安全自动驾驶的工作，显著提高了自动驾驶策略的安全性和效率。"}}
{"id": "2505.16975", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "abstract": "Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development (FDD), a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world feature development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that FDD is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on \\textit{hard} split, underscoring the value of its high-quality training data. Code is available here \\href{", "subjects": "Software Engineering (cs.SE); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16975.pdf", "abstract_url": "https://arxiv.org/abs/2505.16975", "categories": ["Software Engineering (cs.SE)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "SWE-Dev是一个旨在评估和训练自主编码系统在现实世界功能开发任务中的首个大规模数据集，包含14,000个训练样本和500个测试样本，提供可运行环境和开发者编写的可执行单元测试。", "motivation": "解决大型语言模型在功能驱动开发（FDD）这一普遍但未被充分探索的软件工程任务中的挑战。", "method": "引入SWE-Dev数据集，支持监督微调（SFT）和通过可执行单元测试提供准确奖励信号的强化学习（RL）。", "result": "评估显示FDD对当前AI是一个极具挑战性的前沿（如Claude-3.7-Sonnet在困难测试集上仅达到22.45% Pass@3），但微调训练集能使7B模型在困难集上媲美GPT-4o。", "conclusion": "SWE-Dev不仅是评估和训练自主编码系统的有效平台，其高质量训练数据还能显著提升模型性能。"}}
{"id": "2505.16581", "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning", "authors": ["Max Weltevrede", "Moritz A. Zanger", "Matthijs T.J. Spaan", "Wendelin Böhmer"], "abstract": "In the zero-shot policy transfer setting in reinforcement learning, the goal is to train an agent on a fixed set of training environments so that it can generalise to similar, but unseen, testing environments. Previous work has shown that policy distillation after training can sometimes produce a policy that outperforms the original in the testing environments. However, it is not yet entirely clear why that is, or what data should be used to distil the policy. In this paper, we prove, under certain assumptions, a generalisation bound for policy distillation after training. The theory provides two practical insights: for improved generalisation, you should 1) train an ensemble of distilled policies, and 2) distil it on as much data from the training environments as possible. We empirically verify that these insights hold in more general settings, when the assumptions required for the theory no longer hold. Finally, we demonstrate that an ensemble of policies distilled on a diverse dataset can generalise significantly better than the original agent.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16581.pdf", "abstract_url": "https://arxiv.org/abs/2505.16581", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了在强化学习的零样本策略转移设置中，如何通过策略蒸馏提高策略在未见测试环境中的泛化能力。研究发现，训练一个蒸馏策略的集成，并在尽可能多的训练环境数据上进行蒸馏，可以显著提高泛化性能。", "motivation": "解决在强化学习中，训练好的策略如何在相似但未见的环境中更好地泛化的问题。", "method": "通过理论证明和实证研究，提出了策略蒸馏后训练的一般化界限，并基于此提出了两个实践建议：训练蒸馏策略的集成和在尽可能多的训练环境数据上进行蒸馏。", "result": "实证验证了在更一般的设置下，即使理论的假设不再成立，这些见解仍然有效。并且，一个在多样化数据集上蒸馏的策略集成可以比原始代理显著更好地泛化。", "conclusion": "通过策略蒸馏和集成学习，可以显著提高强化学习策略在未见环境中的泛化能力，这为未来的研究提供了新的方向。"}}
{"id": "2505.16732", "title": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs", "authors": ["Hany Abdulsamad", "Sahel Iqbal", "Simo Särkkä"], "abstract": "Optimal decision-making under partial observability requires agents to balance reducing uncertainty (exploration) against pursuing immediate objectives (exploitation). In this paper, we introduce a novel policy optimization framework for continuous partially observable Markov decision processes (POMDPs) that explicitly addresses this challenge. Our method casts policy learning as probabilistic inference in a non-Markovian Feynman--Kac model that inherently captures the value of information gathering by anticipating future observations, without requiring extrinsic exploration bonuses or handcrafted heuristics. To optimize policies under this model, we develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently estimates a history-dependent policy gradient under samples from the optimal trajectory distribution induced by the POMDP. We demonstrate the effectiveness of our algorithm across standard continuous POMDP benchmarks, where existing methods struggle to act under uncertainty.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16732.pdf", "abstract_url": "https://arxiv.org/abs/2505.16732", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Machine Learning (stat.ML)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种新颖的策略优化框架，用于连续部分可观察马尔可夫决策过程（POMDPs），通过将策略学习视为非马尔可夫Feynman-Kac模型中的概率推断，有效平衡了探索与开发的挑战。", "motivation": "解决在部分可观察环境下进行最优决策时，如何在减少不确定性（探索）和追求即时目标（开发）之间取得平衡的问题。", "method": "开发了一种嵌套的顺序蒙特卡洛（SMC）算法，有效估计了由POMDP诱导的最优轨迹分布下的历史依赖策略梯度。", "result": "在标准的连续POMDP基准测试中，该方法在不确定性下表现优异，而现有方法则难以应对。", "conclusion": "提出的框架和算法为连续POMDPs中的策略优化提供了一种有效的方法，无需外部探索奖励或手工启发式，就能自然捕捉信息收集的价值。"}}
{"id": "2505.16801", "title": "A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents", "authors": ["Eleftherios Kalafatis", "Konstantinos Mitsis", "Konstantia Zarkogianni", "Maria Athanasiou", "Konstantina Nikita"], "abstract": "Serious Games (SGs) are nowadays shifting focus to include procedural content generation (PCG) in the development process as a means of offering personalized and enhanced player experience. However, the development of a framework to assess the impact of PCG techniques when integrated into SGs remains particularly challenging. This study proposes a methodology for automated evaluation of PCG integration in SGs, incorporating deep reinforcement learning (DRL) game testing agents. To validate the proposed framework, a previously introduced SG featuring card game mechanics and incorporating three different versions of PCG for nonplayer character (NPC) creation has been deployed. Version 1 features random NPC creation, while versions 2 and 3 utilize a genetic algorithm approach. These versions are used to test the impact of different dynamic SG environments on the proposed framework's agents. The obtained results highlight the superiority of the DRL game testing agents trained on Versions 2 and 3 over those trained on Version 1 in terms of win rate (i.e. number of wins per played games) and training time. More specifically, within the execution of a test emulating regular gameplay, both Versions 2 and 3 peaked at a 97% win rate and achieved statistically significant higher (p=0009) win rates compared to those achieved in Version 1 that peaked at 94%. Overall, results advocate towards the proposed framework's capability to produce meaningful data for the evaluation of procedurally generated content in SGs.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.16801.pdf", "abstract_url": "https://arxiv.org/abs/2505.16801", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种用于自动评估严肃游戏中程序化内容生成（PCG）整合的方法论，结合了深度强化学习（DRL）游戏测试代理。通过一个包含卡牌游戏机制的严肃游戏案例，验证了框架的有效性，展示了使用遗传算法的PCG版本在胜率和训练时间上的优势。", "motivation": "严肃游戏（SGs）越来越多地采用程序化内容生成（PCG）以提供个性化的玩家体验，但评估PCG技术在SGs中整合影响的框架开发仍具挑战性。", "method": "研究提出了一种结合深度强化学习（DRL）游戏测试代理的自动化评估PCG整合的方法论，并通过一个包含三种不同PCG版本的严肃游戏案例进行验证。", "result": "使用遗传算法的PCG版本（版本2和3）在胜率和训练时间上优于随机NPC创建版本（版本1），在模拟常规游戏的测试中，版本2和3的胜率达到97%，显著高于版本1的94%。", "conclusion": "结果表明，所提出的框架能够为评估严肃游戏中的程序化生成内容产生有意义的数据，支持其在PCG评估中的应用。"}}
