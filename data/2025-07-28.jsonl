{"id": "2507.18742", "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "authors": ["Víctor Gallego"], "abstract": "Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\\% of cases, the SSC process reduces this vulnerability by over 90\\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "Accepted to SCALR Workshop @ COLM 2025", "pdf_url": "https://arxiv.org/pdf/2507.18742.pdf", "abstract_url": "https://arxiv.org/abs/2507.18742", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2507.18857", "title": "PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning", "authors": ["Mohammad Kachuee", "Teja Gollapudi", "Minseok Kim", "Yin Huang", "Kai Sun", "Xiao Yang", "Jiaqi Wang", "Nirav Shah", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "abstract": "Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. We propose an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.18857.pdf", "abstract_url": "https://arxiv.org/abs/2507.18857", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2507.18884", "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service", "authors": ["Ming Gong", "Xucheng Huang", "Ziheng Xu", "Vijayan K. Asari"], "abstract": "High-quality dialogue is crucial for e-commerce customer service, yet traditional intent-based systems struggle with dynamic, multi-turn interactions. We present MindFlow+, a self-evolving dialogue agent that learns domain-specific behavior by combining large language models (LLMs) with imitation learning and offline reinforcement learning (RL). MindFlow+ introduces two data-centric mechanisms to guide learning: tool-augmented demonstration construction, which exposes the model to knowledge-enhanced and agentic (ReAct-style) interactions for effective tool use; and reward-conditioned data modeling, which aligns responses with task-specific goals using reward signals. To evaluate the model's role in response generation, we introduce the AI Contribution Ratio, a novel metric quantifying AI involvement in dialogue. Experiments on real-world e-commerce conversations show that MindFlow+ outperforms strong baselines in contextual relevance, flexibility, and task accuracy. These results demonstrate the potential of combining LLMs tool reasoning, and reward-guided learning to build domain-specialized, context-aware dialogue systems.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.18884.pdf", "abstract_url": "https://arxiv.org/abs/2507.18884", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2507.18901", "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?", "authors": ["Chuxuan Hu", "Liyun Zhang", "Yeji Lim", "Aum Wadhwani", "Austin Peters", "Daniel Kang"], "abstract": "Assessing the reproducibility of social science papers is essential for promoting rigor in research processes, but manual assessment is costly. With recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate this process. However, existing benchmarks for reproducing research papers (1) focus solely on reproducing results using provided code and data without assessing their consistency with the paper, (2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task instances, each representing a social science paper with a publicly available reproduction report. The agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the reproducibility of social science papers with complexity comparable to real-world assessments. We evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the highest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at", "subjects": "Computation and Language (cs.CL)", "comments": "Accepted to ACL 2025 Findings", "pdf_url": "https://arxiv.org/pdf/2507.18901.pdf", "abstract_url": "https://arxiv.org/abs/2507.18901", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2507.18910", "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions", "authors": ["Agada Joseph Oche", "Ademola Glory Folashade", "Tirthankar Ghosal", "Arpan Biswas"], "abstract": "Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.", "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": "33 pages, 2 figures", "pdf_url": "https://arxiv.org/pdf/2507.18910.pdf", "abstract_url": "https://arxiv.org/abs/2507.18910", "categories": ["Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent", "agentic", "@RAG"]}
{"id": "2507.18868", "title": "A Neuroscience-Inspired Dual-Process Model of Compositional Generalization", "authors": ["Alex Noviello", "Claas Beger", "Jacob Groner", "Kevin Ellis", "Weinan Sun"], "abstract": "Systematic compositional generalization - constructing and understanding novel combinations of known building blocks - remains a core challenge for AI systems. Human cognition achieves this flexibility via the interplay of the hippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes episodes, and the prefrontal cortex consolidates them into reusable schemas for reasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with Rules and Abstractions from Generalized Experience), a framework that achieves systematic generalization on compositional tasks. MIRAGE has two interacting modules mirroring the brain's deliberative HPC-PFC loop and intuitive neocortical pattern recognition. (1) The meta-trained Transformer Neural Decomposer, paralleling neocortical \"System 1\" computation, is trained on a task-agnostic stream of randomly sampled compositional grammars and applies one decomposition step per pass, with successive passes iteratively refining the sequence representation. (2) The Schema Engine, analogous to the HPC-PFC \"System 2\" loop, dynamically extracts, ranks, and applies reusable schemas, storing variable bindings in episodic memory and expanding them when needed. By explicitly equipping the Transformer component of MIRAGE with actively managed schematic structures, our model performs systematic compositional operations through explicit schema application and transformation, relying solely on frozen weights when solving entirely novel tasks. This approach demonstrates systematic compositional generalization on the SCAN benchmark, achieving > 99% accuracy on all task splits with only 1.19M parameters in the transformer module. Ablation studies confirm that MIRAGE's systematicity critically depends on the quality of extracted schemas and the model's iterative refinement process.", "subjects": "Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.18868.pdf", "abstract_url": "https://arxiv.org/abs/2507.18868", "categories": ["Artificial Intelligence (cs.AI)", "Neural and Evolutionary Computing (cs.NE)"], "matching_keywords": ["@RAG"]}
{"id": "2507.19132", "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?", "authors": ["Xuetian Chen", "Yinghao Chen", "Xinfeng Yuan", "Zhuo Peng", "Lu Chen", "Yuekeng Li", "Zhoujia Zhang", "Yingqian Huang", "Leyan Huang", "Jiaqing Liang", "Tianbao Xie", "Zhiyong Wu", "Qiushi Sun", "Biqing Qi", "Bowen Zhou"], "abstract": "Computer-using agents have shown strong potential to boost human productivity and enable new application forms across platforms. While recent advances have led to usable applications, existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the need for a deeper understanding of current strengths and limitations to drive the future progress in computer-using agents research and deployment. All code, environments, baselines, and data are publicly available at", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)", "comments": "Work in progress", "pdf_url": "https://arxiv.org/pdf/2507.19132.pdf", "abstract_url": "https://arxiv.org/abs/2507.19132", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)", "Human-Computer Interaction (cs.HC)"], "matching_keywords": ["agent"]}
{"id": "2507.19263", "title": "Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games", "authors": ["Achille Morenville", "Éric Piette"], "abstract": "In imperfect-information games, agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic. This paper investigates two approaches to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities. We evaluated the impact of both representations using general-purpose agents across two different games. Our findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.19263.pdf", "abstract_url": "https://arxiv.org/abs/2507.19263", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2507.19364", "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges", "authors": ["Patrick Taillandier", "Jean Daniel Zucker", "Arnaud Grignard", "Benoit Gaudou", "Nghi Quang Huynh", "Alexis Drogoul"], "abstract": "This position paper examines the use of Large Language Models (LLMs) in social simulation, analyzing both their potential and their limitations from a computational social science perspective. The first part reviews recent findings on the ability of LLMs to replicate key aspects of human cognition, including Theory of Mind reasoning and social inference, while also highlighting significant limitations such as cognitive biases, lack of true understanding, and inconsistencies in behavior. The second part surveys emerging applications of LLMs in multi-agent simulation frameworks, focusing on system architectures, scale, and validation strategies. Notable projects such as Generative Agents (Smallville) and AgentSociety are discussed in terms of their design choices, empirical grounding, and methodological innovations. Particular attention is given to the challenges of behavioral fidelity, calibration, and reproducibility in large-scale LLM-driven simulations. The final section distinguishes between contexts where LLMs, like other black-box systems, offer direct value-such as interactive simulations and serious games-and those where their use is more problematic, notably in explanatory or predictive modeling. The paper concludes by advocating for hybrid approaches that integrate LLMs into traditional agent-based modeling platforms (GAMA, Netlogo, etc), enabling modelers to combine the expressive flexibility of language-based reasoning with the transparency and analytical rigor of classical rule-based systems.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.19364.pdf", "abstract_url": "https://arxiv.org/abs/2507.19364", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2507.18956", "title": "A Similarity Measure for Comparing Conversational Dynamics", "authors": ["Sang Min Jung", "Kaixiang Zhang", "Cristian Danescu-Niculescu-Mizil"], "abstract": "The quality of a conversation goes beyond the individual quality of each reply, and instead emerges from how these combine into interactional patterns that give the conversation its distinctive overall \"shape\". However, there is no robust automated method for comparing conversations in terms of their overall interactional dynamics. Such methods could enhance the analysis of conversational data and help evaluate conversational agents more holistically.", "subjects": "Computation and Language (cs.CL)", "comments": ")", "pdf_url": "https://arxiv.org/pdf/2507.18956.pdf", "abstract_url": "https://arxiv.org/abs/2507.18956", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2507.19090", "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "abstract": "Claim verification is critical for enhancing digital literacy. However, the state-of-the-art single-LLM methods struggle with complex claim verification that involves multi-faceted evidences. Inspired by real-world fact-checking practices, we propose DebateCV, the first claim verification framework that adopts a debate-driven methodology using multiple LLM agents. In our framework, two Debaters take opposing stances on a claim and engage in multi-round argumentation, while a Moderator evaluates the arguments and renders a verdict with justifications. To further improve the performance of the Moderator, we introduce a novel post-training strategy that leverages synthetic debate data generated by the zero-shot DebateCV, effectively addressing the scarcity of real-world debate-driven claim verification data. Experimental results show that our method outperforms existing claim verification methods under varying levels of evidence quality. Our code and dataset are publicly available at", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.19090.pdf", "abstract_url": "https://arxiv.org/abs/2507.19090", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2507.18755", "title": "Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback", "authors": ["Chandra Maddila", "Adam Tait", "Claire Chang", "Daniel Cheng", "Nauman Ahmad", "Vijayaraghavan Murali", "Marshall Roch", "Arnaud Avondet", "Aaron Meltzer", "Victor Montalvao", "Michael Hopko", "Chris Waterson", "Parth Thakkar", "Renuka Fernandez", "Kristian Kristensen", "Sivan Barzily", "Sherry Chen", "Rui Abreu", "Nachiappan Nagappan", "Payam Shodjai", "Killian Murphy", "James Everingham", "Aparna Ramani", "Peter C. Rigby"], "abstract": "Aim: With the advent of LLMs, sophisticated agentic program repair has become viable at large organizations with large codebases. In this work, we develop an Engineering Agent that fixes the source code based on test failures at scale across diverse software offerings internally.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Programming Languages (cs.PL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.18755.pdf", "abstract_url": "https://arxiv.org/abs/2507.18755", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)", "Programming Languages (cs.PL)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2507.19232", "title": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene", "authors": ["Donggeun Lim", "Jinseok Bae", "Inwoo Hwang", "Seungmin Lee", "Hwanhee Lee", "Young Min Kim"], "abstract": "In this work, we propose a framework that creates a lively virtual dynamic scene with contextual motions of multiple humans. Generating multi-human contextual motion requires holistic reasoning over dynamic relationships among human-human and human-scene interactions. We adapt the power of a large language model (LLM) to digest the contextual complexity within textual input and convert the task into tangible subproblems such that we can generate multi-agent behavior beyond the scale that was not considered before. Specifically, our event generator formulates the temporal progression of a dynamic scene into a sequence of small events. Each event calls for a well-defined motion involving relevant characters and objects. Next, we synthesize the motions of characters at positions sampled based on spatial guidance. We employ a high-level module to deliver scalable yet comprehensive context, translating events into relative descriptions that enable the retrieval of precise coordinates. As the first to address this problem at scale and with diversity, we offer a benchmark to assess diverse aspects of contextual reasoning. Benchmark results and user studies show that our framework effectively captures scene context with high scalability. The code and benchmark, along with result videos, are available at our project page:", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.19232.pdf", "abstract_url": "https://arxiv.org/abs/2507.19232", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2507.19239", "title": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception", "authors": ["Jiaru Zhong", "Jiahao Wang", "Jiahui Xu", "Xiaofan Li", "Zaiqing Nie", "Haibao Yu"], "abstract": "Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\\% mAP and 32.8\\% AMOTA. The project is available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "Accepted by ICCV 2025 (Highlight)", "pdf_url": "https://arxiv.org/pdf/2507.19239.pdf", "abstract_url": "https://arxiv.org/abs/2507.19239", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2507.19354", "title": "EffiComm: Bandwidth Efficient Multi Agent Communication", "authors": ["Melih Yazgan", "Allen Xavier Arasan", "J. Marius Zöllner"], "abstract": "Collaborative perception allows connected vehicles to exchange sensor information and overcome each vehicle's blind spots. Yet transmitting raw point clouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications, causing latency and scalability problems. We introduce EffiComm, an end-to-end framework that transmits less than 40% of the data required by prior art while maintaining state-of-the-art 3D object detection accuracy. EffiComm operates on Bird's-Eye-View (BEV) feature maps from any modality and applies a two-stage reduction pipeline: (1) Selective Transmission (ST) prunes low-utility regions with a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph Neural Network (GNN) to assign vehicle-specific keep ratios according to role and network load. The remaining features are fused with a soft-gated Mixture-of-Experts (MoE) attention layer, offering greater capacity and specialization for effective feature integration. On the OPV2V benchmark, EffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately 1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve. These results highlight the value of adaptive, learned communication for scalable Vehicle-to-Everything (V2X) perception.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)", "comments": "Accepted for publication at ITSC 2025", "pdf_url": "https://arxiv.org/pdf/2507.19354.pdf", "abstract_url": "https://arxiv.org/abs/2507.19354", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)", "Robotics (cs.RO)"], "matching_keywords": ["agent"]}
{"id": "2507.19102", "title": "Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Jiaming Zhang", "Shuaiqiang Wang", "Dawei Yin", "Xueqi Cheng"], "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.", "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": "9 pages, 5 figures", "pdf_url": "https://arxiv.org/pdf/2507.19102.pdf", "abstract_url": "https://arxiv.org/abs/2507.19102", "categories": ["Information Retrieval (cs.IR)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2507.18812", "title": "MemoCoder: Automated Function Synthesis using LLM-Supported Agents", "authors": ["Yiping Jia", "Zhen Ming Jiang", "Shayan Noei", "Ying Zou"], "abstract": "With the widespread adoption of Large Language Models (LLMs) such as GitHub Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to support code generation. While LLMs can generate syntactically correct solutions for well-structured programming tasks, they often struggle with challenges that require iterative debugging, error handling, or adaptation to diverse problem structures. Existing approaches such as fine-tuning or self-repair strategies either require costly retraining or lack mechanisms to accumulate and reuse knowledge from previous attempts.", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.18812.pdf", "abstract_url": "https://arxiv.org/abs/2507.18812", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2507.18867", "title": "Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise", "authors": ["Xuefei Wu", "Xiao Yin", "Yuanyang Zhu", "Chunlin Chen"], "abstract": "Efficient exploration in multi-agent reinforcement learning (MARL) is a challenging problem when receiving only a team reward, especially in environments with sparse rewards. A powerful method to mitigate this issue involves crafting dense individual rewards to guide the agents toward efficient exploration. However, individual rewards generally rely on manually engineered shaping-reward functions that lack high-order intelligence, thus it behaves ineffectively than humans regarding learning and generalization in complex problems. To tackle these issues, we combine the above two paradigms and propose a novel framework, LIGHT (Learning Individual Intrinsic reward via Incorporating Generalized Human experTise), which can integrate human knowledge into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid unnecessary exploration by considering both individual action distribution and human expertise preference distribution. Then, LIGHT designs individual intrinsic rewards for each agent based on actionable representational transformation relevant to Q-learning so that the agents align their action preferences with the human expertise while maximizing the joint action value. Experimental results demonstrate the superiority of our method over representative baselines regarding performance and better knowledge reusability across different sparse-reward tasks on challenging scenarios.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "IEEE International Conference on Systems, Man, and Cybernetics", "pdf_url": "https://arxiv.org/pdf/2507.18867.pdf", "abstract_url": "https://arxiv.org/abs/2507.18867", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2507.19478", "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents", "authors": ["Xuehui Wang", "Zhenyu Wu", "JingJing Xie", "Zichen Ding", "Bowen Yang", "Zehao Li", "Zhaoyang Liu", "Qingyun Li", "Xuan Dong", "Zhe Chen", "Weiyun Wang", "Xiangyu Zhao", "Jixuan Chen", "Haodong Duan", "Tianbao Xie", "Chenyu Yang", "Shiqian Su", "Yue Yu", "Yuan Huang", "Yiqian Liu", "Xiao Zhang", "Yanting Zhang", "Xiangyu Yue", "Weijie Su", "Xizhou Zhu", "Wei Shen", "Jifeng Dai", "Wenhai Wang"], "abstract": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web platforms. It comprises four levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA) metric to assess GUI agent execution efficiency in online automation scenarios. Through MMBench-GUI, we identify accurate visual grounding as a critical determinant of overall task success, emphasizing the substantial benefits of modular frameworks that integrate specialized grounding modules. Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and cross-platform generalization abilities, with long-context memory, a broad action space, and long-term reasoning playing a critical role. More important, task efficiency remains a critically underexplored dimension, and all models suffer from substantial inefficiencies, with excessive redundant steps even when tasks are ultimately completed. The integration of precise localization, effective planning, and early stopping strategies is indispensable to enable truly efficient and scalable GUI automation. Our benchmark code, evaluation data, and running environment will be publicly available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)", "comments": "in progress", "pdf_url": "https://arxiv.org/pdf/2507.19478.pdf", "abstract_url": "https://arxiv.org/abs/2507.19478", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2507.19333", "title": "Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation", "authors": ["Minghao Tang", "Shiyu Ni", "Jiafeng Guo", "Keping Bi"], "abstract": "Retrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. The code can be found \\href{here}{", "subjects": "Information Retrieval (cs.IR); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.19333.pdf", "abstract_url": "https://arxiv.org/abs/2507.19333", "categories": ["Information Retrieval (cs.IR)", "Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2507.19151", "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination", "authors": ["Michael Amir", "Guang Yang", "Zhan Gao", "Keisuke Okumura", "Heedo Woo", "Amanda Prorok"], "abstract": "Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.19151.pdf", "abstract_url": "https://arxiv.org/abs/2507.19151", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2507.19298", "title": "Controlling Topological Defects in Polar Fluids via Reinforcement Learning", "authors": ["Abhinav Singh", "Petros Koumoutsakos"], "abstract": "Topological defects in active polar fluids exhibit complex dynamics driven by internally generated stresses, reflecting the deep interplay between topology, flow, and non-equilibrium hydrodynamics. Feedback control offers a powerful means to guide such systems, enabling transitions between dynamic states. We investigated closed-loop steering of integer-charged defects in a confined active fluid by modulating the spatial profile of activity. Using a continuum hydrodynamic model, we show that localized control of active stress induces flow fields that can reposition and direct defects along prescribed trajectories by exploiting non-linear couplings in the system. A reinforcement learning framework is used to discover effective control strategies that produce robust defect transport across both trained and novel trajectories. The results highlight how AI agents can learn the underlying dynamics and spatially structure activity to manipulate topological excitations, offering insights into the controllability of active matter and the design of adaptive, self-organized materials.", "subjects": "Soft Condensed Matter (cond-mat.soft); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.19298.pdf", "abstract_url": "https://arxiv.org/abs/2507.19298", "categories": ["Soft Condensed Matter (cond-mat.soft)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
