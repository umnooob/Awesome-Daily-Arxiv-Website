{"id": "2505.04769", "title": "Vision-Language-Action Models: Concepts, Progress, Applications and Challenges", "authors": ["Ranjan Sapkota", "Yang Cao", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "abstract": "Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "36 pages, 18 Figures, 4 Tables", "pdf_url": "https://arxiv.org/pdf/2505.04769.pdf", "abstract_url": "https://arxiv.org/abs/2505.04769", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2505.04921", "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models", "authors": ["Yunxin Li", "Zhenyu Liu", "Zitao Li", "Xuanyu Zhang", "Zhenran Xu", "Xinyu Chen", "Haoyuan Shi", "Shenyuan Jiang", "Xintong Wang", "Jifang Wang", "Shouzheng Huang", "Xinping Zhao", "Borui Jiang", "Lanqing Hong", "Longyue Wang", "Zhuotao Tian", "Baoxing Huai", "Wenhan Luo", "Weihua Luo", "Zheng Zhang", "Baotian Hu", "Min Zhang"], "abstract": "Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2505.04921.pdf", "abstract_url": "https://arxiv.org/abs/2505.04921", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2505.04965", "title": "DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding", "authors": ["Henry Zheng", "Hao Shi", "Qihang Peng", "Yong Xien Chng", "Rui Huang", "Yepeng Weng", "Zhongchao Shi", "Gao Huang"], "abstract": "Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the SOTA in egocentric 3D visual grounding. Our method also achieves 1st place and receives the Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "Accepted by ICLR 2025", "pdf_url": "https://arxiv.org/pdf/2505.04965.pdf", "abstract_url": "https://arxiv.org/abs/2505.04965", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2505.04628", "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks", "authors": ["Yusen Wu", "Junwu Xiong", "Xiaotie Deng"], "abstract": "Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs' capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM's social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs' social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.04628.pdf", "abstract_url": "https://arxiv.org/abs/2505.04628", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Social and Information Networks (cs.SI)"], "matching_keywords": ["agent"]}
{"id": "2505.04638", "title": "Towards Artificial Intelligence Research Assistant for Expert-Involved Learning", "authors": ["Tianyu Liu", "Simeng Han", "Xiao Luo", "Hanchen Wang", "Pan Lu", "Biqing Zhu", "Yuge Wang", "Keyi Li", "Jiapeng Chen", "Rihao Qu", "Yufeng Liu", "Xinyue Cui", "Aviv Yaish", "Yuhang Chen", "Minsheng Hao", "Chuhan Li", "Kexing Li", "Arman Cohan", "Hua Xu", "Mark Gerstein", "James Zou", "Hongyu Zhao"], "abstract": "Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged as transformative tools in scientific research, yet their reliability and specific contributions to biomedical applications remain insufficiently characterized. In this study, we present \\textbf{AR}tificial \\textbf{I}ntelligence research assistant for \\textbf{E}xpert-involved \\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and enhance two critical capabilities of LLMs and LMMs in biomedical research: summarizing extensive scientific texts and interpreting complex biomedical figures. To facilitate rigorous assessment, we create two open-source sets comprising biomedical articles and figures with designed questions. We systematically benchmark both open- and closed-source foundation models, incorporating expert-driven human evaluations conducted by doctoral-level experts. Furthermore, we improve model performance through targeted prompt engineering and fine-tuning strategies for summarizing research papers, and apply test-time computational scaling to enhance the reasoning capabilities of LMMs, achieving superior accuracy compared to human-expert corrections. We also explore the potential of using LMM Agents to generate scientific hypotheses from diverse multimodal inputs. Overall, our results delineate clear strengths and highlight significant limitations of current foundation models, providing actionable insights and guiding future advancements in deploying large-scale language and multi-modal models within biomedical research.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)", "comments": "36 pages, 7 figures", "pdf_url": "https://arxiv.org/pdf/2505.04638.pdf", "abstract_url": "https://arxiv.org/abs/2505.04638", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Information Retrieval (cs.IR)"], "matching_keywords": ["agent"]}
{"id": "2505.04649", "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights", "authors": ["Chengzhang Yu", "Yiming Zhang", "Zhixin Liu", "Zenghui Ding", "Yining Sun", "Zhanpeng Jin"], "abstract": "The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.", "subjects": "Computation and Language (cs.CL)", "comments": "12 pages, 4 figures, 5 table", "pdf_url": "https://arxiv.org/pdf/2505.04649.pdf", "abstract_url": "https://arxiv.org/abs/2505.04649", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2505.04651", "title": "Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions", "authors": ["Adithya Kulkarni", "Fatimah Alotaibi", "Xinyue Zeng", "Longfeng Wu", "Tong Zeng", "Barry Menglong Yao", "Minqian Liu", "Shuaicheng Zhang", "Lifu Huang", "Dawei Zhou"], "abstract": "Large Language Models (LLMs) are transforming scientific hypothesis generation and validation by enabling information synthesis, latent relationship discovery, and reasoning augmentation. This survey provides a structured overview of LLM-driven approaches, including symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. We examine techniques such as retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, highlighting trade-offs in interpretability, novelty, and domain alignment. We contrast early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM pipelines that leverage in-context learning and domain adaptation via fine-tuning, retrieval, and symbolic grounding. For validation, we review simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing iterative assessment in open-world contexts. The survey maps datasets across biomedicine, materials science, environmental science, and social science, introducing new resources like AHTech and CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, positioning LLMs as agents for principled, scalable scientific discovery.", "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.04651.pdf", "abstract_url": "https://arxiv.org/abs/2505.04651", "categories": ["Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2505.04666", "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes", "authors": ["Mohammad Aqib", "Mohd Hamza", "Qipei Mei", "Ying Hei Chui"], "abstract": "Building codes are regulations that establish standards for the design, construction, and safety of buildings to ensure structural integrity, fire protection, and accessibility. They are often extensive, complex, and subject to frequent updates, making manual querying challenging and time-consuming. Key difficulties include navigating large volumes of text, interpreting technical language, and identifying relevant clauses across different sections. A potential solution is to build a Question-Answering (QA) system that answers user queries based on building codes. Among the various methods for building a QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG consists of two components: a retriever and a language model. This study focuses on identifying a suitable retriever method for building codes and optimizing the generational capability of the language model using fine-tuning techniques. We conducted a detailed evaluation of various retrieval methods by performing the retrieval on the National Building Code of Canada (NBCC) and explored the impact of domain-specific fine-tuning on several language models using the dataset derived from NBCC. Our analysis included a comparative assessment of different retrievers and the performance of both pre-trained and fine-tuned models to determine the efficacy and domain-specific adaptation of language models using fine-tuning on the NBCC dataset. Experimental results showed that Elasticsearch proved to be the most robust retriever among all. The findings also indicate that fine-tuning language models on an NBCC-specific dataset can enhance their ability to generate contextually relevant responses. When combined with context retrieved by a powerful retriever like Elasticsearch, this improvement in LLM performance can optimize the RAG system, enabling it to better navigate the complexities of the NBCC.", "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.04666.pdf", "abstract_url": "https://arxiv.org/abs/2505.04666", "categories": ["Computation and Language (cs.CL)", "Information Retrieval (cs.IR)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2505.04646", "title": "Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems", "authors": ["Poria Azadi"], "abstract": "This article explores the emergence of autonomy and agency by connecting fundamental computational limits (decidability, completeness, computational irreducibility) with physical concepts. We introduce a formal model of a \"minimal agent\" operating within potentially Turing-complete environments. Using algorithmic information theory, we argue that the inherent undecidability and computational irreducibility of agent-environment interaction lead to unpredictability and novel information generation, enabling agency (effective goal-directed action). Computational irreducibility prevents full external prediction, creating necessary conditions for autonomous behavior. We relate this to computational sourcehood, where an agent is the irreducible origin of its behavior, though formalizing this concept remains challenging. Our central thesis, formally proven, is that genuine autonomy necessarily implies undecidability from an external perspective, distinguishing autonomous systems from predictable ones. We propose that agency arises when agent-environment coupling complexity allows mutual information between internal states and relevant environmental variables to increase, particularly where analytical solutions are absent and operational closure is needed for persistence. This framework links agency directly to the computational properties of interaction, offering implications for understanding consciousness, designing autonomous AI, and reconceptualizing free will in a deterministic yet computationally irreducible universe.", "subjects": "Artificial Intelligence (cs.AI); Computational Complexity (cs.CC); Information Theory (cs.IT)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.04646.pdf", "abstract_url": "https://arxiv.org/abs/2505.04646", "categories": ["Artificial Intelligence (cs.AI)", "Computational Complexity (cs.CC)", "Information Theory (cs.IT)"], "matching_keywords": ["agent"]}
{"id": "2505.04843", "title": "Large Language Models are Autonomous Cyber Defenders", "authors": ["Sebastián R. Castro", "Roberto Campbell", "Nancy Lau", "Octavio Villalobos", "Jiaqi Duan", "Alvaro A. Cardenas"], "abstract": "Fast and effective incident response is essential to prevent adversarial cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response through Artificial Intelligence (AI) agents that plan and execute actions. Most ACD approaches focus on single-agent scenarios and leverage Reinforcement Learning (RL). However, ACD RL-trained agents depend on costly training, and their reasoning is not always explainable or transferable. Large Language Models (LLMs) can address these concerns by providing explainable actions in general security contexts. Researchers have explored LLM agents for ACD but have not evaluated them on multi-agent scenarios or interacting with other ACD agents. In this paper, we show the first study on how LLMs perform in multi-agent ACD environments by proposing a new integration to the CybORG CAGE 4 environment. We examine how ACD teams of LLM and RL agents can interact by proposing a novel communication protocol. Our results highlight the strengths and weaknesses of LLMs and RL and help us identify promising research directions to create, train, and deploy future teams of ACD agents.", "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)", "comments": "Presented at IEEE CAI Workshop on Adaptive Cyber Defense 2025. Proceedings to appear", "pdf_url": "https://arxiv.org/pdf/2505.04843.pdf", "abstract_url": "https://arxiv.org/abs/2505.04843", "categories": ["Artificial Intelligence (cs.AI)", "Cryptography and Security (cs.CR)"], "matching_keywords": ["agent"]}
{"id": "2505.04927", "title": "Belief Filtering for Epistemic Control in Linguistic State Space", "authors": ["Sebastian Dumbrava"], "abstract": "We examine belief filtering as a mechanism for the epistemic control of artificial agents, focusing on the regulation of internal cognitive states represented as linguistic expressions. This mechanism is developed within the Semantic Manifold framework, where belief states are dynamic, structured ensembles of natural language fragments. Belief filters act as content-aware operations on these fragments across various cognitive transitions. This paper illustrates how the inherent interpretability and modularity of such a linguistically-grounded cognitive architecture directly enable belief filtering, offering a principled approach to agent regulation. The study highlights the potential for enhancing AI safety and alignment through structured interventions in an agent's internal semantic space and points to new directions for architecturally embedded cognitive governance.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "18 pages", "pdf_url": "https://arxiv.org/pdf/2505.04927.pdf", "abstract_url": "https://arxiv.org/abs/2505.04927", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2505.04997", "title": "Foam-Agent: Towards Automated Intelligent CFD Workflows", "authors": ["Ling Yue", "Nithin Somasekharan", "Yadi Cao", "Shaowu Pan"], "abstract": "Computational Fluid Dynamics (CFD) is an essential simulation tool in various engineering disciplines, but it often requires substantial domain expertise and manual configuration, creating barriers to entry. We present Foam-Agent, a multi-agent framework that automates complex OpenFOAM-based CFD simulation workflows from natural language inputs. Our innovation includes (1) a hierarchical multi-index retrieval system with specialized indices for different simulation aspects, (2) a dependency-aware file generation system that provides consistency management across configuration files, and (3) an iterative error correction mechanism that diagnoses and resolves simulation failures without human intervention. Through comprehensive evaluation on the dataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the critical contribution of each system component, with the specialized error correction mechanism providing a 36.4% performance improvement. Foam-Agent substantially lowers the CFD expertise threshold while maintaining modeling accuracy, demonstrating the potential of specialized multi-agent systems to democratize access to complex scientific simulation tools. The code is public at", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.04997.pdf", "abstract_url": "https://arxiv.org/abs/2505.04997", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2505.05029", "title": "A Reputation System for Large Language Model-based Multi-agent Systems to Avoid the Tragedy of the Commons", "authors": ["Siyue Ren", "Wanli Fu", "Xinkun Zou", "Chen Shen", "Yi Cai", "Chen Chu", "Zhen Wang", "Shuyue Hu"], "abstract": "The tragedy of the commons, where individual self-interest leads to collectively disastrous outcomes, is a pervasive challenge in human society. Recent studies have demonstrated that similar phenomena can arise in generative multi-agent systems (MASs). To address this challenge, this paper explores the use of reputation systems as a remedy. We propose RepuNet, a dynamic, dual-level reputation framework that models both agent-level reputation dynamics and system-level network evolution. Specifically, driven by direct interactions and indirect gossip, agents form reputations for both themselves and their peers, and decide whether to connect or disconnect other agents for future interactions. Through two distinct scenarios, we show that RepuNet effectively mitigates the 'tragedy of the commons', promoting and sustaining cooperation in generative MASs. Moreover, we find that reputation systems can give rise to rich emergent behaviors in generative MASs, such as the formation of cooperative clusters, the social isolation of exploitative agents, and the preference for sharing positive gossip rather than negative ones.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05029.pdf", "abstract_url": "https://arxiv.org/abs/2505.05029", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2505.05059", "title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search", "authors": ["Sandro Junior Della Rovere", "Davide Basso", "Luca Bortolussi", "Mirjana Videnovic-Misic", "Husni Habal"], "abstract": "The layout of analog ICs requires making complex trade-offs, while addressing device physics and variability of the circuits. This makes full automation with learning-based solutions hard to achieve. However, reinforcement learning (RL) has recently reached significant results, particularly in solving the floorplanning problem. This paper presents a hybrid method that combines RL with a beam (BS) strategy. The BS algorithm enhances the agent's inference process, allowing for the generation of flexible floorplans by accomodating various objective weightings, and addressing congestion without without the need for policy retraining or fine-tuning. Moreover, the RL agent's generalization ability stays intact, along with its efficient handling of circuit features and constraints. Experimental results show approx. 5-85% improvement in area, dead space and half-perimeter wire length compared to a standard RL application, along with higher rewards for the agent. Moreover, performance and efficiency align closely with those of existing state-of-the-art techniques.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "Published in Proceedings of the 21st International Conference on Synthesis, Modeling, Analysis and Simulation Methods, and Applications to Circuit Design (SMACD 2025). 4 pages, 3 figures", "pdf_url": "https://arxiv.org/pdf/2505.05059.pdf", "abstract_url": "https://arxiv.org/abs/2505.05059", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2505.05108", "title": "Multi-agent Embodied AI: Advances and Future Directions", "authors": ["Zhaohan Feng", "Ruiqi Xue", "Lei Yuan", "Yang Yu", "Ning Ding", "Meiqin Liu", "Bingzhao Gao", "Jian Sun", "Gang Wang"], "abstract": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the application of advanced technologies in the intelligent era, where AI systems are integrated with physical bodies that enable them to perceive, reason, and interact with their environments. Through the use of sensors for input and actuators for action, these systems can learn and adapt based on real-world feedback, allowing them to perform tasks effectively in dynamic and unpredictable environments. As techniques such as deep learning (DL), reinforcement learning (RL), and large language models (LLMs) mature, embodied AI has become a leading field in both academia and industry, with applications spanning robotics, healthcare, transportation, and manufacturing. However, most research has focused on single-agent systems that often assume static, closed environments, whereas real-world embodied AI must navigate far more complex scenarios. In such settings, agents must not only interact with their surroundings but also collaborate with other agents, necessitating sophisticated mechanisms for adaptation, real-time learning, and collaborative problem-solving. Despite increasing interest in multi-agent systems, existing research remains narrow in scope, often relying on simplified models that fail to capture the full complexity of dynamic, open environments for multi-agent embodied AI. Moreover, no comprehensive survey has systematically reviewed the advancements in this area. As embodied AI rapidly evolves, it is crucial to deepen our understanding of multi-agent embodied AI to address the challenges presented by real-world applications. To fill this gap and foster further development in the field, this paper reviews the current state of research, analyzes key contributions, and identifies challenges and future directions, providing insights to guide innovation and progress in this field.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05108.pdf", "abstract_url": "https://arxiv.org/abs/2505.05108", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2505.05318", "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects", "authors": ["Agnese Chiatti", "Sara Bernardini", "Lara Shibelski Godoy Piccolo", "Viola Schiaffonati", "Matteo Matteucci"], "abstract": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. This survey reviews studies on trust dynamics in user-VLM interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. Literature insights and findings from a workshop with prospective VLM users inform preliminary requirements for future VLM trust studies.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05318.pdf", "abstract_url": "https://arxiv.org/abs/2505.05318", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)", "Human-Computer Interaction (cs.HC)", "Robotics (cs.RO)"], "matching_keywords": ["agent"]}
{"id": "2505.04844", "title": "Osiris: A Lightweight Open-Source Hallucination Detection System", "authors": ["Alex Shan", "John Bauer", "Christopher D. Manning"], "abstract": "Retrieval-Augmented Generation (RAG) systems have gained widespread adoption by application builders because they leverage sources of truth to enable Large Language Models (LLMs) to generate more factually sound responses. However, hallucinations, instances of LLM responses that are unfaithful to the provided context, often prevent these systems from being deployed in production environments. Current hallucination detection methods typically involve human evaluation or the use of closed-source models to review RAG system outputs for hallucinations. Both human evaluators and closed-source models suffer from scaling issues due to their high costs and slow inference speeds. In this work, we introduce a perturbed multi-hop QA dataset with induced hallucinations. Via supervised fine-tuning on our dataset, we achieve better recall with a 7B model than GPT-4o on the RAGTruth hallucination detection benchmark and offer competitive performance on precision and accuracy, all while using a fraction of the parameters. Code is released at our repository.", "subjects": "Computation and Language (cs.CL)", "comments": "Stanford 191W", "pdf_url": "https://arxiv.org/pdf/2505.04844.pdf", "abstract_url": "https://arxiv.org/abs/2505.04844", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2505.04847", "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards", "authors": ["Manveer Singh Tamber", "Forrest Sheng Bao", "Chenyu Xu", "Ge Luo", "Suleman Kazi", "Minseok Bae", "Miaoran Li", "Ofer Mendelevitch", "Renyi Qu", "Jimmy Lin"], "abstract": "Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce hallucinations by grounding responses in contexts. However, even when provided context, LLMs still frequently introduce unsupported information or contradictions. This paper presents our efforts to measure LLM hallucinations with a focus on summarization tasks, assessing how often various LLMs introduce hallucinations when summarizing documents. We discuss Vectara's existing LLM hallucination leaderboard, based on the Hughes Hallucination Evaluation Model (HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great research interest, we examine challenges faced by HHEM and current hallucination detection methods by analyzing the effectiveness of these methods on existing hallucination datasets. To address these limitations, we propose FaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination annotations, which substantially improves automated LLM hallucination evaluation over current methods. We introduce an enhanced hallucination leaderboard centered on FaithJudge, alongside our current hallucination leaderboard, enabling more reliable benchmarking of LLMs for hallucinations in RAG.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.04847.pdf", "abstract_url": "https://arxiv.org/abs/2505.04847", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2505.04916", "title": "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education", "authors": ["Ramteja Sajja", "Yusuf Sermet", "Ibrahim Demir"], "abstract": "Recent advances in AI have catalyzed the adoption of intelligent educational tools, yet many semantic retrieval systems remain ill-suited to the unique linguistic and structural characteristics of academic content. This study presents two open-source embedding models fine-tuned for educational question answering, particularly in the context of course syllabi. A synthetic dataset of 3,197 sentence pairs, spanning synonymous terminology, paraphrased questions, and implicit-explicit mappings, was constructed through a combination of manual curation and large language model (LLM)-assisted generation. Two training strategies were evaluated: (1) a baseline model fine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model that combines MNRL with CosineSimilarityLoss to improve both semantic ranking and similarity calibration. Evaluations were conducted on 28 university course syllabi using a fixed set of natural language questions categorized into course, faculty, and teaching assistant information. Results demonstrate that both fine-tuned models outperform strong open-source baselines, including all-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model narrows the performance gap with high-performing proprietary embeddings such as OpenAI's text-embedding-3 series. This work contributes reusable, domain-aligned embedding models and provides a replicable framework for educational semantic retrieval, supporting downstream applications such as academic chatbots, retrieval-augmented generation (RAG) systems, and learning management system (LMS) integrations.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": "17 pages, 3 Tables", "pdf_url": "https://arxiv.org/pdf/2505.04916.pdf", "abstract_url": "https://arxiv.org/abs/2505.04916", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2505.05115", "title": "Is there a half-life for the success rates of AI agents?", "authors": ["Toby Ord"], "abstract": "Building on the recent empirical work of Kwa et al. (2025), I show that within their suite of research-engineering tasks the performance of AI agents on longer-duration tasks can be explained by an extremely simple mathematical model -- a constant rate of failing during each minute a human would take to do the task. This implies an exponentially declining success rate with the length of the task and that each agent could be characterised by its own half-life. This empirical regularity allows us to estimate the success rate for an agent at different task lengths. And the fact that this model is a good fit for the data is suggestive of the underlying causes of failure on longer tasks -- that they involve increasingly large sets of subtasks where failing any one fails the task. Whether this model applies more generally on other suites of tasks is unknown and an important subject for further work.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "9 pages, 5 figures", "pdf_url": "https://arxiv.org/pdf/2505.05115.pdf", "abstract_url": "https://arxiv.org/abs/2505.05115", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2505.05177", "title": "MARK: Memory Augmented Refinement of Knowledge", "authors": ["Anish Ganguli", "Prabal Deb", "Debleena Banerjee"], "abstract": "Large Language Models (LLMs) assist in specialized tasks but struggle to align with evolving domain knowledge without costly fine-tuning. Domain knowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid') and generally accepted principles (e.g., ethical standards); Refined Memory: Evolving insights shaped by business needs and real-world changes. However, a significant gap often exists between a domain expert's deep, nuanced understanding and the system's domain knowledge, which can hinder accurate information retrieval and application. Our Memory-Augmented Refinement of Knowledge (MARK) framework enables LLMs to continuously learn without retraining by leveraging structured refined memory, inspired by the Society of Mind. MARK operates through specialized agents, each serving a distinct role: Residual Refined Memory Agent: Stores and retrieves domain-specific insights to maintain context over time; User Question Refined Memory Agent: Captures user-provided facts, abbreviations, and terminology for better comprehension; LLM Response Refined Memory Agent: Extracts key elements from responses for refinement and personalization. These agents analyse stored refined memory, detect patterns, resolve contradictions, and improve response accuracy. Temporal factors like recency and frequency prioritize relevant information while discarding outdated insights. MARK enhances LLMs in multiple ways: Ground Truth Strategy: Reduces hallucinations by establishing a structured reference; Domain-Specific Adaptation: Essential for fields like healthcare, law, and manufacturing, where proprietary insights are absent from public datasets; Personalized AI Assistants: Improves virtual assistants by remembering user preferences, ensuring coherent responses over time.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05177.pdf", "abstract_url": "https://arxiv.org/abs/2505.05177", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2505.05197", "title": "Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt", "authors": ["Joel Z. Leibo", "Alexander Sasha Vezhnevets", "William A. Cunningham", "Sébastien Krier", "Manfred Diaz", "Simon Osindero"], "abstract": "Artificial Intelligence (AI) systems are increasingly placed in positions where their decisions have real consequences, e.g., moderating online spaces, conducting research, and advising on policy. Ensuring they operate in a safe and ethically acceptable fashion is thus critical. However, most solutions have been a form of one-size-fits-all \"alignment\". We are worried that such systems, which overlook enduring moral diversity, will spark resistance, erode trust, and destabilize our institutions. This paper traces the underlying problem to an often-unstated Axiom of Rational Convergence: the idea that under ideal conditions, rational agents will converge in the limit of conversation on a single ethics. Treating that premise as both optional and doubtful, we propose what we call the appropriateness framework: an alternative approach grounded in conflict theory, cultural evolution, multi-agent systems, and institutional economics. The appropriateness framework treats persistent disagreement as the normal case and designs for it by applying four principles: (1) contextual grounding, (2) community customization, (3) continual adaptation, and (4) polycentric governance. We argue here that adopting these design principles is a good way to shift the main alignment metaphor from moral unification to a more productive metaphor of conflict management, and that taking this step is both desirable and urgent.", "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "comments": "16 pages", "pdf_url": "https://arxiv.org/pdf/2505.05197.pdf", "abstract_url": "https://arxiv.org/abs/2505.05197", "categories": ["Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)"], "matching_keywords": ["agent"]}
{"id": "2505.05440", "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation", "authors": ["Biao Yi", "Xavier Hu", "Yurun Chen", "Shengyu Zhang", "Hongxia Yang", "Fan Wu", "Fei Wu"], "abstract": "Cloud-based mobile agents powered by (multimodal) large language models ((M)LLMs) offer strong reasoning abilities but suffer from high latency and cost. While fine-tuned (M)SLMs enable edge deployment, they often lose general capabilities and struggle with complex tasks. To address this, we propose EcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile automation. EcoAgent features a closed-loop collaboration among a cloud-based Planning Agent and two edge-based agents: the Execution Agent for action execution and the Observation Agent for verifying outcomes. The Observation Agent uses a Pre-Understanding Module to compress screen images into concise text, reducing token usage. In case of failure, the Planning Agent retrieves screen history and replans via a Reflection Module. Experiments on AndroidWorld show that EcoAgent maintains high task success rates while significantly reducing MLLM token consumption, enabling efficient and practical mobile automation.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05440.pdf", "abstract_url": "https://arxiv.org/abs/2505.05440", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2505.05225", "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation", "authors": ["Mengze Hong", "Wailing Ng", "Di Jiang", "Chen Jason Zhang"], "abstract": "The rapid advancement of Chinese large language models (LLMs) underscores the need for domain-specific evaluations to ensure reliable applications. However, existing benchmarks often lack coverage in vertical domains and offer limited insights into the Chinese working context. Leveraging qualification exams as a unified framework for human expertise evaluation, we introduce QualBench, the first multi-domain Chinese QA benchmark dedicated to localized assessment of Chinese LLMs. The dataset includes over 17,000 questions across six vertical domains, with data selections grounded in 24 Chinese qualifications to closely align with national policies and working standards. Through comprehensive evaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with Chinese LLMs consistently surpassing non-Chinese models, highlighting the importance of localized domain knowledge in meeting qualification requirements. The best performance of 75.26% reveals the current gaps in domain coverage within model capabilities. Furthermore, we present the failure of LLM collaboration with crowdsourcing mechanisms and suggest the opportunities for multi-domain RAG knowledge enhancement and vertical domain LLM training with Federated Learning.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05225.pdf", "abstract_url": "https://arxiv.org/abs/2505.05225", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"]}
{"id": "2505.05445", "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "abstract": "The emergence of instruction-tuned large language models (LLMs) has advanced the field of dialogue systems, enabling both realistic user simulations and robust multi-turn conversational agents. However, existing research often evaluates these components in isolation-either focusing on a single user simulator or a specific system design-limiting the generalisability of insights across architectures and configurations. In this work, we propose clem todd (chat-optimized LLMs for task-oriented dialogue systems development), a flexible framework for systematically evaluating dialogue systems under consistent conditions. clem todd enables detailed benchmarking across combinations of user simulators and dialogue systems, whether existing models from literature or newly developed ones. It supports plug-and-play integration and ensures uniform datasets, evaluation metrics, and computational constraints. We showcase clem todd's flexibility by re-evaluating existing task-oriented dialogue systems within this unified setup and integrating three newly proposed dialogue systems into the same evaluation pipeline. Our results provide actionable insights into how architecture, scale, and prompting strategies affect dialogue performance, offering practical guidance for building efficient and effective conversational AI systems.", "subjects": "Computation and Language (cs.CL)", "comments": "30 pages", "pdf_url": "https://arxiv.org/pdf/2505.05445.pdf", "abstract_url": "https://arxiv.org/abs/2505.05445", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2505.04725", "title": "Geometric Fault-Tolerant Neural Network Tracking Control of Unknown Systems on Matrix Lie Groups", "authors": ["Robin Chhabra", "Farzaneh Abdollahi"], "abstract": "We present a geometric neural network-based tracking controller for systems evolving on matrix Lie groups under unknown dynamics, actuator faults, and bounded disturbances. Leveraging the left-invariance of the tangent bundle of matrix Lie groups, viewed as an embedded submanifold of the vector space $\\R^{N\\times N}$, we propose a set of learning rules for neural network weights that are intrinsically compatible with the Lie group structure and do not require explicit parameterization. Exploiting the geometric properties of Lie groups, this approach circumvents parameterization singularities and enables a global search for optimal weights. The ultimate boundedness of all error signals -- including the neural network weights, the coordinate-free configuration error function, and the tracking velocity error -- is established using Lyapunov's direct method. To validate the effectiveness of the proposed method, we provide illustrative simulation results for decentralized formation control of multi-agent systems on the Special Euclidean group.", "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Robotics (cs.RO); Dynamical Systems (math.DS)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.04725.pdf", "abstract_url": "https://arxiv.org/abs/2505.04725", "categories": ["Systems and Control (eess.SY)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)", "Dynamical Systems (math.DS)"], "matching_keywords": ["agent"]}
{"id": "2505.04784", "title": "A Proposal for Evaluating the Operational Risk for ChatBots based on Large Language Models", "authors": ["Pedro Pinacho-Davidson", "Fernando Gutierrez", "Pablo Zapata", "Rodolfo Vergara", "Pablo Aqueveque"], "abstract": "The emergence of Generative AI (Gen AI) and Large Language Models (LLMs) has enabled more advanced chatbots capable of human-like interactions. However, these conversational agents introduce a broader set of operational risks that extend beyond traditional cybersecurity considerations. In this work, we propose a novel, instrumented risk-assessment metric that simultaneously evaluates potential threats to three key stakeholders: the service-providing organization, end users, and third parties. Our approach incorporates the technical complexity required to induce erroneous behaviors in the chatbot--ranging from non-induced failures to advanced prompt-injection attacks--as well as contextual factors such as the target industry, user age range, and vulnerability severity. To validate our metric, we leverage Garak, an open-source framework for LLM vulnerability testing. We further enhance Garak to capture a variety of threat vectors (e.g., misinformation, code hallucinations, social engineering, and malicious code generation). Our methodology is demonstrated in a scenario involving chatbots that employ retrieval-augmented generation (RAG), showing how the aggregated risk scores guide both short-term mitigation and longer-term improvements in model design and deployment. The results underscore the importance of multi-dimensional risk assessments in operationalizing secure, reliable AI-driven conversational systems.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)", "comments": "21 pages", "pdf_url": "https://arxiv.org/pdf/2505.04784.pdf", "abstract_url": "https://arxiv.org/abs/2505.04784", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)", "Computers and Society (cs.CY)"], "matching_keywords": ["agent", "@RAG"]}
{"id": "2505.04846", "title": "HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights", "authors": ["Ozan Gokdemir", "Carlo Siebenschuh", "Alexander Brace", "Azton Wells", "Brian Hsu", "Kyle Hippe", "Priyanka V. Setty", "Aswathy Ajith", "J. Gregory Pauloski", "Varuni Sastry", "Sam Foreman", "Huihuo Zheng", "Heng Ma", "Bharat Kale", "Nicholas Chia", "Thomas Gibbs", "Michael E. Papka", "Thomas Brettin", "Francis J. Alexander", "Anima Anandkumar", "Ian Foster", "Rick Stevens", "Venkatram Vishwanath", "Arvind Ramanathan"], "abstract": "The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.", "subjects": "Information Retrieval (cs.IR); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)", "comments": "This paper has been accepted at the Platform for Advanced Scientific Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland", "pdf_url": "https://arxiv.org/pdf/2505.04846.pdf", "abstract_url": "https://arxiv.org/abs/2505.04846", "categories": ["Information Retrieval (cs.IR)", "Computational Engineering, Finance, and Science (cs.CE)", "Computation and Language (cs.CL)", "Distributed, Parallel, and Cluster Computing (cs.DC)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2505.05223", "title": "Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving", "authors": ["Hendrik Surmann", "Jorge de Heuvel", "Maren Bennewitz"], "abstract": "Human drivers exhibit individual preferences regarding driving style. Adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. However, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. We propose a novel approach using multi-objective reinforcement learning (MORL) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. Preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\\unicode{x2013}$without requiring policy retraining. Our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the CARLA simulator. Experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion.", "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05223.pdf", "abstract_url": "https://arxiv.org/abs/2505.05223", "categories": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2505.05015", "title": "An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication", "authors": ["Roberto Dillon", "Arushi"], "abstract": "Continuous authentication systems leveraging free-text keyboard dynamics offer a promising additional layer of security in a multifactor authentication setup that can be used in a transparent way with no impact on user experience. This study investigates the efficacy of behavioral biometrics by employing an Agent-Based Model (ABM) to simulate diverse typing profiles across mechanical and membrane keyboards. Specifically, we generated synthetic keystroke data from five unique agents, capturing features related to dwell time, flight time, and error rates within sliding 5-second windows updated every second. Two machine learning approaches, One-Class Support Vector Machine (OC-SVM) and Random Forest (RF), were evaluated for user verification. Results revealed a stark contrast in performance: while One-Class SVM failed to differentiate individual users within each group, Random Forest achieved robust intra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize across keyboards for the same user, highlighting the significant impact of keyboard hardware on typing behavior. These findings suggest that: (1) keyboard-specific user profiles may be necessary for reliable authentication, and (2) ensemble methods like RF outperform One-Class SVM in capturing fine-grained user-specific patterns.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)", "comments": "16 pages, 5 figures, 12 tables", "pdf_url": "https://arxiv.org/pdf/2505.05015.pdf", "abstract_url": "https://arxiv.org/abs/2505.05015", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Cryptography and Security (cs.CR)"], "matching_keywords": ["agent"]}
{"id": "2505.05211", "title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality", "authors": ["Chara Podimata"], "abstract": "The article explores the emerging domain of incentive-aware machine learning (ML), which focuses on algorithmic decision-making in contexts where individuals can strategically modify their inputs to influence outcomes. It categorizes the research into three perspectives: robustness, aiming to design models resilient to \"gaming\"; fairness, analyzing the societal impacts of such systems; and improvement/causality, recognizing situations where strategic actions lead to genuine personal or societal improvement. The paper introduces a unified framework encapsulating models for these perspectives, including offline, online, and causal settings, and highlights key challenges such as differentiating between gaming and improvement and addressing heterogeneity among agents. By synthesizing findings from diverse works, we outline theoretical advancements and practical solutions for robust, fair, and causally-informed incentive-aware ML systems.", "subjects": "Computer Science and Game Theory (cs.GT); Artificial Intelligence (cs.AI)", "comments": "This literature review was published in SIGEcom Exchanges in 2025", "pdf_url": "https://arxiv.org/pdf/2505.05211.pdf", "abstract_url": "https://arxiv.org/abs/2505.05211", "categories": ["Computer Science and Game Theory (cs.GT)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2505.05262", "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration", "authors": ["Andreas Kontogiannis", "Konstantinos Papathanasiou", "Yi Shen", "Giorgos Stamou", "Michael M. Zavlanos", "George Vouros"], "abstract": "Learning to cooperate in distributed partially observable environments with no communication abilities poses significant challenges for multi-agent deep reinforcement learning (MARL). This paper addresses key concerns in this domain, focusing on inferring state representations from individual agent observations and leveraging these representations to enhance agents' exploration and collaborative task execution policies. To this end, we propose a novel state modelling framework for cooperative MARL, where agents infer meaningful belief representations of the non-observable state, with respect to optimizing their own policies, while filtering redundant and less informative joint state information. Building upon this framework, we propose the MARL SMPE algorithm. In SMPE, agents enhance their own policy's discriminative abilities under partial observability, explicitly by incorporating their beliefs into the policy network, and implicitly by adopting an adversarial type of exploration policies which encourages agents to discover novel, high-value states while improving the discriminative abilities of others. Experimentally, we show that SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative tasks from the MPE, LBF, and RWARE benchmarks.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "Accepted (Poster) at ICML 2025", "pdf_url": "https://arxiv.org/pdf/2505.05262.pdf", "abstract_url": "https://arxiv.org/abs/2505.05262", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2505.05283", "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for CodeLLMs and Agents", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Chong Wang", "Weisong Sun", "Yang Liu", "Bin Shi"], "abstract": "Code large language models (CodeLLMs) and agents have shown great promise in tackling complex software engineering", "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2505.05283.pdf", "abstract_url": "https://arxiv.org/abs/2505.05283", "categories": ["Software Engineering (cs.SE)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
