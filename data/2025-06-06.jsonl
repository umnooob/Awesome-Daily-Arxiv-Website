{"id": "2506.04363", "title": "WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning", "authors": ["Delong Chen", "Willy Chung", "Yejin Bang", "Ziwei Ji", "Pascale Fung"], "abstract": "Humans are known to have an internal \"world model\" that enables us to carry out action planning based on world states. AI agents need to have such a world model for action planning as well. It is not clear how current AI models, especially generative models, are able to learn such world models and carry out procedural planning in diverse environments. We introduce WorldPrediction, a video-based benchmark for evaluating world modeling and procedural planning capabilities of different AI models. In contrast to prior benchmarks that focus primarily on low-level world modeling and robotic motion planning, WorldPrediction is the first benchmark that emphasizes actions with temporal and semantic abstraction. Given initial and final world states, the task is to distinguish the proper action (WorldPrediction-WM) or the properly ordered sequence of actions (WorldPrediction-PP) from a set of counterfactual distractors. This discriminative task setup enable us to evaluate different types of world models and planners and realize a thorough comparison across different hypothesis. The benchmark represents states and actions using visual observations. In order to prevent models from exploiting low-level continuity cues in background scenes, we provide \"action equivalents\" - identical actions observed in different contexts - as candidates for selection. This benchmark is grounded in a formal framework of partially observable semi-MDP, ensuring better reliability and robustness of the evaluation. We conduct extensive human filtering and validation on our benchmark and show that current frontier models barely achieve 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP whereas humans are able to solve both tasks perfectly.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04363.pdf", "abstract_url": "https://arxiv.org/abs/2506.04363", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2506.04595", "title": "Hierarchical-Task-Aware Multi-modal Mixture of Incremental LoRA Experts for Embodied Continual Learning", "authors": ["Ziqi Jia", "Anmin Wang", "Xiaoyang Qu", "Xiaowen Yang", "Jianzong Wang"], "abstract": "Previous continual learning setups for embodied intelligence focused on executing low-level actions based on human commands, neglecting the ability to learn high-level planning and multi-level knowledge. To address these issues, we propose the Hierarchical Embodied Continual Learning Setups (HEC) that divide the agent's continual learning process into two layers: high-level instructions and low-level actions, and define five embodied continual learning sub-setups. Building on these setups, we introduce the Task-aware Mixture of Incremental LoRA Experts (Task-aware MoILE) method. This approach achieves task recognition by clustering visual-text embeddings and uses both a task-level router and a token-level router to select the appropriate LoRA experts. To effectively address the issue of catastrophic forgetting, we apply Singular Value Decomposition (SVD) to the LoRA parameters obtained from prior tasks, preserving key components while orthogonally training the remaining parts. The experimental results show that our method stands out in reducing the forgetting of old tasks compared to other methods, effectively supporting agents in retaining prior knowledge while continuously learning new tasks.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "Accepted by the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)", "pdf_url": "https://arxiv.org/pdf/2506.04595.pdf", "abstract_url": "https://arxiv.org/abs/2506.04595", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2506.04606", "title": "SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents", "authors": ["Alexander Huang-Menders", "Xinhang Liu", "Andy Xu", "Yuyao Zhang", "Chi-Keung Tang", "Yu-Wing Tai"], "abstract": "SmartAvatar is a vision-language-agent-driven framework for generating fully rigged, animation-ready 3D human avatars from a single photo or textual prompt. While diffusion-based methods have made progress in general 3D object generation, they continue to struggle with precise control over human identity, body shape, and animation readiness. In contrast, SmartAvatar leverages the commonsense reasoning capabilities of large vision-language models (VLMs) in combination with off-the-shelf parametric human generators to deliver high-quality, customizable avatars. A key innovation is an autonomous verification loop, where the agent renders draft avatars, evaluates facial similarity, anatomical plausibility, and prompt alignment, and iteratively adjusts generation parameters for convergence. This interactive, AI-guided refinement process promotes fine-grained control over both facial and body features, enabling users to iteratively refine their avatars via natural-language conversations. Unlike diffusion models that rely on static pre-trained datasets and offer limited flexibility, SmartAvatar brings users into the modeling loop and ensures continuous improvement through an LLM-driven procedural generation and verification system. The generated avatars are fully rigged and support pose manipulation with consistent identity and appearance, making them suitable for downstream animation and interactive applications. Quantitative benchmarks and user studies demonstrate that SmartAvatar outperforms recent text- and image-driven avatar generation systems in terms of reconstructed mesh quality, identity fidelity, attribute accuracy, and animation readiness, making it a versatile tool for realistic, customizable avatar creation on consumer-grade hardware.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "16 pages", "pdf_url": "https://arxiv.org/pdf/2506.04606.pdf", "abstract_url": "https://arxiv.org/abs/2506.04606", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2506.04676", "title": "Gen-n-Val: Agentic Image Data Generation and Validation", "authors": ["Jing-En Huang", "I-Sheng Fang", "Tzuhsuan Huang", "Chih-Yu Wang", "Jun-Cheng Chen"], "abstract": "Recently, Large Language Models (LLMs) and Vision Large Language Models (VLLMs) have demonstrated impressive performance as agents across various tasks while data scarcity and label noise remain significant challenges in computer vision tasks, such as object detection and instance segmentation. A common solution for resolving these issues is to generate synthetic data. However, current synthetic data generation methods struggle with issues, such as multiple objects per mask, inaccurate segmentation, and incorrect category labels, limiting their effectiveness. To address these issues, we introduce Gen-n-Val, a novel agentic data generation framework that leverages Layer Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt agent, an LLM, optimizes prompts for LD to generate high-quality foreground instance images and segmentation masks. These optimized prompts ensure the generation of single-object synthetic data with precise instance masks and clean backgrounds. (2) The data validation agent, a VLLM, which filters out low-quality synthetic instance images. The system prompts for both agents are refined through TextGrad. Additionally, we use image harmonization to combine multiple instances within scenes. Compared to state-of-the-art synthetic data approaches like MosaicFusion, our approach reduces invalid synthetic data from 50% to 7% and improves performance by 1% mAP on rare classes in COCO instance segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance of YOLOv9 and YOLO11 families in instance segmentation and object detection.", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04676.pdf", "abstract_url": "https://arxiv.org/abs/2506.04676", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.04743", "title": "SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs", "authors": ["Shuhan Xu", "Siyuan Liang", "Hongling Zheng", "Yong Luo", "Aishan Liu", "Dacheng Tao"], "abstract": "Vision-Language Models (VLMs) have achieved remarkable performance in image captioning, but recent studies show they are vulnerable to backdoor attacks. Attackers can inject imperceptible perturbations-such as local pixel triggers or global semantic phrases-into the training data, causing the model to generate malicious, attacker-controlled captions for specific inputs. These attacks are hard to detect and defend due to their stealthiness and cross-modal nature. By analyzing attack samples, we identify two key vulnerabilities: (1) abnormal attention concentration on specific image regions, and (2) semantic drift and incoherence in generated captions. To counter this, we propose Semantic Reward Defense (SRD), a reinforcement learning framework that mitigates backdoor behavior without prior knowledge of triggers. SRD uses a Deep Q-Network to learn policies for applying discrete perturbations (e.g., occlusion, color masking) to sensitive image regions, aiming to disrupt the activation of malicious pathways. We design a semantic fidelity score as the reward signal, which jointly evaluates semantic consistency and linguistic fluency of the output, guiding the agent toward generating robust yet faithful captions. Experiments across mainstream VLMs and datasets show SRD reduces attack success rates to 5.6%, while preserving caption quality on clean inputs with less than 10% performance drop. SRD offers a trigger-agnostic, interpretable defense paradigm against stealthy backdoor threats in multimodal generative models.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04743.pdf", "abstract_url": "https://arxiv.org/abs/2506.04743", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2506.04790", "title": "LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table", "authors": ["Yusuke Matsui"], "abstract": "Approximate nearest neighbor search (ANNS) is an essential building block for applications like RAG but can sometimes yield results that are overly similar to each other. In certain scenarios, search results should be similar to the query and yet diverse. We propose LotusFilter, a post-processing module to diversify ANNS results. We precompute a cutoff table summarizing vectors that are close to each other. During the filtering, LotusFilter greedily looks up the table to delete redundant vectors from the candidates. We demonstrated that the LotusFilter operates fast (0.02 [ms/query]) in settings resembling real-world RAG applications, utilizing features such as OpenAI embeddings. Our code is publicly available at", "subjects": "Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Machine Learning (cs.LG)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2506.04790.pdf", "abstract_url": "https://arxiv.org/abs/2506.04790", "categories": ["Computer Vision and Pattern Recognition (cs.CV)", "Information Retrieval (cs.IR)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2506.04245", "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning", "authors": ["Guangchen Lan", "Huseyin A. Inan", "Sahar Abdelnabi", "Janardhan Kulkarni", "Lukas Wutschitz", "Reza Shokri", "Christopher G. Brinton", "Robert Sim"], "abstract": "As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04245.pdf", "abstract_url": "https://arxiv.org/abs/2506.04245", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.04251", "title": "Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation", "authors": ["Zhengyang Li"], "abstract": "This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04251.pdf", "abstract_url": "https://arxiv.org/abs/2506.04251", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2506.04252", "title": "A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy", "authors": ["Yang Zhao", "Chengxiao Dai", "Dusit Niyato", "Chuan Fu Tan", "Keyi Xiang", "Yueyang Wang", "Zhiquan Yeo", "Daren Tan Zong Loong", "Jonathan Low Zhaozhi", "Eugene H.Z. HO"], "abstract": "Large language models (LLMs) hold promise for sustainable manufacturing, but often hallucinate industrial codes and emission factors, undermining regulatory and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented generation (RAG) framework that grounds LLMs outputs in a domain-specific knowledge graph for the circular economy. This graph connects 117,380 industrial and waste entities with classification codes and GWP100 emission data, enabling structured multi-hop reasoning. Natural language queries are translated into SPARQL and verified subgraphs are retrieved to ensure accuracy and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG achieves superior performance in single-hop and multi-hop question answering, with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also improves efficiency, halving the response time and reducing token usage by 16% in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready support for circular economy planning, advancing reliable, low-carbon resource decision making.", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04252.pdf", "abstract_url": "https://arxiv.org/abs/2506.04252", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2506.04253", "title": "HADA: Human-AI Agent Decision Alignment Architecture", "authors": ["Tapio Pitkäranta", "Leena Pitkäranta"], "abstract": "We present HADA (Human-AI Agent Decision Alignment), a protocol- and framework agnostic reference architecture that keeps both large language model (LLM) agents and legacy algorithms aligned with organizational targets and values. HADA wraps any algorithm or LLM in role-specific stakeholder agents -- business, data-science, audit, ethics, and customer -- each exposing conversational APIs so that technical and non-technical actors can query, steer, audit, or contest every decision across strategic, tactical, and real-time horizons. Alignment objectives, KPIs, and value constraints are expressed in natural language and are continuously propagated, logged, and versioned while thousands of heterogeneous agents run on different orchestration stacks. A cloud-native proof of concept packages a production credit-scoring model (getLoanDecision) and deploys it on Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target changes, parameter tweaks, explanation requests, and ethics triggers flow end to end through the architecture. Evaluation followed the Design-Science Research Methodology. Walkthrough observation and log inspection demonstrated complete coverage of six predefined objectives: every role could invoke conversational control, trace KPIs and value constraints, detect and mitigate ZIP-code bias, and reproduce full decision lineage, independent of the underlying LLM or agent library. Contributions: (1) an open-source HADA architecture, (2) a mid-range design theory for human-AI alignment in multi-agent systems, and (3) empirical evidence that framework-agnostic, protocol-compliant stakeholder agents improve accuracy, transparency, and ethical compliance in real-world decision pipelines.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "18 pages, 4 figures", "pdf_url": "https://arxiv.org/pdf/2506.04253.pdf", "abstract_url": "https://arxiv.org/abs/2506.04253", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04287", "title": "Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback", "authors": ["Yongjin Yang", "Sinjae Kang", "Juyong Lee", "Dongjun Lee", "Se-Young Yun", "Kimin Lee"], "abstract": "Training large language model (LLM) agents to acquire necessary skills and perform diverse tasks within an environment is gaining interest as a means to enable open-endedness. However, creating the training dataset for their skill acquisition faces several challenges. Manual trajectory collection requires significant human effort. Another approach, where LLMs directly propose tasks to learn, is often invalid, as the LLMs lack knowledge of which tasks are actually feasible. Moreover, the generated data may not provide a meaningful learning signal, as agents often already perform well on the proposed tasks. To address this, we propose a novel automatic skill discovery framework EXIF for LLM-powered agents, designed to improve the feasibility of generated target behaviors while accounting for the agents' capabilities. Our method adopts an exploration-first strategy by employing an exploration agent (Alice) to train the target agent (Bob) to learn essential skills in the environment. Specifically, Alice first interacts with the environment to retrospectively generate a feasible, environment-grounded skill dataset, which is then used to train Bob. Crucially, we incorporate an iterative feedback loop, where Alice evaluates Bob's performance to identify areas for improvement. This feedback then guides Alice's next round of exploration, forming a closed-loop data generation process. Experiments on Webshop and Crafter demonstrate EXIF's ability to effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without any human intervention, achieving substantial performance improvements. Interestingly, we observe that setting Alice to the same model as Bob also notably improves performance, demonstrating EXIF's potential for building a self-evolving system.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "Preprint, under review", "pdf_url": "https://arxiv.org/pdf/2506.04287.pdf", "abstract_url": "https://arxiv.org/abs/2506.04287", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.04481", "title": "CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective", "authors": ["Jiayu Liu", "Zhenya Huang", "Wei Dai", "Cheng Cheng", "Jinze Wu", "Jing Sha", "Song Li", "Qi Liu", "Shijin Wang", "Enhong Chen"], "abstract": "Although large language models (LLMs) show promise in solving complex mathematical tasks, existing evaluation paradigms rely solely on a coarse measure of overall answer accuracy, which are insufficient for assessing their authentic capabilities. In this paper, we propose \\textbf{CogMath}, which comprehensively assesses LLMs' mathematical abilities through the lens of human cognition. Specifically, inspired by psychological theories, CogMath formalizes human reasoning process into 3 stages: \\emph{problem comprehension}, \\emph{problem solving}, and \\emph{solution summarization}. Within these stages, we investigate perspectives such as numerical calculation, knowledge, and counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In each dimension, we develop an ``\\emph{Inquiry}-\\emph{Judge}-\\emph{Reference}'' multi-agent system to generate inquiries that assess LLMs' mastery from this dimension. An LLM is considered to truly master a problem only when excelling in all inquiries from the 9 dimensions. By applying CogMath on three benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs are overestimated by 30\\%-40\\%. Moreover, we locate their strengths and weaknesses across specific stages/dimensions, offering in-depth insights to further enhance their reasoning abilities.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04481.pdf", "abstract_url": "https://arxiv.org/abs/2506.04481", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04571", "title": "OpenAg: Democratizing Agricultural Intelligence", "authors": ["Srikanth Thudumu", "Jason Fisher"], "abstract": "Agriculture is undergoing a major transformation driven by artificial intelligence (AI), machine learning, and knowledge representation technologies. However, current agricultural intelligence systems often lack contextual understanding, explainability, and adaptability, especially for smallholder farmers with limited resources. General-purpose large language models (LLMs), while powerful, typically lack the domain-specific knowledge and contextual reasoning needed for practical decision support in farming. They tend to produce recommendations that are too generic or unrealistic for real-world applications. To address these challenges, we present OpenAg, a comprehensive framework designed to advance agricultural artificial general intelligence (AGI). OpenAg combines domain-specific foundation models, neural knowledge graphs, multi-agent reasoning, causal explainability, and adaptive transfer learning to deliver context-aware, explainable, and actionable insights. The system includes: (i) a unified agricultural knowledge base that integrates scientific literature, sensor data, and farmer-generated knowledge; (ii) a neural agricultural knowledge graph for structured reasoning and inference; (iii) an adaptive multi-agent reasoning system where AI agents specialize and collaborate across agricultural domains; and (iv) a causal transparency mechanism that ensures AI recommendations are interpretable, scientifically grounded, and aligned with real-world constraints. OpenAg aims to bridge the gap between scientific knowledge and the tacit expertise of experienced farmers to support scalable and locally relevant agricultural decision-making.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "10 pages, 1 figure", "pdf_url": "https://arxiv.org/pdf/2506.04571.pdf", "abstract_url": "https://arxiv.org/abs/2506.04571", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04636", "title": "CHANCERY: Evaluating corporate governance reasoning capabilities in language models", "authors": ["Lucas Irwin", "Arda Kaz", "Peiyao Sheng", "Pramod Viswanath"], "abstract": "Law has long been a domain that has been popular in natural language processing (NLP) applications. Reasoning (ratiocination and the ability to make connections to precedent) is a core part of the practice of the law in the real world. Nevertheless, while multiple legal datasets exist, none have thus far focused specifically on reasoning tasks. We focus on a specific aspect of the legal landscape by introducing a corporate governance reasoning benchmark (CHANCERY) to test a model's ability to reason about whether executive/board/shareholder's proposed actions are consistent with corporate governance charters. This benchmark introduces a first-of-its-kind corporate governance reasoning test for language models - modeled after real world corporate governance law. The benchmark consists of a corporate charter (a set of governing covenants) and a proposal for executive action. The model's task is one of binary classification: reason about whether the action is consistent with the rules contained within the charter. We create the benchmark following established principles of corporate governance - 24 concrete corporate governance principles established in and 79 real life corporate charters selected to represent diverse industries from a total dataset of 10k real life corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents exhibit superior performance, with agents based on the ReAct and CodeAct frameworks scoring 76.1% and 78.1% respectively, further confirming the advanced legal reasoning capabilities required to score highly on the benchmark. We also conduct an analysis of the types of questions which current reasoning models struggle on, revealing insights into the legal reasoning capabilities of SOTA models.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04636.pdf", "abstract_url": "https://arxiv.org/abs/2506.04636", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04651", "title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning", "authors": ["Nikolas Belle", "Dakota Barnes", "Alfonso Amayuelas", "Ivan Bercovich", "Xin Eric Wang", "William Wang"], "abstract": "Recent advances in LLMs have enabled their use as autonomous agents across a range of tasks, yet they continue to struggle with formulating and adhering to coherent long-term strategies. In this paper, we investigate whether LLM agents can self-improve when placed in environments that explicitly challenge their strategic planning abilities. Using the board game Settlers of Catan, accessed through the open-source Catanatron framework, we benchmark a progression of LLM-based agents, from a simple game-playing agent to systems capable of autonomously rewriting their own prompts and their player agent's code. We introduce a multi-agent architecture in which specialized roles (Analyzer, Researcher, Coder, and Player) collaborate to iteratively analyze gameplay, research new strategies, and modify the agent's logic or prompt. By comparing manually crafted agents to those evolved entirely by LLMs, we evaluate how effectively these systems can diagnose failure and adapt over time. Our results show that self-evolving agents, particularly when powered by models like Claude 3.7 and GPT-4o, outperform static baselines by autonomously adopting their strategies, passing along sample behavior to game-playing agents, and demonstrating adaptive reasoning over multiple iterations.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04651.pdf", "abstract_url": "https://arxiv.org/abs/2506.04651", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04654", "title": "E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction", "authors": ["Zhichao Yang", "Jiashu He", "Mohammad B. Al-Khasawneh", "Darshan Pandit", "Cirillo Cinzia"], "abstract": "Electric bicycles (e-bikes) are rapidly increasing in use, raising safety concerns due to a rise in accident reports. However, e-bike incident reports often use unstructured narrative formats, which hinders quantitative safety analysis. This study introduces E-bike agents, a framework that uses large language models (LLM) powered agents to classify and extract safety variables from unstructured incident reports. Our framework consists of four LLM agents, handling data classification, information extraction, injury cause determination, and component linkage, to extract the key factors that could lead to E-bike accidents and cause varying severity levels. Furthermore, we used an ordered logit model to examine the relationship between the severity of the incident and the factors retrieved, such as gender, the type of cause, and environmental conditions. Our research shows that equipment issues are slightly more common than human-related ones, but human-related incidents are more often fatal. Specifically, pedals, tires, and brakes are frequent contributors to accidents. The model achieves a high weighted F1 score of 0.87 in classification accuracy, highlighting the potential of using LLMs to extract unstructured data in niche domains, such as transportation. Our method offers a scalable solution to improve e-bike safety analytics and provides actionable information for policy makers, designers, and regulators.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04654.pdf", "abstract_url": "https://arxiv.org/abs/2506.04654", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04699", "title": "Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling", "authors": ["Bihan Xu", "Shiwei Zhao", "Runze Wu", "Zhenya Huang", "Jiawei Wang", "Zhipeng Hu", "Kai Wang", "Haoyu Liu", "Tangjie Lv", "Le Li", "Changjie Fan", "Xin Tong", "Jiangze Han"], "abstract": "Within the domain of Massively Multiplayer Online (MMO) economy research, Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game economics, evolving from rule-based agents to decision-making agents enhanced by reinforcement learning. Nevertheless, existing works encounter significant challenges when attempting to emulate human-like economic activities among agents, particularly regarding agent reliability, sociability, and interpretability. In this study, we take a preliminary step in introducing a novel approach using Large Language Models (LLMs) in MMO economy simulation. Leveraging LLMs' role-playing proficiency, generative capacity, and reasoning aptitude, we design LLM-driven agents with human-like decision-making and adaptability. These agents are equipped with the abilities of role-playing, perception, memory, and reasoning, addressing the aforementioned challenges effectively. Simulation experiments focusing on in-game economic activities demonstrate that LLM-empowered agents can promote emergent phenomena like role specialization and price fluctuations in line with market rules.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "KDD2025 Accepted", "pdf_url": "https://arxiv.org/pdf/2506.04699.pdf", "abstract_url": "https://arxiv.org/abs/2506.04699", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04828", "title": "Safe Planning and Policy Optimization via World Model Learning", "authors": ["Artem Latyshev", "Gregory Gorbov", "Aleksandr I. Panov"], "abstract": "Reinforcement Learning (RL) applications in real-world scenarios must prioritize safety and reliability, which impose strict constraints on agent behavior. Model-based RL leverages predictive world models for action planning and policy optimization, but inherent model inaccuracies can lead to catastrophic failures in safety-critical settings. We propose a novel model-based RL framework that jointly optimizes task performance and safety. To address world model errors, our method incorporates an adaptive mechanism that dynamically switches between model-based planning and direct policy execution. We resolve the objective mismatch problem of traditional model-based approaches using an implicit world model. Furthermore, our framework employs dynamic safety thresholds that adapt to the agent's evolving capabilities, consistently selecting actions that surpass safe policy suggestions in both performance and safety. Experiments demonstrate significant improvements over non-adaptive methods, showing that our approach optimizes safety and performance simultaneously rather than merely meeting minimum safety requirements. The proposed framework achieves robust performance on diverse safety-critical continuous control tasks, outperforming existing methods.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04828.pdf", "abstract_url": "https://arxiv.org/abs/2506.04828", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04849", "title": "Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles", "authors": ["Julien Soulé", "Jean-Paul Jamont", "Michel Occello", "Paul Théron", "Louis-Marie Traonouez"], "abstract": "As cyber-attacks show to be more and more complex and coordinated, cyber-defenders strategy through multi-agent approaches could be key to tackle against cyber-attacks as close as entry points in a networked system. This paper presents a Markovian modeling and implementation through a simulator of fighting cyber-attacker agents and cyber-defender agents deployed on host network nodes. It aims to provide an experimental framework to implement realistically based coordinated cyber-attack scenarios while assessing cyber-defenders dynamic organizations. We abstracted network nodes by sets of properties including agents' ones. Actions applied by agents model how the network reacts depending in a given state and what properties are to change. Collective choice of the actions brings the whole environment closer or farther from respective cyber-attackers and cyber-defenders goals. Using the simulator, we implemented a realistically inspired scenario with several behavior implementation approaches for cyber-defenders and cyber-attackers.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04849.pdf", "abstract_url": "https://arxiv.org/abs/2506.04849", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04867", "title": "LLMs for sensory-motor control: Combining in-context and iterative learning", "authors": ["Jônata Tyska Carvalho", "Stefano Nolfi"], "abstract": "We propose a method that enables large language models (LLMs) to control embodied agents by directly mapping continuous observation vectors to continuous action vectors. Initially, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. In most cases, it successfully identifies optimal or high-performing solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.", "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Robotics (cs.RO)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2506.04867.pdf", "abstract_url": "https://arxiv.org/abs/2506.04867", "categories": ["Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)", "Machine Learning (cs.LG)", "Robotics (cs.RO)"], "matching_keywords": ["agent"]}
{"id": "2506.04916", "title": "Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life", "authors": ["Atahan Karagoz"], "abstract": "This paper introduces Energentic Intelligence, a class of autonomous systems defined not by task performance, but by their capacity to sustain themselves through internal energy regulation. Departing from conventional reward-driven paradigms, these agents treat survival-maintaining functional operation under fluctuating energetic and thermal conditions-as the central objective. We formalize this principle through an energy-based utility function and a viability-constrained survival horizon, and propose a modular architecture that integrates energy harvesting, thermal regulation, and adaptive computation into a closed-loop control system. A simulated environment demonstrates the emergence of stable, resource-aware behavior without external supervision. Together, these contributions provide a theoretical and architectural foundation for deploying autonomous agents in resource-volatile settings where persistence must be self-regulated and infrastructure cannot be assumed.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04916.pdf", "abstract_url": "https://arxiv.org/abs/2506.04916", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent"]}
{"id": "2506.04998", "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning", "authors": ["Mehdi Azarafza", "Mojtaba Nayyeri", "Faezeh Pasandideh", "Steffen Staab", "Achim Rettberg"], "abstract": "Autonomous UAV operation necessitates reliable mathematical reasoning for tasks such as trajectory planning and power management. While traditional flight control relies on hardcoded equations, recent Large Language Models (LLMs) offer potential for more flexible problem-solving but struggle with reliably selecting and applying correct mathematical formulations and executing precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented generation framework designed to improve the mathematical reasoning of several LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in UAV-specific contexts by providing access to relevant domain literature. To conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem set comprising 20 UAV-centric mathematical problems across four difficulty levels. Our experiments demonstrate that incorporating retrieval substantially increases exact answer accuracy (achieving up to 75% with o1), reduces instances of incorrect formulation selection (from 25% without RAG to 5% with RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders of magnitude for the best-performing models. This pilot study indicates that RAG can enable general-purpose LLMs to function as more reliable tools for engineering analysis, although direct real-time flight control requires further investigation and validation on a larger scale. All benchmark data, question and answer are publicly available.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "15 pages, 7 figures, 4 appendix subsections", "pdf_url": "https://arxiv.org/pdf/2506.04998.pdf", "abstract_url": "https://arxiv.org/abs/2506.04998", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.05109", "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning", "authors": ["Tennison Liu", "Mihaela van der Schaar"], "abstract": "Self-improving agents aim to continuously acquire new capabilities with minimal supervision. However, current approaches face two key limitations: their self-improvement processes are often rigid, fail to generalize across tasks domains, and struggle to scale with increasing agent capabilities. We argue that effective self-improvement requires intrinsic metacognitive learning, defined as an agent's intrinsic ability to actively evaluate, reflect on, and adapt its own learning processes. Drawing inspiration from human metacognition, we introduce a formal framework comprising three components: metacognitive knowledge (self-assessment of capabilities, tasks, and learning strategies), metacognitive planning (deciding what and how to learn), and metacognitive evaluation (reflecting on learning experiences to improve future learning). Analyzing existing self-improving agents, we find they rely predominantly on extrinsic metacognitive mechanisms, which are fixed, human-designed loops that limit scalability and adaptability. Examining each component, we contend that many ingredients for intrinsic metacognition are already present. Finally, we explore how to optimally distribute metacognitive responsibilities between humans and agents, and robustly evaluate and improve intrinsic metacognitive learning, key challenges that must be addressed to enable truly sustained, generalized, and aligned self-improvement.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "Published as a conference paper at ICML 2025", "pdf_url": "https://arxiv.org/pdf/2506.05109.pdf", "abstract_url": "https://arxiv.org/abs/2506.05109", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.05296", "title": "Control Tax: The Price of Keeping AI in Check", "authors": ["Mikhail Terekhov", "Zhen Ning David Liu", "Caglar Gulcehre", "Samuel Albanie"], "abstract": "The rapid integration of agentic AI into high-stakes real-world applications requires robust oversight mechanisms. The emerging field of AI Control (AIC) aims to provide such an oversight mechanism, but practical adoption depends heavily on implementation overhead. To study this problem better, we introduce the notion of Control tax -- the operational and financial cost of integrating control measures into AI pipelines. Our work makes three key contributions to the field of AIC: (1) we introduce a theoretical framework that quantifies the Control Tax and maps classifier performance to safety assurances; (2) we conduct comprehensive evaluations of state-of-the-art language models in adversarial settings, where attacker models insert subtle backdoors into code while monitoring models attempt to detect these vulnerabilities; and (3) we provide empirical financial cost estimates for control protocols and develop optimized monitoring strategies that balance safety and cost-effectiveness while accounting for practical constraints like auditing budgets. Our framework enables practitioners to make informed decisions by systematically connecting safety guarantees with their costs, advancing AIC through principled economic feasibility assessment across different deployment contexts.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.05296.pdf", "abstract_url": "https://arxiv.org/abs/2506.05296", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.03922", "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.03922.pdf", "abstract_url": "https://arxiv.org/abs/2506.03922", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2506.04236", "title": "Spore in the Wild: Case Study on Spore.fun, a Real-World Experiment of Sovereign Agent Open-ended Evolution on Blockchain with TEEs", "authors": ["Botao Amber Hu", "Helena Rong"], "abstract": "In Artificial Life (ALife) research, replicating Open-Ended Evolution (OEE)-the continuous emergence of novelty observed in biological life-has traditionally been pursued within isolated closed system simulations, such as Tierra and Avida, which have typically plateaued after an initial burst of novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires an \"open\" system that continually exchanges information or energy with its environment. A recent technological innovation in decentralized physical infrastructure networks (DePIN) providing permissionless computational substrates enables deploying large language model (LLM)-based AI agents on blockchains integrated with Trusted Execution Environments (TEEs). This enables on-chain agents to operate autonomously \"in the wild,\" achieving self-sovereignty without human oversight. These agents can control their own social media accounts and cryptocurrency wallets, allowing them to interact directly with blockchain-based financial networks and broader human social media. Building on this new paradigm of on-chain agents,", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Neural and Evolutionary Computing (cs.NE)", "comments": "Submitted to ALIFE 2025", "pdf_url": "https://arxiv.org/pdf/2506.04236.pdf", "abstract_url": "https://arxiv.org/abs/2506.04236", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Human-Computer Interaction (cs.HC)", "Neural and Evolutionary Computing (cs.NE)"], "matching_keywords": ["agent"]}
{"id": "2506.04265", "title": "CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients", "authors": ["Mengda Ji", "Genjiu Xu", "Liying Wang"], "abstract": "This work focuses on the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). Sharing the global advantage among agents often leads to suboptimal policy updates as it fails to account for the distinct contributions of agents. Although numerous methods consider global or individual contributions for credit assignment, a detailed analysis at the coalition level remains lacking in many approaches. This work analyzes the over-updating problem during multi-agent policy updates from a coalition-level perspective. To address this issue, we propose a credit assignment method called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates coalitional advantages via marginal contributions from all possible coalitions and decomposes advantages using the core solution from cooperative game theory, ensuring coalitional rationality. To reduce computational overhead, CORA employs random coalition sampling. Experiments on matrix games, differential games, and multi-agent collaboration benchmarks demonstrate that CORA outperforms strong baselines, particularly in tasks with multiple local optima. These findings highlight the importance of coalition-aware credit assignment for improving MARL performance.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04265.pdf", "abstract_url": "https://arxiv.org/abs/2506.04265", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Computer Science and Game Theory (cs.GT)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.04276", "title": "Autonomous Collaborative Scheduling of Time-dependent UAVs, Workers and Vehicles for Crowdsensing in Disaster Response", "authors": ["Lei Han", "Yitong Guo", "Pengfei Yang", "Zhiyong Yu", "Liang Wang", "Quan Wang", "Zhiwen Yu"], "abstract": "Natural disasters have caused significant losses to human society, and the timely and efficient acquisition of post-disaster environmental information is crucial for the effective implementation of rescue operations. Due to the complexity of post-disaster environments, existing sensing technologies face challenges such as weak environmental adaptability, insufficient specialized sensing capabilities, and limited practicality of sensing solutions. This paper explores the heterogeneous multi-agent online autonomous collaborative scheduling algorithm HoAs-PALN, aimed at achieving efficient collection of post-disaster environmental information. HoAs-PALN is realized through adaptive dimensionality reduction in the matching process and local Nash equilibrium game, facilitating autonomous collaboration among time-dependent UAVs, workers and vehicles to enhance sensing scheduling. (1) In terms of adaptive dimensionality reduction during the matching process, HoAs-PALN significantly reduces scheduling decision time by transforming a five-dimensional matching process into two categories of three-dimensional matching processes; (2) Regarding the local Nash equilibrium game, HoAs-PALN combines the softmax function to optimize behavior selection probabilities and introduces a local Nash equilibrium determination mechanism to ensure scheduling decision performance. Finally, we conducted detailed experiments based on extensive real-world and simulated data. Compared with the baselines (GREEDY, K-WTA, MADL and MARL), HoAs-PALN improves task completion rates by 64.12%, 46.48%, 16.55%, and 14.03% on average, respectively, while each online scheduling decision takes less than 10 seconds, demonstrating its effectiveness in dynamic post-disaster environments.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04276.pdf", "abstract_url": "https://arxiv.org/abs/2506.04276", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04293", "title": "AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents", "authors": ["Fengze Liu", "Haoyu Wang", "Joonhyuk Cho", "Dan Roth", "Andrew W. Lo"], "abstract": "Clinical trials are critical for advancing medical treatments but remain prohibitively expensive and time-consuming. Accurate prediction of clinical trial outcomes can significantly reduce research and development costs and accelerate drug discovery. While recent deep learning models have shown promise by leveraging unstructured data, their black-box nature, lack of interpretability, and vulnerability to label leakage limit their practical use in high-stakes biomedical contexts. In this work, we propose AutoCT, a novel framework that combines the reasoning capabilities of large language models with the explainability of classical machine learning. AutoCT autonomously generates, evaluates, and refines tabular features based on public information without human input. Our method uses Monte Carlo Tree Search to iteratively optimize predictive performance. Experimental results show that AutoCT performs on par with or better than SOTA methods on clinical trial prediction tasks within only a limited number of self-refinement iterations, establishing a new paradigm for scalable, interpretable, and cost-efficient clinical trial prediction.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04293.pdf", "abstract_url": "https://arxiv.org/abs/2506.04293", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.04390", "title": "Through the Stealth Lens: Rethinking Attacks and Defenses in RAG", "authors": ["Sarthak Choudhary", "Nils Palumbo", "Ashish Hooda", "Krishnamurthy Dj Dvijotham", "Somesh Jha"], "abstract": "Retrieval-augmented generation (RAG) systems are vulnerable to attacks that inject poisoned passages into the retrieved set, even at low corruption rates. We show that existing attacks are not designed to be stealthy, allowing reliable detection and mitigation. We formalize stealth using a distinguishability-based security game. If a few poisoned passages are designed to control the response, they must differentiate themselves from benign ones, inherently compromising stealth. This motivates the need for attackers to rigorously analyze intermediate signals involved in generation$\\unicode{x2014}$such as attention patterns or next-token probability distributions$\\unicode{x2014}$to avoid easily detectable traces of manipulation. Leveraging attention patterns, we propose a passage-level score$\\unicode{x2014}$the Normalized Passage Attention Score$\\unicode{x2014}$used by our Attention-Variance Filter algorithm to identify and filter potentially poisoned passages. This method mitigates existing attacks, improving accuracy by up to $\\sim 20 \\%$ over baseline defenses. To probe the limits of attention-based defenses, we craft stealthier adaptive attacks that obscure such traces, achieving up to $35 \\%$ attack success rate, and highlight the challenges in improving stealth.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04390.pdf", "abstract_url": "https://arxiv.org/abs/2506.04390", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.04399", "title": "Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning", "authors": ["Suzan Ece Ada", "Emre Ugur"], "abstract": "We introduce Unsupervised Meta-Testing with Conditional Neural Processes (UMCNP), a novel hybrid few-shot meta-reinforcement learning (meta-RL) method that uniquely combines, yet distinctly separates, parameterized policy gradient-based (PPG) and task inference-based few-shot meta-RL. Tailored for settings where the reward signal is missing during meta-testing, our method increases sample efficiency without requiring additional samples in meta-training. UMCNP leverages the efficiency and scalability of Conditional Neural Processes (CNPs) to reduce the number of online interactions required in meta-testing. During meta-training, samples previously collected through PPG meta-RL are efficiently reused for learning task inference in an offline manner. UMCNP infers the latent representation of the transition dynamics model from a single test task rollout with unknown parameters. This approach allows us to generate rollouts for self-adaptation by interacting with the learned dynamics model. We demonstrate our method can adapt to an unseen test task using significantly fewer samples during meta-testing than the baselines in 2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with unknown angle sensor bias, walker agent with randomized dynamics parameters.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)", "comments": "Published in IEEE Robotics and Automation Letters Volume: 9, Issue: 10, 8427 - 8434, October 2024. 8 pages, 7 figures", "pdf_url": "https://arxiv.org/pdf/2506.04399.pdf", "abstract_url": "https://arxiv.org/abs/2506.04399", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)"], "matching_keywords": ["agent"]}
{"id": "2506.04405", "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale", "authors": ["Ran Xu", "Yuchen Zhuang", "Yishan Zhong", "Yue Yu", "Xiangru Tang", "Hang Wu", "May D. Wang", "Peifeng Ruan", "Donghan Yang", "Tao Wang", "Guanghua Xiao", "Carl Yang", "Yang Xie", "Wenqi Shi"], "abstract": "We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04405.pdf", "abstract_url": "https://arxiv.org/abs/2506.04405", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.04409", "title": "Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction", "authors": ["Lev Morozov", "Aleksandr Mogilevskii", "Alexander Shirnin"], "abstract": "This paper describes EmoRAG, a system designed to detect perceived emotions in text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We focus on predicting the perceived emotions of the speaker from a given text snippet, labeling it with emotions such as joy, sadness, fear, anger, surprise, and disgust. Our approach does not require additional model training and only uses an ensemble of models to predict emotions. EmoRAG achieves results comparable to the best performing systems, while being more efficient, scalable, and easier to implement.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "Accepted to SemEval-2025, an ACL 2025 workshop", "pdf_url": "https://arxiv.org/pdf/2506.04409.pdf", "abstract_url": "https://arxiv.org/abs/2506.04409", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.05010", "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development", "authors": ["Zhenran Xu", "Xue Yang", "Yiyu Wang", "Qingli Hu", "Zijiao Wu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "abstract": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at", "subjects": "Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2506.05010.pdf", "abstract_url": "https://arxiv.org/abs/2506.05010", "categories": ["Computation and Language (cs.CL)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"]}
{"id": "2506.04836", "title": "Oversight Structures for Agentic AI in Public-Sector Organizations", "authors": ["Chris Schmitz", "Jonathan Rystrøm", "Jan Batzner"], "abstract": "This paper finds that the introduction of agentic AI systems intensifies existing challenges to traditional public sector oversight mechanisms -- which rely on siloed compliance units and episodic approvals rather than continuous, integrated supervision. We identify five governance dimensions essential for responsible agent deployment: cross-departmental implementation, comprehensive evaluation, enhanced security protocols, operational visibility, and systematic auditing. We evaluate the capacity of existing oversight structures to meet these challenges, via a mixed-methods approach consisting of a literature review and interviews with civil servants in AI-related roles. We find that agent oversight poses intensified versions of three existing governance challenges: continuous oversight, deeper integration of governance and operational capabilities, and interdepartmental coordination. We propose approaches that both adapt institutional structures and design agent oversight compatible with public sector constraints.", "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)", "comments": "To appear at REALM@ACL2025", "pdf_url": "https://arxiv.org/pdf/2506.04836.pdf", "abstract_url": "https://arxiv.org/abs/2506.04836", "categories": ["Computers and Society (cs.CY)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.04838", "title": "On Automating Security Policies with Contemporary LLMs", "authors": ["Pablo Fernández Saura", "K. R. Jayaram", "Vatche Isahagian", "Jorge Bernal Bernabé", "Antonio Skarmeta"], "abstract": "The complexity of modern computing environments and the growing sophistication of cyber threats necessitate a more robust, adaptive, and automated approach to security enforcement. In this paper, we present a framework leveraging large language models (LLMs) for automating attack mitigation policy compliance through an innovative combination of in-context learning and retrieval-augmented generation (RAG). We begin by describing how our system collects and manages both tool and API specifications, storing them in a vector database to enable efficient retrieval of relevant information. We then detail the architectural pipeline that first decomposes high-level mitigation policies into discrete tasks and subsequently translates each task into a set of actionable API calls. Our empirical evaluation, conducted using publicly available CTI policies in STIXv2 format and Windows API documentation, demonstrates significant improvements in precision, recall, and F1-score when employing RAG compared to a non-RAG baseline.", "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)", "comments": "Short Paper. Accepted To Appear in IEEE SSE 2025 (part of SERVICES 2025)", "pdf_url": "https://arxiv.org/pdf/2506.04838.pdf", "abstract_url": "https://arxiv.org/abs/2506.04838", "categories": ["Cryptography and Security (cs.CR)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.05020", "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System", "authors": ["Haokun Liu", "Zhaoqi Ma", "Yunong Li", "Junichiro Sugihara", "Yicheng Chen", "Jinjie Li", "Moju Zhao"], "abstract": "Heterogeneous multi-robot systems show great potential in complex tasks requiring coordinated hybrid cooperation. However, traditional approaches relying on static models often struggle with task diversity and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical framework integrating a prompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision Language Model (VLM). The LLM performs task decomposition and global semantic map construction, while the VLM extracts task-specified semantic labels and 2D spatial information from aerial images to support local planning. Within this framework, the aerial robot follows a globally optimized semantic path and continuously provides bird-view images, guiding the ground robot's local semantic navigation and manipulation, including target-absent scenarios where implicit alignment is maintained. Experiments on a real-world letter-cubes arrangement task demonstrate the framework's adaptability and robustness in dynamic environments. To the best of our knowledge, this is the first demonstration of an aerial-ground heterogeneous system integrating VLM-based perception with LLM-driven task reasoning and motion planning.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.05020.pdf", "abstract_url": "https://arxiv.org/abs/2506.05020", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"]}
{"id": "2506.05154", "title": "Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation", "authors": ["Chenyu Lin", "Yilin Wen", "Du Su", "Fei Sun", "Muhan Chen", "Chenfu Bao", "Zhonghou Lv"], "abstract": "Retrieval-augmented generation (RAG) is a mainstream method for improving performance on knowledge-intensive tasks. However,current RAG systems often place too much emphasis on retrieved contexts. This can lead to reliance on inaccurate sources and overlook the model's inherent knowledge, especially when dealing with misleading or excessive information. To resolve this imbalance, we propose Knowledgeable-r1 that using joint sampling and define multi policy distributions in knowledge capability exploration to stimulate large language models'self-integrated utilization of parametric and contextual knowledge. Experiments show that Knowledgeable-r1 significantly enhances robustness and reasoning accuracy in both parameters and contextual conflict tasks and general RAG tasks, especially outperforming baselines by 17.07% in counterfactual scenarios and demonstrating consistent gains across RAG tasks. Our code are available at", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.05154.pdf", "abstract_url": "https://arxiv.org/abs/2506.05154", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2506.05167", "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG", "authors": ["Yeonseok Jeong", "Jinsu Kim", "Dohyeon Lee", "Seung-won Hwang"], "abstract": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or \\textbf{ECoRAG} framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.05167.pdf", "abstract_url": "https://arxiv.org/abs/2506.05167", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"]}
{"id": "2506.05265", "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams", "authors": ["Mohammed Almutairi"], "abstract": "Effective teamwork is essential across diverse domains. During the team formation stage, a key challenge is forming teams that effectively balance user preferences with task objectives to enhance overall team satisfaction. In the team performing stage, maintaining cohesion and engagement is critical for sustaining high team performance. However, existing computational tools and algorithms for team optimization often rely on static data inputs, narrow algorithmic objectives, or solutions tailored for specific contexts, failing to account for the dynamic interplay of team members personalities, evolving goals, and changing individual preferences. Therefore, teams may encounter member dissatisfaction, as purely algorithmic assignments can reduce members commitment to team goals or experience suboptimal engagement due to the absence of timely, personalized guidance to help members adjust their behaviors and interactions as team dynamics evolve. Ultimately, these challenges can lead to reduced overall team performance. My Ph.D. dissertation aims to develop AI-augmented team optimization frameworks and practical systems that enhance team satisfaction, engagement, and performance. First, I propose a team formation framework that leverages a multi-armed bandit algorithm to iteratively refine team composition based on user preferences, ensuring alignment between individual needs and collective team goals to enhance team satisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an AI-powered system that utilizes large language models (LLMs) to deliver immediate, personalized feedback to both teams and individual members, enhancing cohesion and engagement. Finally, I present PuppeteerLLM, an LLM-based simulation framework that simulates multi-agent teams to model complex team dynamics within realistic environments, incorporating task-driven collaboration and long-term coordination.", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "5 pages, UMAP 25, June 16_19, 2025, New York City, NY, USA", "pdf_url": "https://arxiv.org/pdf/2506.05265.pdf", "abstract_url": "https://arxiv.org/abs/2506.05265", "categories": ["Human-Computer Interaction (cs.HC)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"]}
{"id": "2506.05278", "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning", "authors": ["Nan Huo", "Jinyang Li", "Bowen Qin", "Ge Qu", "Xiaolong Li", "Xiaodong Li", "Chenhao Ma", "Reynold Cheng"], "abstract": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "Accepted by ACL 2025 Main", "pdf_url": "https://arxiv.org/pdf/2506.05278.pdf", "abstract_url": "https://arxiv.org/abs/2506.05278", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"]}
{"id": "2506.05305", "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback", "authors": ["Deepak Pandita", "Tharindu Cyril Weerasooriya", "Ankit Parag Shah", "Christopher M. Homan", "Wei Wei"], "abstract": "Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, are becoming increasingly prevalent. However, these workflows often suffer from error propagation and sub-optimal performance, largely due to poorly designed prompts that fail to effectively guide individual agents. This is a critical problem because it limits the reliability and scalability of these powerful systems. We introduce ProRefine, an innovative inference-time prompt optimization method that leverages textual feedback from large language models (LLMs) to address this challenge. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to match the performance of larger ones, highlighting its potential for efficient and scalable AI deployment, and democratizing access to high-performing AI.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.05305.pdf", "abstract_url": "https://arxiv.org/abs/2506.05305", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent", "agentic"]}
{"id": "2506.05309", "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games", "authors": ["Niv Eckhaus", "Uri Berger", "Gabriel Stanovsky"], "abstract": "LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.", "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.05309.pdf", "abstract_url": "https://arxiv.org/abs/2506.05309", "categories": ["Multiagent Systems (cs.MA)", "Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.04344", "title": "GEM: Empowering LLM for both Embedding Generation and Language Understanding", "authors": ["Caojin Zhang", "Qiang Zhang", "Ke Li", "Sai Vidyaranya Nuthalapati", "Benyu Zhang", "Jason Liu", "Serena Li", "Lizhu Zhang", "Xiangjun Fan"], "abstract": "Large decoder-only language models (LLMs) have achieved remarkable success in generation and reasoning tasks, where they generate text responses given instructions. However, many applications, e.g., retrieval augmented generation (RAG), still rely on separate embedding models to generate text embeddings, which can complicate the system and introduce discrepancies in understanding of the query between the embedding model and LLMs. To address this limitation, we propose a simple self-supervised approach, Generative Embedding large language Model (GEM), that enables any large decoder-only LLM to generate high-quality text embeddings while maintaining its original text generation and reasoning capabilities. Our method inserts new special token(s) into a text body, and generates summarization embedding of the text by manipulating the attention mask. This method could be easily integrated into post-training or fine tuning stages of any existing LLMs. We demonstrate the effectiveness of our approach by applying it to two popular LLM families, ranging from 1B to 8B parameters, and evaluating the transformed models on both text embedding benchmarks (MTEB) and NLP benchmarks (MMLU). The results show that our proposed method significantly improves the original LLMs on MTEB while having a minimal impact on MMLU. Our strong results indicate that our approach can empower LLMs with state-of-the-art text embedding capabilities while maintaining their original NLP performance", "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04344.pdf", "abstract_url": "https://arxiv.org/abs/2506.04344", "categories": ["Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["@RAG"]}
{"id": "2506.04389", "title": "Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care", "authors": ["Saurabh Kumar", "Sourav Bansal", "Neeraj Agrawal", "Priyanka Bhatt"], "abstract": "Customer care is an essential pillar of the e-commerce shopping experience with companies spending millions of dollars each year, employing automation and human agents, across geographies (like US, Canada, Mexico, Chile), channels (like Chat, Interactive Voice Response (IVR)), and languages (like English, Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on annotated data have shown good performance in downstream tasks relevant to Customer Care. However, model performance is largely subject to the availability of sufficient annotated domain-specific data. Cross-domain availability of data remains a bottleneck, thus building an intent classifier that generalizes across domains (defined by channel, geography, and language) with only a few annotations, is of great practical value. In this paper, we propose an embedder-cum-classifier model architecture which extends state-of-the-art domain-specific models to other domains with only a few labeled samples. We adopt a supervised fine-tuning approach with isotropic regularizers to train a domain-specific sentence embedder and a multilingual knowledge distillation strategy to generalize this embedder across multiple domains. The trained embedder, further augmented with a simple linear classifier can be deployed for new domains. Experiments on Canada and Mexico e-commerce Customer Care dataset with few-shot intent detection show an increase in accuracy by 20-23% against the existing state-of-the-art pre-trained models.", "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04389.pdf", "abstract_url": "https://arxiv.org/abs/2506.04389", "categories": ["Computation and Language (cs.CL)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"]}
{"id": "2506.04572", "title": "Demonstrations of Integrity Attacks in Multi-Agent Systems", "authors": ["Can Zheng", "Yuhan Cao", "Xiaoning Dong", "Tianxing He"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, code generation, and complex planning. Simultaneously, Multi-Agent Systems (MAS) have garnered attention for their potential to enable cooperation among distributed agents. However, from a multi-party perspective, MAS could be vulnerable to malicious agents that exploit the system to serve self-interests without disrupting its core functionality. This work explores integrity attacks where malicious agents employ subtle prompt manipulation to bias MAS operations and gain various benefits. Four types of attacks are examined: \\textit{Scapegoater}, who misleads the system monitor to underestimate other agents' contributions; \\textit{Boaster}, who misleads the system monitor to overestimate their own performance; \\textit{Self-Dealer}, who manipulates other agents to adopt certain tools; and \\textit{Free-Rider}, who hands off its own task to others. We demonstrate that strategically crafted prompts can introduce systematic biases in MAS behavior and executable instructions, enabling malicious agents to effectively mislead evaluation systems and manipulate collaborative agents. Furthermore, our attacks can bypass advanced LLM-based monitors, such as GPT-4o-mini and o3-mini, highlighting the limitations of current detection mechanisms. Our findings underscore the critical need for MAS architectures with robust security protocols and content validation mechanisms, alongside monitoring systems capable of comprehensive risk scenario assessment.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04572.pdf", "abstract_url": "https://arxiv.org/abs/2506.04572", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.04616", "title": "Subjective Perspectives within Learned Representations Predict High-Impact Innovation", "authors": ["Likun Cao", "Rui Pan", "James Evans"], "abstract": "Existing studies of innovation emphasize the power of social structures to shape innovation capacity. Emerging machine learning approaches, however, enable us to model innovators' personal perspectives and interpersonal innovation opportunities as a function of their prior trajectories of experience. We theorize then quantify subjective perspectives and innovation opportunities based on innovator positions within the geometric space of concepts inscribed by dynamic language representations. Using data on millions of scientists, inventors, writers, entrepreneurs, and Wikipedia contributors across the creative domains of science, technology, film, entrepreneurship, and Wikipedia, here we show that measured subjective perspectives anticipate what ideas individuals and groups creatively attend to and successfully combine in future. When perspective and background diversity are decomposed as the angular difference between collaborators' perspectives on their creation and between their experiences, the former consistently anticipates creative achievement while the latter portends its opposite, across all cases and time periods examined. We analyze a natural experiment and simulate creative collaborations between AI (large language model) agents designed with various perspective and background diversity, which are consistent with our observational findings. We explore mechanisms underlying these findings and identify how successful collaborators leverage common language to weave together diverse experience obtained through trajectories of prior work that converge to provoke one another and innovate. We explore the importance of these findings for team assembly and research policy.", "subjects": "Computation and Language (cs.CL); Applications (stat.AP); Machine Learning (stat.ML)", "comments": "107 pages, 20 figures", "pdf_url": "https://arxiv.org/pdf/2506.04616.pdf", "abstract_url": "https://arxiv.org/abs/2506.04616", "categories": ["Computation and Language (cs.CL)", "Applications (stat.AP)", "Machine Learning (stat.ML)"], "matching_keywords": ["agent"]}
{"id": "2506.04625", "title": "Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning", "authors": ["Zhiyuan Ma", "Jiayu Liu", "Xianzhen Luo", "Zhenya Huang", "Qingfu Zhu", "Wanxiang Che"], "abstract": "Empowering large language models (LLMs) with effective tool utilization capabilities is crucial for enabling AI agents to solve complex problems. However, current models face two major limitations: (1) unreliable tool planning and invocation due to low-quality instruction datasets (e.g., widespread hallucinated API calls), and (2) weak tool reflection abilities (over 90% of errors cannot be corrected) resulting from static imitation learning. To address these critical limitations, we propose Tool-MVR, a novel Tool-Augmented LLM that achieves comprehensive System 2 reasoning through two key innovations. Specifically, we first introduce Multi-Agent Meta-Verification (MAMV), a systematic pipeline that rigorously validates APIs, queries, and reasoning trajectories to construct ToolBench-V, a new high-quality instruction dataset that addresses the limitation of unreliable tool planning and invocation. Second, we propose Exploration-based Reflection Learning (EXPLORE), which enhances tool reflection capabilities by leveraging tool feedback through a dynamic \"Error -> Reflection -> Correction\" learning paradigm, resulting in our reflection dataset ToolBench-R and addressing the critical weakness in tool reflection. Finally, we obtain Tool-MVR by finetuning open-source LLMs (e.g., Qwen-7B) on both ToolBench-V and ToolBench-R. Our experiments demonstrate that Tool-MVR achieves state-of-the-art performance on StableToolBench, surpassing both ToolLLM (by 23.9%) and GPT-4 (by 15.3%) while reducing API calls by 31.4%, with strong generalization capabilities across unseen tools and scenarios. Additionally, on our proposed RefineToolBench, the first benchmark specifically designed to evaluate tool reflection capabilities, Tool-MVR achieves a 58.9% error correction rate, significantly outperforming ToolLLM's 9.1%.", "subjects": "Computation and Language (cs.CL)", "comments": "Accepted at the Research Track of KDD 2025", "pdf_url": "https://arxiv.org/pdf/2506.04625.pdf", "abstract_url": "https://arxiv.org/abs/2506.04625", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
{"id": "2506.04649", "title": "Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents", "authors": ["Juhyun Oh", "Eunsu Kim", "Alice Oh"], "abstract": "Real-world planning problems require constant adaptation to changing requirements and balancing of competing constraints. However, current benchmarks for evaluating LLMs' planning capabilities primarily focus on static, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark that evaluates language models' ability to reason flexibly in dynamic planning scenarios. Building on the TravelPlanner dataset~\\citep{xie2024travelplanner}, we introduce two novel evaluation settings: (1) sequential constraint introduction across multiple turns, and (2) scenarios with explicitly prioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B reveals several key findings: models' performance on single-turn tasks poorly predicts their ability to adapt plans across multiple turns; constraint introduction order significantly affects performance; and models struggle with constraint prioritization, often incorrectly favoring newly introduced lower priority preferences over existing higher-priority constraints. These findings highlight the importance of evaluating LLMs in more realistic, dynamic planning scenarios and suggest specific directions for improving model performance on complex planning tasks. The code and dataset for our framework are publicly available at", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2506.04649.pdf", "abstract_url": "https://arxiv.org/abs/2506.04649", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"]}
