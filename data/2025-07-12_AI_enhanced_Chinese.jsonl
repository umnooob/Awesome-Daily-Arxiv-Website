{"id": "2507.07307", "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation", "authors": ["Anirban Saha Anik", "Xiaoying Song", "Elliott Wang", "Bryan Wang", "Bengisu Yarimbas", "Lingzi Hong"], "abstract": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation (RAG) have demonstrated powerful capabilities in generating counterspeech against misinformation. However, current studies rely on limited evidence and offer less control over final outputs. To address these challenges, we propose a Multi-agent Retrieval-Augmented Framework to generate counterspeech against health misinformation, incorporating multiple LLMs to optimize knowledge retrieval, evidence enhancement, and response refinement. Our approach integrates both static and dynamic evidence, ensuring that the generated counterspeech is relevant, well-grounded, and up-to-date. Our method outperforms baseline approaches in politeness, relevance, informativeness, and factual accuracy, demonstrating its effectiveness in generating high-quality counterspeech. To further validate our approach, we conduct ablation studies to verify the necessity of each component in our framework. Furthermore, human evaluations reveal that refinement significantly enhances counterspeech quality and obtains human preference.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07307.pdf", "abstract_url": "https://arxiv.org/abs/2507.07307", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本文提出了一种多代理检索增强框架，用于生成针对健康错误信息的基于证据的反驳话语，通过整合多个大型语言模型优化知识检索、证据增强和回答精炼，显著提高了反驳话语的质量和准确性。", "motivation": "当前利用大型语言模型生成反驳错误信息的研究存在证据有限和对最终输出控制不足的问题，本文旨在解决这些挑战。", "method": "采用多代理检索增强框架，结合静态和动态证据，利用多个大型语言模型进行知识检索、证据增强和回答精炼。", "result": "该方法在礼貌性、相关性、信息量和事实准确性方面优于基线方法，人类评估证实精炼过程显著提高了反驳话语的质量。", "conclusion": "提出的框架有效生成了高质量的反驳话语，通过消融研究验证了各组成部分的必要性，展示了在对抗健康错误信息方面的潜力。"}}
{"id": "2507.07280", "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "abstract": "Interruption plays a crucial role in collaborative learning, shaping group interactions and influencing knowledge construction. AI-driven support can assist teachers in monitoring these interactions. However, most previous work on interruption detection and interpretation has been conducted in single-conversation environments with relatively clean audio. AI agents deployed in classrooms for collaborative learning within small groups will need to contend with multiple concurrent conversations -- in this context, overlapping speech will be ubiquitous, and interruptions will need to be identified in other ways. In this work, we analyze interruption detection in single-conversation and multi-group dialogue settings. We then create a state-of-the-art method for interruption identification that is robust to overlapping speech, and thus could be deployed in classrooms. Further, our work highlights meaningful linguistic and prosodic information about how interruptions manifest in collaborative group interactions. Our investigation also paves the way for future works to account for the influence of overlapping speech from multiple groups when tracking group dialog.", "subjects": "Computation and Language (cs.CL)", "comments": "Long Paper AIED 2025", "pdf_url": "https://arxiv.org/pdf/2507.07280.pdf", "abstract_url": "https://arxiv.org/abs/2507.07280", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文研究了背景语音对协作小组中打断检测的影响，提出了一种在重叠语音环境下仍能有效识别打断的最先进方法，并探讨了打断在协作小组互动中的语言和韵律特征。", "motivation": "解决在多人同时对话的环境中，如何有效检测和解释打断行为的问题，特别是在教室等协作学习场景中，重叠语音普遍存在的情况下。", "method": "分析了单对话和多组对话设置中的打断检测，创建了一种对重叠语音鲁棒的打断识别方法。", "result": "开发了一种在重叠语音环境下仍能有效识别打断的方法，并揭示了打断在协作小组互动中的语言和韵律特征。", "conclusion": "本研究不仅为教室等多人对话环境中的打断检测提供了实用的解决方案，还为未来研究如何考虑多组重叠语音对小组对话跟踪的影响奠定了基础。"}}
{"id": "2507.07115", "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation", "authors": ["Javal Vyas", "Mehmet Mercangoz"], "abstract": "The increasing complexity of modern chemical processes, coupled with workforce shortages and intricate fault scenarios, demands novel automation paradigms that blend symbolic reasoning with adaptive control. In this work, we introduce a unified agentic framework that leverages large language models (LLMs) for both discrete fault-recovery planning and continuous process control within a single architecture. We adopt Finite State Machines (FSMs) as interpretable operating envelopes: an LLM-driven planning agent proposes recovery sequences through the FSM, a Simulation Agent executes and checks each transition, and a Validator-Reprompting loop iteratively refines invalid plans. In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25 states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path success within five reprompts-outperforming open-source LLMs in both accuracy and latency. In Case Study 2, the same framework modulates dual-heater inputs on a laboratory TCLab platform (and its digital twin) to maintain a target average temperature under persistent asymmetric disturbances. Compared to classical PID control, our LLM-based controller attains similar performance, while ablation of the prompting loop reveals its critical role in handling nonlinear dynamics. We analyze key failure modes-such as instruction following lapses and coarse ODE approximations. Our results demonstrate that, with structured feedback and modular agents, LLMs can unify high-level symbolic planningand low-level continuous control, paving the way towards resilient, language-driven automation in chemical engineering.", "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07115.pdf", "abstract_url": "https://arxiv.org/abs/2507.07115", "categories": ["Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)", "Systems and Control (eess.SY)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文介绍了一种利用大型语言模型（LLMs）的统一代理框架，用于化学工程中的离散故障恢复规划和连续过程控制，通过案例研究展示了其在提高准确性和延迟方面的优势。", "motivation": "现代化学过程的复杂性增加、劳动力短缺以及复杂的故障场景，需要新的自动化范式，将符号推理与自适应控制相结合。", "method": "采用有限状态机（FSMs）作为可解释的操作框架，通过LLM驱动的规划代理提出恢复序列，模拟代理执行和检查每个转换，以及验证-重新提示循环迭代优化无效计划。", "result": "在案例研究1中，GPT-4o和GPT-4o-mini在随机生成的FSMs上实现了100%的有效路径成功率；在案例研究2中，LLM-based控制器在保持目标平均温度方面与经典PID控制表现相似，同时揭示了提示循环在处理非线性动态中的关键作用。", "conclusion": "研究表明，通过结构化反馈和模块化代理，LLMs可以统一高级符号规划和低级连续控制，为化学工程中的弹性、语言驱动自动化铺平道路。"}}
{"id": "2507.07257", "title": "Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery", "authors": ["Licong Xu", "Milind Sarkar", "Anto I. Lonappan", "Íñigo Zubeldia", "Pablo Villanueva-Domingo", "Santiago Casas", "Christian Fidler", "Chetana Amancharla", "Ujjwal Tiwari", "Adrian Bayer", "Chadi Ait Ekiou", "Miles Cranmer", "Adrian Dimitrov", "James Fergusson", "Kahaan Gandhi", "Sven Krippendorf", "Andrew Laverick", "Julien Lesgourgues", "Antony Lewis", "Thomas Meier", "Blake Sherwin", "Kristen Surrao", "Francisco Villaescusa-Navarro", "Chi Wang", "Xueqing Xu", "Boris Bolliet"], "abstract": "We present a multi-agent system for automation of scientific research tasks, cmbagent. The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud.", "subjects": "Artificial Intelligence (cs.AI); Instrumentation and Methods for Astrophysics (astro-ph.IM); Computation and Language (cs.CL); Multiagent Systems (cs.MA)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07257.pdf", "abstract_url": "https://arxiv.org/abs/2507.07257", "categories": ["Artificial Intelligence (cs.AI)", "Instrumentation and Methods for Astrophysics (astro-ph.IM)", "Computation and Language (cs.CL)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "介绍了一个名为cmbagent的多代理系统，用于自动化科学研究任务，该系统由约30个大型语言模型（LLM）代理组成，实现了无需人工干预的规划与控制策略，成功应用于宇宙学博士级任务。", "motivation": "解决科学研究任务自动化的问题，减少人工干预，提高研究效率。", "method": "采用多代理系统和规划与控制策略，每个代理专注于不同的任务，如检索科学论文和代码库、编写代码、解释结果等，并能本地执行代码。", "result": "在宇宙学参数测量任务中表现出色，性能优于最先进的LLM。", "conclusion": "cmbagent系统展示了自动化科学研究的潜力，其开源代码和部署资源为更广泛的应用提供了可能。"}}
{"id": "2507.07441", "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation", "authors": ["Yu Xia", "Yiran Jenny Shen", "Junda Wu", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Lina Yao", "Julian McAuley"], "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07441.pdf", "abstract_url": "https://arxiv.org/abs/2507.07441", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了SAND框架，通过自我教导的动作审议来提升LLM代理的性能，使其在行动前能明确审议候选动作，从而避免看似合理但次优的行动选择。", "motivation": "现有的LLM代理调优方法多侧重于模仿特定专家行为或偏好优化，缺乏对替代动作的深入推理和比较，导致代理可能过度承诺于看似合理但次优的动作。", "method": "SAND框架通过自我一致性动作采样和执行引导的动作批评，利用LLM代理的基础模型合成步骤级动作审议思想，并通过迭代方式将这些审议轨迹用于调优LLM代理本身。", "result": "在两个代表性交互代理任务上的评估显示，SAND比初始监督微调平均提高了20%的性能，并且优于最先进的代理调优方法。", "conclusion": "SAND框架通过引入动作审议机制，显著提升了LLM代理在复杂任务中的表现，为代理调优提供了新的方向。"}}
{"id": "2507.07302", "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation", "authors": ["Ashish Kumar"], "abstract": "Efficient exploration is a well known problem in deep reinforcement learning and this problem is exacerbated in multi-agent reinforcement learning due the intrinsic complexities of such algorithms. There are several approaches to efficiently explore an environment to learn to solve tasks by multi-agent operating in that environment, of which, the idea of expert exploration is investigated in this work. More specifically, this work investigates the application of large-language models as expert planners for efficient exploration in planning based tasks for multiple agents.", "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07302.pdf", "abstract_url": "https://arxiv.org/abs/2507.07302", "categories": ["Artificial Intelligence (cs.AI)", "Robotics (cs.RO)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文探讨了在多智能体强化学习中，利用大型语言模型作为专家规划器，以提高基于规划任务中多智能体的探索效率。", "motivation": "解决多智能体强化学习中的高效探索问题，特别是在基于规划的任务中，由于算法的内在复杂性，这一问题更加突出。", "method": "研究采用大型语言模型作为专家规划器，指导多智能体在环境中的探索。", "result": "研究表明，大型语言模型可以作为有效的专家规划器，提高多智能体在规划任务中的探索效率。", "conclusion": "利用大型语言模型作为专家规划器，是提高多智能体在复杂环境中探索效率的一种有前景的方法。"}}
{"id": "2507.07306", "title": "ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning", "authors": ["Yichen Lu", "Wei Dai", "Jiaen Liu", "Ching Wing Kwok", "Zongheng Wu", "Xudong Xiao", "Ao Sun", "Sheng Fu", "Jianyuan Zhan", "Yian Wang", "Takatomo Saito", "Sicheng Lai"], "abstract": "LLM-based translation agents have achieved highly human-like translation results and are capable of handling longer and more complex contexts with greater efficiency. However, they are typically limited to text-only inputs. In this paper, we introduce ViDove, a translation agent system designed for multimodal input. Inspired by the workflow of human translators, ViDove leverages visual and contextual background information to enhance the translation process. Additionally, we integrate a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge, enabling the agent to perform more accurately and adaptively in real-world scenarios. As a result, ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark for long-form automatic video subtitling and translation, featuring 17 hours of high-quality, human-annotated data. Our code is available here:", "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07306.pdf", "abstract_url": "https://arxiv.org/abs/2507.07306", "categories": ["Artificial Intelligence (cs.AI)", "Computation and Language (cs.CL)", "Audio and Speech Processing (eess.AS)"], "matching_keywords": ["agent"], "AI": {"tldr": "ViDove是一个基于LLM的多模态输入翻译代理系统，通过结合视觉和上下文背景信息以及领域特定知识，显著提高了翻译质量。", "motivation": "解决现有LLM翻译代理仅限于文本输入的问题，通过多模态输入提升翻译的准确性和适应性。", "method": "利用视觉和上下文背景信息，集成多模态记忆系统和长短时记忆模块，结合领域特定知识。", "result": "在字幕生成和一般翻译任务中，ViDove的BLEU分数提高了28%，SubER提高了15%。", "conclusion": "ViDove通过多模态输入和记忆增强推理，显著提升了翻译质量，并引入了新的长视频自动字幕和翻译基准DoveBench。"}}
{"id": "2507.07426", "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "abstract": "Recent advances in large language models have demonstrated considerable potential in scientific domains such as drug discovery. However, their effectiveness remains constrained when reasoning extends beyond the knowledge acquired during pretraining. Conventional approaches, such as fine-tuning or retrieval-augmented generation, face limitations in either imposing high computational overhead or failing to fully exploit structured scientific data. To overcome these challenges, we propose DrugMCTS, a novel framework that synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. The framework employs five specialized agents tasked with retrieving and analyzing molecular and protein information, thereby enabling structured and iterative reasoning. Without requiring domain-specific fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by over 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially higher recall and robustness compared to both general-purpose LLMs and deep learning baselines. Our results highlight the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in advancing LLM applications for drug discovery.", "subjects": "Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07426.pdf", "abstract_url": "https://arxiv.org/abs/2507.07426", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "DrugMCTS是一个结合多智能体、RAG和蒙特卡洛树搜索的药物重定位框架，旨在克服现有方法在计算开销和利用结构化科学数据方面的限制。", "motivation": "解决大型语言模型在药物发现领域中推理能力受限于预训练知识，以及传统方法如微调或检索增强生成在计算开销和利用结构化数据方面的不足。", "method": "提出DrugMCTS框架，整合RAG、多智能体协作和蒙特卡洛树搜索，使用五个专门代理检索和分析分子及蛋白质信息，实现结构化和迭代推理。", "result": "在不需领域特定微调的情况下，DrugMCTS使Qwen2.5-7B-Instruct性能超过Deepseek-R1超过20%，在DrugBank和KIBA数据集上显示出更高的召回率和鲁棒性。", "conclusion": "结构化推理、基于智能体的协作和反馈驱动的搜索机制对于推进LLM在药物发现中的应用至关重要。"}}
{"id": "2507.07445", "title": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley", "authors": ["Weihao Tan", "Changjiu Jiang", "Yu Duan", "Mingcong Lei", "Jiageng Li", "Yitian Hong", "Xinrun Wang", "Bo An"], "abstract": "Autonomous agents navigating human society must master both production activities and social interactions, yet existing benchmarks rarely evaluate these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel benchmark based on Stardew Valley, designed to assess AI agents in open-ended production-living simulations. In StarDojo, agents are tasked to perform essential livelihood activities such as farming and crafting, while simultaneously engaging in social interactions to establish relationships within a vibrant community. StarDojo features 1,000 meticulously curated tasks across five key domains: farming, crafting, exploration, combat, and social interactions. Additionally, we provide a compact subset of 100 representative tasks for efficient model evaluation. The benchmark offers a unified, user-friendly interface that eliminates the need for keyboard and mouse control, supports all major operating systems, and enables the parallel execution of multiple environment instances, making it particularly well-suited for evaluating the most capable foundation agents, powered by multimodal large language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents demonstrate substantial limitations, with the best-performing model, GPT-4.1, achieving only a 12.7% success rate, primarily due to challenges in visual understanding, multimodal reasoning and low-level manipulation. As a user-friendly environment and benchmark, StarDojo aims to facilitate further research towards robust, open-ended agents in complex production-living environments.", "subjects": "Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07445.pdf", "abstract_url": "https://arxiv.org/abs/2507.07445", "categories": ["Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "StarDojo是一个基于Stardew Valley的新型基准测试，旨在评估AI代理在开放式生产生活模拟中的表现，涵盖农业、手工艺、探索、战斗和社交互动等五个关键领域。", "motivation": "现有的基准测试很少同时评估自主代理在人类社会中的生产活动和社会互动技能，StarDojo旨在填补这一空白。", "method": "StarDojo提供了1,000个精心策划的任务和一个包含100个代表性任务的紧凑子集，支持所有主要操作系统，并允许并行执行多个环境实例，特别适合评估由多模态大型语言模型（MLLMs）驱动的基础代理。", "result": "对最先进的MLLMs代理的广泛评估显示，表现最佳的模型GPT-4.1的成功率仅为12.7%，主要由于视觉理解、多模态推理和低级操作方面的挑战。", "conclusion": "作为一个用户友好的环境和基准测试，StarDojo旨在促进在复杂生产生活环境中开发更强大、开放式的代理的进一步研究。"}}
{"id": "2507.07544", "title": "Position: We Need An Algorithmic Understanding of Generative AI", "authors": ["Oliver Eberle", "Thomas McGee", "Hamza Giaffar", "Taylor Webb", "Ida Momennejad"], "abstract": "What algorithms do LLMs actually learn and use to solve problems? Studies addressing this question are sparse, as research priorities are focused on improving performance through scale, leaving a theoretical and empirical gap in understanding emergent algorithms. This position paper proposes AlgEval: a framework for systematic research into the algorithms that LLMs learn and use. AlgEval aims to uncover algorithmic primitives, reflected in latent representations, attention, and inference-time compute, and their algorithmic composition to solve task-specific problems. We highlight potential methodological paths and a case study toward this goal, focusing on emergent search algorithms. Our case study illustrates both the formation of top-down hypotheses about candidate algorithms, and bottom-up tests of these hypotheses via circuit-level analysis of attention patterns and hidden states. The rigorous, systematic evaluation of how LLMs actually solve tasks provides an alternative to resource-intensive scaling, reorienting the field toward a principled understanding of underlying computations. Such algorithmic explanations offer a pathway to human-understandable interpretability, enabling comprehension of the model's internal reasoning performance measures. This can in turn lead to more sample-efficient methods for training and improving performance, as well as novel architectures for end-to-end and multi-agent systems.", "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "comments": "Accepted at ICML 2025 as a Spotlight Position Paper", "pdf_url": "https://arxiv.org/pdf/2507.07544.pdf", "abstract_url": "https://arxiv.org/abs/2507.07544", "categories": ["Artificial Intelligence (cs.AI)", "Machine Learning (cs.LG)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出AlgEval框架，旨在系统地研究大型语言模型（LLMs）学习和使用的算法，填补当前对涌现算法理解的理论和实证空白。", "motivation": "解决当前研究优先关注通过规模提升性能，而忽视理解LLMs实际学习和使用的算法的问题。", "method": "提出AlgEval框架，通过潜在表示、注意力和推理时间计算揭示算法原语及其组合，以解决特定任务问题。", "result": "案例研究展示了通过自上而下的假设和自下而上的电路级分析，验证涌现搜索算法的形成。", "conclusion": "系统评估LLMs如何解决任务，为资源密集型扩展提供了替代方案，促进对底层计算的原则性理解，从而实现更高效的训练方法和新颖架构设计。"}}
{"id": "2507.07505", "title": "Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models", "authors": ["Varin Sikka", "Vishal Sikka"], "abstract": "With widespread adoption of transformer-based language models in AI, there is significant interest in the limits of LLMs capabilities, specifically so-called hallucinations, occurrences in which LLMs provide spurious, factually incorrect or nonsensical information when prompted on certain subjects. Furthermore, there is growing interest in agentic uses of LLMs - that is, using LLMs to create agents that act autonomously or semi-autonomously to carry out various tasks, including tasks with applications in the real world. This makes it important to understand the types of tasks LLMs can and cannot perform. We explore this topic from the perspective of the computational complexity of LLM inference. We show that LLMs are incapable of carrying out computational and agentic tasks beyond a certain complexity, and further that LLMs are incapable of verifying the accuracy of tasks beyond a certain complexity. We present examples of both, then discuss some consequences of this work.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "6 pages; to be submitted to AAAI-26 after reviews", "pdf_url": "https://arxiv.org/pdf/2507.07505.pdf", "abstract_url": "https://arxiv.org/abs/2507.07505", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文探讨了基于Transformer的语言模型（LLMs）在计算复杂性和代理任务执行能力上的基本限制，特别是所谓的‘幻觉’现象，即LLMs在特定主题上提供虚假、事实错误或无意义信息的情况。", "motivation": "随着基于Transformer的语言模型在AI中的广泛应用，人们对其能力的限制，尤其是‘幻觉’现象，以及LLMs在自主或半自主代理任务中的应用潜力产生了浓厚兴趣。理解LLMs能执行和不能执行的任务类型变得尤为重要。", "method": "从LLM推理的计算复杂性角度出发，本文展示了LLMs在执行超出一定复杂度的计算和代理任务上的无能，以及LLMs在验证超出一定复杂度任务准确性上的限制。", "result": "研究结果表明，LLMs在执行和验证高复杂度任务方面存在固有的局限性。", "conclusion": "这项工作揭示了LLMs在计算和代理任务中的基本限制，对于理解LLMs的实际应用潜力和限制具有重要意义。"}}
{"id": "2507.07543", "title": "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora", "authors": ["Chen Amiraz", "Yaroslav Fyodorov", "Elad Haramaty", "Zohar Karnin", "Liane Lewin-Eytan"], "abstract": "Cross-lingual retrieval-augmented generation (RAG) is a critical capability for retrieving and generating answers across languages. Prior work in this context has mostly focused on generation and relied on benchmarks derived from open-domain sources, most notably Wikipedia. In such settings, retrieval challenges often remain hidden due to language imbalances, overlap with pretraining data, and memorized content. To address this gap, we study Arabic-English RAG in a domain-specific setting using benchmarks derived from real-world corporate datasets. Our benchmarks include all combinations of languages for the user query and the supporting document, drawn independently and uniformly at random. This enables a systematic study of multilingual retrieval behavior.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07543.pdf", "abstract_url": "https://arxiv.org/abs/2507.07543", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Information Retrieval (cs.IR)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文研究了阿拉伯语-英语跨语言检索增强生成（RAG）在特定领域设置中的检索偏差问题。", "motivation": "解决跨语言检索增强生成中由于语言不平衡、与预训练数据重叠以及记忆内容导致的检索挑战未被充分研究的问题。", "method": "使用来自真实世界企业数据集的基准，包括用户查询和支持文档的所有语言组合，进行系统研究。", "result": "揭示了在多语言检索行为中的偏差问题。", "conclusion": "强调了在特定领域设置中跨语言RAG检索偏差的重要性，为未来研究提供了新的方向。"}}
{"id": "2507.07509", "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System", "authors": ["Yuanchen Shi", "Longyin Zhang", "Fang Kong"], "abstract": "The growing need for psychological support due to increasing pressures has exposed the scarcity of relevant datasets, particularly in non-English languages. To address this, we propose a framework that leverages limited real-world data and expert knowledge to fine-tune two large language models: Dialog Generator and Dialog Modifier. The Generator creates large-scale psychological counseling dialogues based on predefined paths, which guide system response strategies and user interactions, forming the basis for effective support. The Modifier refines these dialogues to align with real-world data quality. Through both automated and manual review, we construct the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K dialogues across 13 groups, 16 psychological problems, 13 causes, and 12 support focuses. Additionally, we introduce the Comprehensive Agent Dialogue Support System (CADSS), where a Profiler analyzes user characteristics, a Summarizer condenses dialogue history, a Planner selects strategies, and a Supporter generates empathetic responses. The experimental results of the Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate that CADSS achieves state-of-the-art performance on both CPsDD and ESConv datasets.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)", "comments": "10pages,8 figures", "pdf_url": "https://arxiv.org/pdf/2507.07509.pdf", "abstract_url": "https://arxiv.org/abs/2507.07509", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Multiagent Systems (cs.MA)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一个框架，利用有限的真实世界数据和专家知识来微调两个大型语言模型：对话生成器和对话修改器，以构建中文心理支持对话数据集（CPsDD），并介绍了综合代理对话支持系统（CADSS）。", "motivation": "解决因压力增加而导致的心理支持需求增长，特别是在非英语语言中相关数据集的稀缺问题。", "method": "通过对话生成器基于预定义路径创建大规模心理咨询对话，对话修改器对这些对话进行细化以符合真实世界数据质量，构建CPsDD数据集，并开发CADSS系统。", "result": "构建了包含68K对话的CPsDD数据集，CADSS在策略预测和情感支持对话（ESC）任务上达到了最先进的性能。", "conclusion": "提出的框架和系统为解决心理支持对话的数据稀缺问题提供了有效方案，并在相关任务上展示了卓越性能。"}}
{"id": "2507.07634", "title": "FrugalRAG: Learning to retrieve and reason for multi-hop QA", "authors": ["Abhinav Java", "Srivathsan Koundinyan", "Nagarajan Natarajan", "Amit Sharma"], "abstract": "We consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-of-thought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples).", "subjects": "Computation and Language (cs.CL)", "comments": "Accepted at ICML Workshop: Efficient Systems for Foundation Models", "pdf_url": "https://arxiv.org/pdf/2507.07634.pdf", "abstract_url": "https://arxiv.org/abs/2507.07634", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文提出FrugalRAG方法，旨在通过改进提示和微调技术，减少多跳问答中的检索次数，同时保持或提高检索增强生成（RAG）的性能指标。", "motivation": "解决在大型非结构化文档库中回答复杂问题时，检索增强生成（RAG）方法在检索效率方面的不足，特别是在减少检索次数（即节俭性）方面的需求。", "method": "采用标准ReAct流程结合改进的提示，以及监督和基于强化学习（RL）的微调技术，以减少检索次数并保持RAG性能。", "result": "在HotPotQA等基准测试中，该方法以较少的检索次数（近乎一半）实现了与现有技术相媲美的RAG性能指标，且训练成本较低（1000个示例）。", "conclusion": "研究表明，大规模微调并非提高RAG性能的必要条件，而通过改进提示和微调技术可以在保持性能的同时显著提高检索效率，为多跳问答系统的发展提供了新的方向。"}}
{"id": "2507.07695", "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities", "authors": ["Hruday Markondapatnaikuni", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "abstract": "Fine-tuning is an immensely resource-intensive process when retraining Large Language Models (LLMs) to incorporate a larger body of knowledge. Although many fine-tuning techniques have been developed to reduce the time and computational cost involved, the challenge persists as LLMs continue to grow in size and complexity. To address this, a new approach to knowledge expansion in LLMs is needed. Retrieval-Augmented Generation (RAG) offers one such alternative by storing external knowledge in a database and retrieving relevant chunks to support question answering. However, naive implementations of RAG face significant limitations in scalability and answer accuracy. This paper introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome these limitations. Inspired by the divide-and-conquer paradigm, K2RAG integrates dense and sparse vector search, knowledge graphs, and text summarization to improve retrieval quality and system efficiency. The framework also includes a preprocessing step that summarizes the training data, significantly reducing the training time. K2RAG was evaluated using the MultiHopRAG dataset, where the proposed pipeline was trained on the document corpus and tested on a separate evaluation set. Results demonstrated notable improvements over common naive RAG implementations. K2RAG achieved the highest mean answer similarity score of 0.57, and reached the highest third quartile (Q3) similarity of 0.82, indicating better alignment with ground-truth answers. In addition to improved accuracy, the framework proved highly efficient. The summarization step reduced the average training time of individual components by 93%, and execution speed was up to 40% faster than traditional knowledge graph-based RAG systems. K2RAG also demonstrated superior scalability, requiring three times less VRAM than several naive RAG implementations tested in this study.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": "21 pages, 14 figures", "pdf_url": "https://arxiv.org/pdf/2507.07695.pdf", "abstract_url": "https://arxiv.org/abs/2507.07695", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "K^2RAG是一种改进的RAG方法，通过结合密集和稀疏向量搜索、知识图谱和文本摘要技术，提高了LLM问答能力的效果和效率。", "motivation": "解决大型语言模型(LLMs)在知识扩展时面临的资源密集型微调问题，以及传统RAG方法在可扩展性和答案准确性上的局限性。", "method": "提出KeyKnowledgeRAG (K2RAG)框架，集成密集和稀疏向量搜索、知识图谱和文本摘要技术，包括预处理步骤以减少训练时间。", "result": "在MultiHopRAG数据集上评估，K2RAG在答案相似度得分和效率上均优于传统RAG实现，训练时间减少93%，执行速度提高40%，VRAM需求减少三倍。", "conclusion": "K2RAG通过创新的方法显著提升了RAG的性能和效率，为LLM的知识扩展和问答能力提供了有效的解决方案。"}}
{"id": "2507.07902", "title": "MIRA: A Novel Framework for Fusing Modalities in Medical RAG", "authors": ["Jinhong Wang", "Tajamul Ashraf", "Zongyan Han", "Jorma Laaksonen", "Rao Mohammad Anwer"], "abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced AI-assisted medical diagnosis, but they often generate factually inconsistent responses that deviate from established medical knowledge. Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external sources, but it presents two key challenges. First, insufficient retrieval can miss critical information, whereas excessive retrieval can introduce irrelevant or misleading content, disrupting model output. Second, even when the model initially provides correct answers, over-reliance on retrieved data can lead to factual errors. To address these issues, we introduce the Multimodal Intelligent Retrieval and Augmentation (MIRA) framework, designed to optimize factual accuracy in MLLM. MIRA consists of two key components: (1) a calibrated Rethinking and Rearrangement module that dynamically adjusts the number of retrieved contexts to manage factual risk, and (2) A medical RAG framework integrating image embeddings and a medical knowledge base with a query-rewrite module for efficient multimodal reasoning. This enables the model to effectively integrate both its inherent knowledge and external references. Our evaluation of publicly available medical VQA and report generation benchmarks demonstrates that MIRA substantially enhances factual accuracy and overall performance, achieving new state-of-the-art results. Code is released at", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "ACM Multimedia 2025", "pdf_url": "https://arxiv.org/pdf/2507.07902.pdf", "abstract_url": "https://arxiv.org/abs/2507.07902", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文介绍了MIRA框架，旨在通过优化多模态大型语言模型（MLLMs）中的事实准确性，解决医疗诊断中的问题。MIRA通过动态调整检索上下文数量和集成图像嵌入与医疗知识库，显著提高了事实准确性和整体性能。", "motivation": "多模态大型语言模型在医疗诊断中生成的事实不一致响应，以及检索增强生成（RAG）中的检索不足或过度检索问题，是本研究试图解决的关键问题。", "method": "MIRA框架包含两个关键组件：一个校准的重新思考和重排模块，用于动态调整检索上下文数量以管理事实风险；以及一个集成了图像嵌入和医疗知识库的医疗RAG框架，带有查询重写模块，以支持高效的多模态推理。", "result": "在公开可用的医疗VQA和报告生成基准测试中，MIRA显著提高了事实准确性和整体性能，达到了新的最先进水平。", "conclusion": "MIRA框架通过优化事实准确性和集成多模态数据，为医疗诊断中的AI辅助提供了有效的解决方案，其代码已公开发布。"}}
{"id": "2507.07984", "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "authors": ["JingLi Lin", "Chenming Zhu", "Runsen Xu", "Xiaohan Mao", "Xihui Liu", "Tai Wang", "Jiangmiao Pang"], "abstract": "Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is:", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07984.pdf", "abstract_url": "https://arxiv.org/abs/2507.07984", "categories": ["Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "OST-Bench是一个旨在评估多模态大型语言模型（MLLMs）在线时空场景理解能力的新基准，强调在主动探索场景时处理和推理增量获取的观察结果的能力。", "motivation": "现有基准大多在离线设置下评估模型，无法反映现实世界中的动态空间推理挑战。OST-Bench旨在填补这一空白，评估模型在在线时空理解方面的能力。", "method": "通过高效的数据收集流程，OST-Bench基于ScanNet、Matterport3D和ARKitScenes构建，包含1.4k个场景和10k个问答对，用于评估MLLMs的在线时空推理能力。", "result": "评估显示，领先的MLLMs在需要复杂时空推理的任务上表现不佳，特别是在在线设置下，随着探索范围的扩大和记忆的增长，其准确性下降。", "conclusion": "研究揭示了模型在复杂线索基础的空间推理和长期记忆检索需求方面的核心挑战，为改进在线体现推理提供了方向。所有代码、数据集和基准均已公开，以促进该领域的进一步研究和开发。"}}
{"id": "2507.07847", "title": "From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems", "authors": ["Youngjoon Jang", "Seongtae Hong", "Junyoung Son", "Sungjin Park", "Chanjun Park", "Heuiseok Lim"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in natural language processing (NLP), improving factual consistency and reducing hallucinations by integrating external document retrieval with large language models (LLMs). However, the effectiveness of RAG is often hindered by coreferential complexity in retrieved documents, introducing ambiguity that disrupts in-context learning. In this study, we systematically investigate how entity coreference affects both document retrieval and generative performance in RAG-based systems, focusing on retrieval relevance, contextual understanding, and overall response quality. We demonstrate that coreference resolution enhances retrieval effectiveness and improves question-answering (QA) performance. Through comparative analysis of different pooling strategies in retrieval tasks, we find that mean pooling demonstrates superior context capturing ability after applying coreference resolution. In QA tasks, we discover that smaller models benefit more from the disambiguation process, likely due to their limited inherent capacity for handling referential ambiguity. With these findings, this study aims to provide a deeper understanding of the challenges posed by coreferential complexity in RAG, providing guidance for improving retrieval and generation in knowledge-intensive AI applications.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07847.pdf", "abstract_url": "https://arxiv.org/abs/2507.07847", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "本文探讨了核心解析在检索增强生成（RAG）系统中的变革性作用，通过系统研究实体核心如何影响RAG系统的文档检索和生成性能，发现核心解析能提升检索效果和问答性能。", "motivation": "检索增强生成（RAG）框架在自然语言处理（NLP）中虽提高了事实一致性和减少了幻觉，但检索文档中的核心复杂性引入了模糊性，影响了上下文学习的效果。", "method": "通过比较分析不同池化策略在检索任务中的表现，研究了核心解析对检索相关性和生成响应质量的影响。", "result": "研究发现，应用核心解析后，均值池化在捕捉上下文能力上表现最佳；在问答任务中，较小的模型因处理指代模糊的能力有限，从消歧过程中获益更多。", "conclusion": "本研究深化了对RAG中核心复杂性挑战的理解，为提升知识密集型AI应用中的检索和生成提供了指导。"}}
{"id": "2507.07870", "title": "DocCHA: Towards LLM-Augmented Interactive Online diagnosis System", "authors": ["Xinyi Liu", "Dachun Sun", "Yi R. Fung", "Dilek Hakkani-Tür", "Tarek Abdelzaher"], "abstract": "Despite the impressive capabilities of Large Language Models (LLMs), existing Conversational Health Agents (CHAs) remain static and brittle, incapable of adaptive multi-turn reasoning, symptom clarification, or transparent decision-making. This hinders their real-world applicability in clinical diagnosis, where iterative and structured dialogue is essential. We propose DocCHA, a confidence-aware, modular framework that emulates clinical reasoning by decomposing the diagnostic process into three stages: (1) symptom elicitation, (2) history acquisition, and (3) causal graph construction. Each module uses interpretable confidence scores to guide adaptive questioning, prioritize informative clarifications, and refine weak reasoning links.", "subjects": "Computation and Language (cs.CL)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07870.pdf", "abstract_url": "https://arxiv.org/abs/2507.07870", "categories": ["Computation and Language (cs.CL)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了DocCHA，一个基于大型语言模型（LLMs）的交互式在线诊断系统框架，旨在通过模块化和置信度感知的方法模拟临床推理过程，解决现有对话式健康代理（CHAs）在适应性多轮推理、症状澄清和透明决策方面的不足。", "motivation": "现有的对话式健康代理（CHAs）在临床诊断中缺乏适应性多轮推理、症状澄清和透明决策能力，限制了其在真实世界中的应用。", "method": "DocCHA框架将诊断过程分解为三个阶段：症状诱发、病史采集和因果图构建，每个阶段使用可解释的置信度分数来指导适应性提问、优先信息澄清和优化弱推理链接。", "result": "DocCHA通过模块化和置信度感知的方法，能够更有效地模拟临床推理过程，提高诊断的准确性和透明度。", "conclusion": "DocCHA框架为开发更适应临床诊断需求的交互式在线诊断系统提供了新的方向，有望提升CHAs在真实世界中的应用价值。"}}
{"id": "2507.07887", "title": "Automating MD simulations for Proteins using Large language Models: NAMD-Agent", "authors": ["Achuth Chandrasekhar", "Amir Barati Farimani"], "abstract": "Molecular dynamics simulations are an essential tool in understanding protein structure, dynamics, and function at the atomic level. However, preparing high quality input files for MD simulations can be a time consuming and error prone process. In this work, we introduce an automated pipeline that leverages Large Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with python scripting and Selenium based web automation to streamline the generation of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based interface for preparing simulation-ready inputs for NAMD. By integrating Gemini's code generation and iterative refinement capabilities, simulation scripts are automatically written, executed, and revised to navigate CHARMM GUI, extract appropriate parameters, and produce the required NAMD input files. Post processing is performed using additional software to further refine the simulation outputs, thereby enabling a complete and largely hands free workflow. Our results demonstrate that this approach reduces setup time, minimizes manual errors, and offers a scalable solution for handling multiple protein systems in parallel. This automated framework paves the way for broader application of LLMs in computational structural biology, offering a robust and adaptable platform for future developments in simulation automation.", "subjects": "Computation and Language (cs.CL); Computational Engineering, Finance, and Science (cs.CE); Biomolecules (q-bio.BM)", "comments": "34 pages", "pdf_url": "https://arxiv.org/pdf/2507.07887.pdf", "abstract_url": "https://arxiv.org/abs/2507.07887", "categories": ["Computation and Language (cs.CL)", "Computational Engineering, Finance, and Science (cs.CE)", "Biomolecules (q-bio.BM)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了一种利用大型语言模型（LLMs）自动化生成蛋白质分子动力学（MD）模拟输入文件的流程，旨在减少准备时间和人为错误。", "motivation": "分子动力学模拟是理解蛋白质结构、动力学和功能的重要工具，但准备高质量的输入文件是一个耗时且容易出错的过程。", "method": "研究采用大型语言模型Gemini 2.0 Flash，结合Python脚本和基于Selenium的网络自动化，通过CHARMM GUI的网页接口自动生成NAMD模拟所需的输入文件。", "result": "该方法显著减少了设置时间，最小化了手动错误，并为并行处理多个蛋白质系统提供了可扩展的解决方案。", "conclusion": "这一自动化框架为计算结构生物学中LLMs的更广泛应用铺平了道路，为模拟自动化未来的发展提供了一个强大且适应性强的平台。"}}
{"id": "2507.07155", "title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics", "authors": ["Xueqing Xu", "Boris Bolliet", "Adrian Dimitrov", "Andrew Laverick", "Francisco Villaescusa-Navarro", "Licong Xu", "Íñigo Zubeldia"], "abstract": "We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this", "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Artificial Intelligence (cs.AI)", "comments": "", "pdf_url": "https://arxiv.org/pdf/2507.07155.pdf", "abstract_url": "https://arxiv.org/abs/2507.07155", "categories": ["Instrumentation and Methods for Astrophysics (astro-ph.IM)", "Cosmology and Nongalactic Astrophysics (astro-ph.CO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "本文评估了9种检索增强生成（RAG）代理配置在105个宇宙学问答对上的表现，这些问答对专为此研究构建。", "motivation": "解决在天体物理学领域进行自主科学发现时，检索增强生成代理的有效性问题。", "method": "使用9种不同的RAG代理配置，并在专门构建的105个宇宙学问答对上进行了评估。", "result": "关键发现或结果未在摘要中明确提及，需查阅全文获取。", "conclusion": "主要结论和意义未在摘要中明确提及，需查阅全文获取。"}}
{"id": "2507.07197", "title": "Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning", "authors": ["Elia Piccoli", "Malio Li", "Giacomo Carfì", "Vincenzo Lomonaco", "Davide Bacciu"], "abstract": "The recent focus and release of pre-trained models have been a key components to several advancements in many fields (e.g. Natural Language Processing and Computer Vision), as a matter of fact, pre-trained models learn disparate latent embeddings sharing insightful representations. On the other hand, Reinforcement Learning (RL) focuses on maximizing the cumulative reward obtained via agent's interaction with the environment. RL agents do not have any prior knowledge about the world, and they either learn from scratch an end-to-end mapping between the observation and action spaces or, in more recent works, are paired with monolithic and computationally expensive Foundational Models. How to effectively combine and leverage the hidden information of different pre-trained models simultaneously in RL is still an open and understudied question. In this work, we propose Weight Sharing Attention (WSA), a new architecture to combine embeddings of multiple pre-trained models to shape an enriched state representation, balancing the tradeoff between efficiency and performance. We run an extensive comparison between several combination modes showing that WSA obtains comparable performance on multiple Atari games compared to end-to-end models. Furthermore, we study the generalization capabilities of this approach and analyze how scaling the number of models influences agents' performance during and after training.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "Published at 4th Conference on Lifelong Learning Agents (CoLLAs), 2025", "pdf_url": "https://arxiv.org/pdf/2507.07197.pdf", "abstract_url": "https://arxiv.org/abs/2507.07197", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为权重共享注意力（WSA）的新架构，用于结合多个预训练模型的嵌入，以形成丰富的状态表示，平衡效率与性能之间的权衡。通过在多个Atari游戏上的广泛比较，展示了WSA与端到端模型相比具有可比性的性能。", "motivation": "强化学习（RL）代理通常没有关于世界的先验知识，它们要么从零开始学习观察与动作空间之间的端到端映射，要么与单一且计算成本高的基础模型配对。如何有效地同时结合和利用不同预训练模型的隐藏信息在RL中仍是一个开放且研究不足的问题。", "method": "提出了一种名为权重共享注意力（WSA）的新架构，用于结合多个预训练模型的嵌入，以形成丰富的状态表示。", "result": "在多个Atari游戏上的广泛比较显示，WSA与端到端模型相比具有可比性的性能。此外，研究了这种方法的泛化能力，并分析了模型数量的扩展如何影响代理在训练期间和之后的性能。", "conclusion": "WSA架构有效地结合了多个预训练模型的嵌入，提供了一种平衡效率与性能的方法，为强化学习中的状态表示提供了新的视角。"}}
{"id": "2507.07299", "title": "LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation", "authors": ["Sonia Raychaudhuri", "Enrico Cancelli", "Tommaso Campari", "Lamberto Ballan", "Manolis Savva", "Angel X. Chang"], "abstract": "Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Despite these advances, we still lack a clear, language-focused benchmark for testing how well such agents ground the words in their instructions. We address this gap with LangNav, an open-set dataset specifically created to test an agent's ability to locate objects described at different levels of detail, from broad category names to fine attributes and object-object relations. Every description in LangNav was manually checked, yielding a lower error rate than existing lifelong- and semantic-navigation datasets. On top of LangNav we build LangNavBench, a benchmark that measures how well current semantic-navigation methods understand and act on these descriptions while moving toward their targets. LangNavBench allows us to systematically compare models on their handling of attributes, spatial and relational cues, and category hierarchies, offering the first thorough, language-centric evaluation of embodied navigation systems. We also present Multi-Layered Feature Map (MLFM), a method that builds a queryable multi-layered semantic map, particularly effective when dealing with small objects or instructions involving spatial relations. MLFM outperforms state-of-the-art mapping-based navigation baselines on the LangNav dataset.", "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07299.pdf", "abstract_url": "https://arxiv.org/abs/2507.07299", "categories": ["Robotics (cs.RO)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文介绍了LangNavBench，一个专门用于测试基于语言的语义导航代理能力的开放集数据集和基准测试。通过LangNavBench，作者系统地比较了模型在处理属性、空间和关系线索以及类别层次结构方面的能力，并提出了一种名为多层特征地图（MLFM）的新方法，该方法在LangNav数据集上优于现有的基于地图的导航基线。", "motivation": "当前缺乏一个明确的、以语言为中心的基准来测试基于语言的语义导航代理如何在其指令中定位词语。本文旨在填补这一空白。", "method": "作者创建了LangNav数据集，并在此基础上构建了LangNavBench基准测试。此外，提出了多层特征地图（MLFM）方法，该方法构建了一个可查询的多层语义地图，特别适用于处理小对象或涉及空间关系的指令。", "result": "MLFM在LangNav数据集上优于现有的基于地图的导航基线，展示了其在处理复杂语言指令方面的有效性。", "conclusion": "LangNavBench为基于语言的语义导航系统提供了一个全面的、以语言为中心的评估框架，而MLFM方法在处理特定类型的导航任务时显示出优越性能。"}}
{"id": "2507.07957", "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents", "authors": ["Yu Wang", "Xi Chen"], "abstract": "Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the field's most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07957.pdf", "abstract_url": "https://arxiv.org/abs/2507.07957", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "@RAG"], "AI": {"tldr": "MIRIX是一个模块化的多代理记忆系统，旨在解决AI记忆领域的关键挑战，使语言模型能够真正记住信息。它通过六种不同的记忆类型和多代理框架，实现了对多样化、长期用户数据的持久化、推理和准确检索。在ScreenshotVQA和LOCOMO两个基准测试中，MIRIX均表现出色，显著超越了现有基线。", "motivation": "现有的AI代理记忆解决方案主要依赖于扁平、范围狭窄的记忆组件，限制了它们在个性化、抽象和可靠回忆用户特定信息方面的能力。MIRIX旨在解决这一问题，通过引入一个能够处理丰富视觉和多模态体验的记忆系统，使记忆在现实世界场景中真正有用。", "method": "MIRIX由六种不同的记忆类型（核心、情景、语义、程序、资源记忆和知识库）组成，结合一个多代理框架，动态控制和协调记忆的更新和检索。这种方法使代理能够持久化、推理和准确检索多样化的长期用户数据。", "result": "在ScreenshotVQA多模态基准测试中，MIRIX比RAG基线提高了35%的准确率，同时减少了99.9%的存储需求。在LOCOMO长对话基准测试中，MIRIX达到了85.4%的最先进性能，远超现有基线。", "conclusion": "MIRIX为记忆增强的LLM代理设定了新的性能标准，通过其模块化和多代理的设计，有效地解决了AI记忆领域的核心挑战。此外，MIRIX提供的打包应用使用户能够体验到其记忆系统，包括实时屏幕监控、个性化记忆库构建以及直观的可视化和安全的本地存储。"}}
{"id": "2507.07983", "title": "Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology", "authors": ["Sabine Felde", "Rüdiger Buchkremer", "Gamal Chehab", "Christian Thielscher", "Jörg HW Distler", "Matthias Schneider", "Jutta G. Richter"], "abstract": "Large language models (LLMs) show promise for supporting clinical decision-making in complex fields such as rheumatology. Our evaluation shows that smaller language models (SLMs), combined with retrieval-augmented generation (RAG), achieve higher diagnostic and therapeutic performance than larger models, while requiring substantially less energy and enabling cost-efficient, local deployment. These features are attractive for resource-limited healthcare. However, expert oversight remains essential, as no model consistently reached specialist-level accuracy in rheumatology.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07983.pdf", "abstract_url": "https://arxiv.org/abs/2507.07983", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["@RAG"], "AI": {"tldr": "大型语言模型（LLMs）在复杂领域如风湿病学中支持临床决策显示出潜力。我们的评估显示，结合检索增强生成（RAG）的小型语言模型（SLMs）比大型模型实现了更高的诊断和治疗性能，同时需要显著更少的能量并支持成本效益高的本地部署。这些特点对资源有限的医疗保健具有吸引力。然而，专家监督仍然必不可少，因为没有模型在风湿病学中持续达到专家级准确性。", "motivation": "解决在资源有限的医疗保健环境中，如何高效、准确地支持临床决策的问题，特别是在复杂的风湿病学领域。", "method": "使用小型语言模型（SLMs）结合检索增强生成（RAG）技术，与大型语言模型（LLMs）进行比较评估。", "result": "SLMs结合RAG在诊断和治疗性能上优于LLMs，同时能耗更低，支持成本效益高的本地部署。", "conclusion": "尽管SLMs结合RAG在性能和能效方面优于LLMs，但在风湿病学领域达到专家级准确性仍需专家监督。"}}
{"id": "2507.07998", "title": "PyVision: Agentic Vision with Dynamic Tooling", "authors": ["Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Ming Li", "Qilong Wu", "Kaipeng Zhang", "Chen Wei"], "abstract": "LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.", "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)", "comments": "26 Pages, 10 Figures, Technical report", "pdf_url": "https://arxiv.org/pdf/2507.07998.pdf", "abstract_url": "https://arxiv.org/abs/2507.07998", "categories": ["Computation and Language (cs.CL)", "Artificial Intelligence (cs.AI)", "Computer Vision and Pattern Recognition (cs.CV)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "PyVision是一个交互式、多轮次的框架，使MLLMs能够自主生成、执行和优化基于Python的工具，以实现灵活和可解释的问题解决。", "motivation": "解决视觉推理中预定义工作流和静态工具集的限制，提升模型的灵活性和问题解决能力。", "method": "开发了一个动态生成、执行和优化Python工具的多轮次框架，并对工具的使用进行了分类和分析。", "result": "PyVision在多个基准测试中实现了性能提升，GPT-4.1在V*上提升了7.8%，Claude-4.0-Sonnet在VLMsAreBlind-mini上提升了31.1%。", "conclusion": "动态工具生成不仅使模型能够使用工具，还能发明工具，推动了更高级的视觉推理能力的发展。"}}
{"id": "2507.07376", "title": "PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments", "authors": ["Hengrui Liu", "Yi Feng", "Qilong Zhang"], "abstract": "Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster response, exploration, and reconnaissance. However, dynamic and unknown environments pose significant challenges due to target unpredictability and environmental uncertainty. To tackle these issues, we propose PILOC, a framework that operates without global prior knowledge, leveraging local perception and communication. It introduces a pheromone inverse guidance mechanism to enable efficient coordination and dynamic target localization. PILOC promotes decentralized cooperation through local communication, significantly reducing reliance on global channels. Unlike conventional heuristics, the pheromone mechanism is embedded into the observation space of Deep Reinforcement Learning (DRL), supporting indirect agent coordination based on environmental cues. We further integrate this strategy into a DRL-based multi-agent architecture and conduct extensive experiments. Results show that combining local communication with pheromone-based guidance significantly boosts search efficiency, adaptability, and system robustness. Compared to existing methods, PILOC performs better under dynamic and communication-constrained scenarios, offering promising directions for future MASAR applications.", "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)", "comments": null, "pdf_url": "https://arxiv.org/pdf/2507.07376.pdf", "abstract_url": "https://arxiv.org/abs/2507.07376", "categories": ["Robotics (cs.RO)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了一种名为PILOC的框架，用于在未知环境中进行多智能体动态目标搜索，通过局部感知和通信以及信息素逆向引导机制，提高了搜索效率和系统鲁棒性。", "motivation": "解决在多智能体搜索和救援（MASAR）任务中，由于目标不可预测和环境不确定性带来的挑战。", "method": "提出PILOC框架，结合局部通信和信息素逆向引导机制，并将信息素机制嵌入深度强化学习（DRL）的观察空间中，支持基于环境线索的间接智能体协调。", "result": "实验结果表明，结合局部通信和信息素引导的策略显著提高了搜索效率、适应性和系统鲁棒性，在动态和通信受限的场景下表现优于现有方法。", "conclusion": "PILOC为未来的MASAR应用提供了有前景的方向，特别是在动态和通信受限的环境中。"}}
{"id": "2507.07906", "title": "Agentic Retrieval of Topics and Insights from Earnings Calls", "authors": ["Anant Gupta", "Rajarshi Bhowmik", "Geoffrey Gunow"], "abstract": "Tracking the strategic focus of companies through topics in their earnings calls is a key task in financial analysis. However, as industries evolve, traditional topic modeling techniques struggle to dynamically capture emerging topics and their relationships. In this work, we propose an LLM-agent driven approach to discover and retrieve emerging topics from quarterly earnings calls. We propose an LLM-agent to extract topics from documents, structure them into a hierarchical ontology, and establish relationships between new and existing topics through a topic ontology. We demonstrate the use of extracted topics to infer company-level insights and emerging trends over time. We evaluate our approach by measuring ontology coherence, topic evolution accuracy, and its ability to surface emerging financial trends.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)", "comments": "The 2nd Workshop on Financial Information Retrieval in the Era of Generative AI, The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval July 13-17, 2025 | Padua, Italy", "pdf_url": "https://arxiv.org/pdf/2507.07906.pdf", "abstract_url": "https://arxiv.org/abs/2507.07906", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)"], "matching_keywords": ["agent", "agentic"], "AI": {"tldr": "本文提出了一种基于LLM-agent的方法，用于从季度收益电话会议中发现和检索新兴主题，通过构建层次化本体来动态捕捉新兴主题及其关系，并评估了该方法在推断公司层面见解和新兴趋势方面的有效性。", "motivation": "传统主题建模技术在动态捕捉行业演变中的新兴主题及其关系方面存在困难，本文旨在解决这一问题。", "method": "使用LLM-agent从文档中提取主题，将其结构化为层次化本体，并通过主题本体建立新旧主题之间的关系。", "result": "通过测量本体一致性、主题演变准确性及其在浮现新兴金融趋势方面的能力，评估了所提方法的有效性。", "conclusion": "本文提出的LLM-agent驱动方法能够有效地发现和检索新兴主题，为财务分析提供了动态捕捉公司战略焦点的新工具。"}}
{"id": "2507.07969", "title": "Reinforcement Learning with Action Chunking", "authors": ["Qiyang Li", "Zhiyuan Zhou", "Sergey Levine"], "abstract": "We present Q-chunking, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks. Our recipe is designed for the offline-to-online RL setting, where the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. Effective exploration and sample-efficient learning remain central challenges in this setting, as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. Our key insight is that action chunking, a technique popularized in imitation learning where sequences of future actions are predicted rather than a single action at each timestep, can be applied to temporal difference (TD)-based RL methods to mitigate the exploration challenge. Q-chunking adopts action chunking by directly running RL in a 'chunked' action space, enabling the agent to (1) leverage temporally consistent behaviors from offline data for more effective online exploration and (2) use unbiased $n$-step backups for more stable and efficient TD learning. Our experimental results demonstrate that Q-chunking exhibits strong offline performance and online sample efficiency, outperforming prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks.", "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)", "comments": "25 pages, 15 figures", "pdf_url": "https://arxiv.org/pdf/2507.07969.pdf", "abstract_url": "https://arxiv.org/abs/2507.07969", "categories": ["Machine Learning (cs.LG)", "Artificial Intelligence (cs.AI)", "Robotics (cs.RO)", "Machine Learning (stat.ML)"], "matching_keywords": ["agent"], "AI": {"tldr": "本文提出了Q-chunking，一种简单而有效的方法，用于改进长视野、稀疏奖励任务的强化学习算法。该方法专为离线到在线RL设置设计，旨在利用离线先验数据集最大化在线学习的样本效率。Q-chunking通过直接在“分块”动作空间中运行RL，使代理能够利用离线数据中的时间一致行为进行更有效的在线探索，并使用无偏的n步备份进行更稳定和高效的TD学习。实验结果表明，Q-chunking在长视野、稀疏奖励操作任务上表现出强大的离线性能和在线样本效率，优于之前的最佳离线到在线方法。", "motivation": "解决在离线到在线强化学习设置中，如何利用离线数据来获取良好的探索策略，以及如何提高探索和样本效率学习的核心挑战。", "method": "Q-chunking，一种将动作分块技术应用于基于时间差分（TD）的强化学习方法，通过在“分块”动作空间中直接运行RL，利用离线数据中的时间一致行为进行更有效的在线探索，并使用无偏的n步备份进行更稳定和高效的TD学习。", "result": "Q-chunking在长视野、稀疏奖励操作任务上表现出强大的离线性能和在线样本效率，优于之前的最佳离线到在线方法。", "conclusion": "Q-chunking通过动作分块技术有效解决了离线到在线强化学习中的探索挑战，提高了学习效率和稳定性，为长视野、稀疏奖励任务提供了一种新的解决方案。"}}
